Published as a conference paper at ICLR 2024
GENERALIZED NEURAL SORTING NETWORKS WITH
ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS
Jungtaek Kim1 Jeongbeen Yoon2 Minsu Cho2 1University of Pittsburgh 2POSTECH
jungtaek.kim@pitt.edu {jeongbeen,mscho}@postech.ac.kr
ABSTRACT
Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
1 INTRODUCTION
Traditional sorting algorithms (Cormen et al., 2022), e.g., bubble sort, insertion sort, and quick sort, are a well-established approach to arranging given instances in computer science. Since such a sorting algorithm is a basic component to build diverse computer systems, it has been a longstanding significant research area in science and engineering. Moreover, sorting networks (Knuth, 1998; Ajtai et al., 1983), which are structurally designed as an abstract device with a fixed number of wires, have been widely used to perform a sorting algorithm on computing hardware, where each wire corresponds to a connection for a single swap operation.
Given an unordered sequence of n elements s = [s1, . . . , sn] ∈ Rn, the problem of sorting is defined
to find a permutation matrix P ∈ {0, 1}n×n that transforms s into an ordered sequence so:
so = P⊤s, (1)
where a sorting algorithm is a function f of s that predicts a permutation matrix P:
P = f (s). (2)
We generalize the formulation of traditional sorting problems to handle more diverse and expressive types of inputs, e.g., multi-digit images and image fragments, which can contain ordinal information semantically. To this end, we extend the sequence of scalars s to the sequence of vectors X = [x1, . . . , xn]⊤ ∈ Rn×d, where d ≫ 1 is an input dimensionality, and consider the following:
Xo = P⊤X, (3)
where Xo and X are ordered and unordered inputs, respectively. This generalized sorting problem
can be reduced to (1) if we are given a proper mapping g from an input x ∈ Rd to an ordinal value s ∈ R. Without such a mapping g, predicting P in (3) remains more challenging than in (1) because x is often a highly implicative high-dimensional input. We address this generalized sorting problem by learning a neural sorting network together with a mapping g in an end-to-end manner, given training data {(X(i), P(i)
gt )}N
i=1. The main challenge is to make the whole network f ([g(x1), . . . , g(xn)]) with mapping and sorting components differentiable in order to effectively train the network with a gradient-based learning scheme, which is not the case in general. To tackle
1
arXiv:2310.07174v2 [cs.LG] 14 Mar 2024


Published as a conference paper at ICLR 2024
the differentiability issue for such a composite function, there has been recent research (Grover et al., 2019; Cuturi et al., 2019; Blondel et al., 2020; Petersen et al., 2021; 2022).
In this paper, following a sorting network-based sorting algorithm with differentiable swap functions (DSFs) (Petersen et al., 2021; 2022), we first define a softening error by a sorting network, which indicates a difference between original and smoothed elements. Then, we propose an errorfree DSF that resolves an error accumulation problem induced by a soft DSF; this allows us to guarantee a zero error in mapping X to proper ordinal values. Based on this, we develop the sorting network with error-free DSFs where we adopt a permutation-equivariant Transformer architecture with multi-head attention (Vaswani et al., 2017) to capture dependency between high-dimensional inputs and also leverage the model capacity of the neural network with a self-attention scheme.
Our contributions can be summarized as follows: (i) We define a softening error that measures a difference between original and smoothed values; (ii) We propose an error-free DSF that resolves the error accumulation problem of conventional DSFs and is still differentiable; (iii) We adopt a permutation-equivariant network with multi-head attention as a mapping from inputs to ordinal variables g(X), unlike g(x); (iv) We demonstrate that our proposed methods are effective in diverse sorting benchmarks, compared to existing baseline methods.
2 SORTING NETWORKS WITH DIFFERENTIABLE SWAP FUNCTIONS
Following traditional sorting algorithms such as bubble sort, quick sort, and merge sort (Cormen et al., 2022) and sorting networks that are constructed by a fixed number of wires (Knuth, 1998; Ajtai et al., 1983), a swap function is a key ingredient of sorting algorithms and sorting networks:
(x′, y′) = swap(x, y), (4)
where x′ = min(x, y) and y′ = max(x, y), which makes the order of x and y correct. For example, if x > y, then x′ = y and y′ = x. Without loss of generality, we can express min(·, ·) and max(·, ·) with the following equations:
min(x, y) = x⌊σ(y − x)⌉ + y⌊σ(x − y)⌉ and max(x, y) = x⌊σ(x − y)⌉ + y⌊σ(y − x)⌉, (5)
where ⌊·⌉ rounds to the nearest integer and σ(·) ∈ [0, 1] transforms an input to a bounded value, i.e., a probability over inputs. Computing (5) is straightforward, but they are not differentiable. To enable us to differentiate a swap function, the soft versions of min and max can be defined:
min(x, y) = xσ(y − x) + yσ(x − y) and max(x, y) = xσ(x − y) + yσ(y − x), (6)
where σ(·) is differentiable. In addition to its differentiability, either (5) or (6) can be achieved with a sigmoid function σ(x), i.e., a s-shaped function, which satisfies the following properties that (i) σ(x) is non-decreasing, (ii) σ(x) = 1 if x → ∞, (iii) σ(x) = 0 if x → −∞, (iv) σ(0) = 0.5, and (v) σ(x) = 1 − σ(−x). Also, as discussed by Petersen et al. (2022), the choice of σ affects the performance of neural network-based sorting network in theory as well as in practice. For example, an optimal monotonic sigmoid function, which is visualized in Figure 4, is defined as the following:
σO(x) =



−1
16 (βx)−1 if βx < −0.25,
1− 1
16 (βx)−1 if βx > 0.25, βx + 0.5 otherwise,
(7)
where β is steepness; see the work (Petersen et al., 2022) for the details of these numerical and theoretical analyses. Here, we would like to emphasize that the important point of such monotonic sigmoid functions is strict monotonicity. However, as will be discussed in Section 3, it induces an error accumulation problem, which can degrade the performance of the sorting network.
By either (5) or (6), the permutation matrix P (henceforth, denoted as Phard and Psoft for (5) and (6), respectively) is calculated by the following procedure of a sorting network: (i) Building a predefined sorting network with a fixed number of wires – a wire is a component for comparing and swapping two elements; (ii) Feeding an unordered sequence s into the pre-defined sorting network and calculating a wire-wise permutation matrix Pi for each wire i iteratively; (iii) Calculating the permutation matrix P by multiplying all wire-wise permutation matrices.
As shown in Figure 1, a set of wires represents a set of swap operations that are operated simultaneously, so that each set produces an intermediate permutation matrix Pi at the ith step. Consequently,
2


Published as a conference paper at ICLR 2024
Figure 1: A sorting network with 5 wire sets and their permutation matrices.
x=0.1
y=0.0 x=0.2
y=0.0 x=0.5
y=0.0 x=1.0
y=0.0 x=2.0
y=0.0 x=4.0
y=0.0 x=8.0
y=0.0
0.0
0.2
0.4
0.6
0.8
1.0
max(x,y)−min(x,y)
max(x,y)−min(x,y)
Logistic Logistic w/ ART Reciprocal Cauchy Optimal Error-Free DSF
Figure 2: Comparisons of diverse DSFs where a swap function is applied once. After a single operation, two input values x and y are softened while our error-free DSF does not change two values. If |x − y| is small, softening will be more significant.
P⊤ = P1⊤P2⊤ · · · P⊤
k = (Pk · · · P2P1)⊤, where k is the number of wire sets. For example, in Fig
ure 1, k = 5.
The doubly-stochastic matrix property of P is shown by the following proposition:
Proposition 1 (Modification of Lemma 3 in the work (Petersen et al., 2022)). A permutation matrix
P ∈ Rn×n is doubly-stochastic, which implies that Pn
i=1[P]ij = 1 and Pn
j=1[P]ij = 1. In
particular, regardless of the definition of a swap function with min, max, min, and max, hard and soft permutation matrices, i.e., Phard and Psoft, are doubly-stochastic.
Proof. The proof of Proposition 1 is provided in Section D.
In Sections 3 and 4, we present an error-free DSF and a neural sorting network with error-free DSFs.
3 ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS
Before introducing our error-free DSF, we start by describing the motivation of the error-free DSF.
Due to the nature of min and max, which is described in (6), the monotonic DSF changes original input values. For example, if x < y, then x < min(x, y) and max(x, y) < y after applying the swap function. It can be a serious problem because changes by the DSF are accumulated as the DSF applies iteratively, called an error accumulation problem in this paper. The results of sigmoid functions such as the logistic, logistic with ART, reciprocal, Cauchy, and optimal monotonic functions, and also our error-free DSF are presented in Figure 2, where a swap function is applied once; see the work (Petersen et al., 2022) for the respective sigmoid functions. All DSFs except for our error-free DSF change two values, so that they can make two values not distinguishable. In particular, if a difference between two values is small, the consequence of softening is more significant than a case with a large difference. Moreover, if we apply a swap function repeatedly, they eventually become identical; see Figure 5 in Section B. While a swap function is not applied as many as it is tested in the synthetic example shown in Figure 5, it can still cause the error accumulation problem with a few operations. Here we formally define a softening error, which has been mentioned in this paragraph:
Definition 1. Suppose that we are given x and y where x < y. By (6), these values x and y are softened by a monotonic DSF and they satisfy the following inequalities:
x < x′ = min(x, y) ≤ y′ = max(x, y) < y. (8)
Therefore, we define a difference between the original and softened values, x′ − x or y − y′:
y − y′ = y − max(x, y) = x′ − x = min(x, y) − x > 0, (9)
which is called a softening error in this paper. Without loss of generality, the softening error is min(x, y) − min(x, y) or max(x, y) − max(x, y) for any x, y.
3


Published as a conference paper at ICLR 2024
Note that (9) is satisfied by y − max(x, y) = y(1 − σ(y − x)) − xσ(x − y) = yσ(x − y) − x(1 − σ(y − x)) = min(x, y) − x, using (6) and σ(x − y) = 1 − σ(y − x).
With Definition 1, we are able to specify the seriousness of the error accumulation problem:
Proposition 2. Suppose that x and y are given and a DSF is applied k times. Assuming an extreme scenario that k → ∞, error accumulation becomes (max(x, y) − min(x, y))/2, under the assumption that ∇xσ(x) > 0.
Proof. The proof of this proposition can be found in Section E.
As mentioned in the proof of Proposition 2 and empirically shown in Figure 2, a swap function with relatively large ∇xσ(x) changes the original values x, y significantly compared to a swap function with relatively small ∇xσ(x) – they tend to become identical with the small number of operations in the case of large ∇xσ(x).
In addition to the error accumulation problem, such a DSF depends on the scale of |x − y| as shown in Figure 2. If x < y but x and y are close enough, σ(y − x) is between 0.5 and 1, which implies that the error can be induced by the scale of |x − y| as well.
To tackle the aforementioned problem of error accumulation, we propose an error-free DSF:
(x′, y′) = swaperror-free(x, y), (10)
where
x′ = min(x, y) − min(x, y) sg+min(x, y) and y′ = (max(x, y) − max(x, y))sg+max(x, y).
(11) Note that sg indicates that gradients are stopped amid backward propagation, inspired by a straightthrough estimator (Bengio et al., 2013). At a step for forward propagation, the error-free DSF produces x′ = min(x, y) and y′ = max(x, y). On the contrary, at a step for backward propagation, the gradients of min and max are used to update learnable parameters. Consequently, our error-free DSF does not smooth the original elements as shown in Figure 2 and our DSF shows 100% accuracy for accem and accew (see Section 5 for their definitions) as shown in Figure 6. Compared to our DSF, the existing DSFs do not correspond the original elements to the elements that have been compared and fail to achieve reasonable performance as a sequence length increases, in the cases of Figure 6.
By (5), (6), and (11), we obtain the following:
x′ = ((x⌊σ(y − x)⌉+y⌊σ(x − y)⌉)−(xσ(y − x)+yσ(x − y)))sg +(xσ(y − x)+yσ(x − y))
= x ((⌊σ(y − x)⌉−σ(y − x))sg +σ(y − x))+y ((⌊σ(x − y)⌉−σ(x − y))sg +σ(x − y)) , (12)
y′ = x ((⌊σ(x − y)⌉−σ(x − y))sg +σ(x − y))+y ((⌊σ(y − x)⌉−σ(y − x))sg +σ(y − x)) , (13)
which can be used to define a permutation matrix with the error-free DSF. For example, if n = 2, a permutation matrix P over [x, y] is
P = (⌊σ(y − x)⌉ − σ(y − x))sg + σ(y − x) (⌊σ(x − y)⌉ − σ(x − y))sg + σ(x − y)
(⌊σ(x − y)⌉ − σ(x − y))sg + σ(x − y) (⌊σ(y − x)⌉ − σ(y − x))sg + σ(y − x) . (14)
To sum up, we can describe the following proposition on our error-free DSF, swaperror-free(·, ·):
Proposition 3. By (11), the softening error x′ − min(x, y) or max(x, y) − y′ for an error-free DSF is zero.
Proof. The proof of this proposition is presented in Section F.
4 NEURAL SORTING NETWORKS WITH ERROR-FREE DIFFERENTIABLE
SWAP FUNCTIONS
In this section we propose a generalized neural network-based sorting network with an error-free DSF and a permutation-equivariant neural network, considering the properties covered in Section 3.
4


Published as a conference paper at ICLR 2024
Figure 3: Illustration of our neural sorting network with error-free DSFs. Given high-dimensional inputs X, a permutation-equivariant network produces a vector of ordinal variables s, which is used to be swapped using a soft or hard sorting network.
First, we describe a procedure for transforming high-dimensional inputs to ordinal scores. Such a mapping g : Rd → R, which consists of a set of learnable parameters, has to satisfy a permutationequivariant property:
[g(xπ1 ), g(xπ2 ), . . . , g(xπn )] = π([g(x1), g(x2), . . . , g(xn)]), (15)
where πi = [π([1, 2, . . . , n])]i ∀i ∈ [n], for any permutation function π. Typically, an instancewise neural network, which is applied to each element in a sequence given, is permutationequivariant (Zaheer et al., 2017). Based on this property, instance-wise CNNs are employed in differentiable sorting algorithms (Grover et al., 2019; Cuturi et al., 2019; Petersen et al., 2021; 2022). However, such an instance-wise architecture is limited since it is ineffective for capturing essential features from a sequence. Some types of neural networks such as long short-term memory (Hochreiter & Schmidhuber, 1997) and the standard Transformer architecture (Vaswani et al., 2017) are capable of modeling a sequence of instances, utilizing recurrent connections, scaled dot-product attention, or parameter sharing across elements. While they are powerful for modeling a sequence, they are not obviously permutation-equivariant. Instead of such permutation-sensitive models, we adopt a robust Transformer-based network that satisfies the permutation-equivariant property, which is inspired by the recent work (Vaswani et al., 2017; Lee et al., 2019).
To explain our network, we briefly introduce scaled dot-product attention and multi-head attention:
att(Q, K, V) = softmax QK⊤
√dm
V and mha(Q, K, V) = [head1, head2, . . . , headh]Wo,
(16) where headi = att(QW(i)
q , KW(i)
k , VW(i)
v ), Q, K, V ∈ Rn×hdm , W(i)
q , W(i)
k , W(i)
v ∈ Rhdm×dm ,
and Wo ∈ Rhdm×hdm . Similar to the Transformer network, a series of mha blocks is stacked with layer normalization (Ba et al., 2016) and residual connections (He et al., 2016), and in this paper X is processed by mha(Z, Z, Z) where Z = g′(X) or Z is the output of a previous layer; see Section I for the details of the architectures. Note that g′(·) is an instance-wise embedding layer, e.g., a simple fully-connected network or a simple CNN. Importantly, compared to the standard Transformer model, our network does not include a positional embedding, in order to satisfy the permutation-equivariant property; mha(Z, Z, Z) satisfies (15) for the permutation of z1, z2, . . . , zn
where Z = [z1, z2, . . . , zn]⊤. The output of our network is s, followed by the last instance-wise fully-connected layer. Finally, as shown in Figure 3, our sorting network is able to produce differentiable permutation matrices over s, i.e., Phard and Psoft, by utilizing (11) and (6), respectively. Note that Phard and Psoft are doubly-stochastic by Proposition 1. In addition, the details of the permutation-equivariant network with multi-head attention are briefly visualized in Figure 7.
To learn the permutation-equivariant network g, we define both objectives for Psoft and Phard:
Lsoft = −
n
X
i=1
n
X
j=1
[Pgt log Psoft + (1 − Pgt) log(1 − Psoft)]ij , (17)
Lhard = ∥Xo,hard − Xo,gt∥2
F = ∥P⊤
hardX − P⊤
gtX∥2
F , (18)
5


Published as a conference paper at ICLR 2024
Table 1: Results on sorting the four-digit MNIST dataset. The results are measured in accem and accew (in parentheses). FLOPs is on the basis of a sequence length 3. All the values are averaged over 5 runs with different seeds.
Method Model Sequence Length FLOPs #Param.
3 5 7 9 15 32
NeuralSort
CNN
91.9 (94.5) 77.7 (90.1) 61.0 (86.2) 43.4 (82.4) 9.7 (71.6) 0.0 (38.8)
130M 855K
Sinkhorn Sort 92.8 (95.0) 81.1 (91.7) 65.6 (88.2) 49.7 (84.7) 12.6 (74.2) 0.0 (41.2) Fast Sort & Rank 90.6 (93.5) 71.5 (87.2) 49.7 (81.3) 29.0 (75.2) 2.8 (60.9) 
Diffsort
Logistic 92.0 (94.5) 77.2 (89.8) 54.8 (83.6) 37.2 (79.4) 4.7 (62.3) 0.0 (56.3) Logistic w/ ART 94.3 (96.1) 83.4 (92.6) 71.6 (90.0) 56.3 (86.7) 23.5 (79.4) 0.5 (64.9) Reciprocal 94.4 (96.1) 85.0 (93.3) 73.4 (90.7) 60.8 (88.1) 30.2 (81.9) 1.0 (66.8) Cauchy 94.2 (96.0) 84.9 (93.2) 73.3 (90.5) 63.8 (89.1) 31.1 (82.2) 0.8 (63.3) Optimal 94.6 (96.3) 85.0 (93.3) 73.6 (90.7) 62.2 (88.5) 31.8 (82.3) 1.4 (67.9)
Ours Error-Free DSFs
CNN 95.2 (96.7) 87.2 (94.2) 76.6 (91.6) 64.8 (89.2) 34.7 (83.3) 2.1 (69.2) 130M 855K Transformer-S 95.9 (97.1) 94.8 (97.5) 90.8 (96.5) 86.9 (95.7) 74.3 (93.6) 37.8 (87.7) 130M 665K Transformer-L 96.5 (97.5) 95.4 (97.7) 92.9 (97.2) 90.1 (96.5) 82.5 (95.0) 46.2 (88.9) 137M 3.104M
where Pgt is a ground-truth permutation matrix. Note that all the operations in Lsoft are entry-wise. Similar to (17), the objective (18) for Phard should be designed as the form of binary cross-entropy, which tends to be generally robust for training deep neural networks. However, we struggle to apply the binary cross-entropy for Phard into our problem formulation, due to discretized loss values. In particular, the form of cross-entropy for Phard can be used to train the sorting network, but degrades its performance in our preliminary experiments. Thus, we choose the objective for Phard
as ∥Xo,hard − Xo,gt∥2
F with the Frobenius norm, which helps to train the network more robustly.
In addition, using a proposition on splitting Phard, which is discussed in Section H, the objective (18) for Phard can be modified by splitting Phard, Pgt, and X, which is able to reduce the number of possible permutations; see the associated section for details. Eventually, our network g is trained by the combined loss L = Lsoft + λLhard, where λ is a balancing hyperparameter; an analysis on λ can be found in the appendices. As mentioned above, a landscape of Lhard is not smooth due to the property of a straight-through estimator, even though we use Lhard. Thus, we combine both objectives to the form of a single loss, which is widely adopted in the deep learning community.
5 EXPERIMENTS
We demonstrate experimental results to show the validity of our methods. Our neural networkbased sorting network aims to solve two benchmarks: sorting (i) multi-digit images and (ii) image fragments. Unless otherwise specified, an odd-even sorting network is used in the experiments. We measure the performance of each method in accem and accew:
accem =
PN i=1
Tn
j=1 1 sˆ(i)
j = s ̃(i)
j
N and accew =
PN i=1
Pn
j=1 1 sˆ(i)
j = s ̃(i)
j
Nn ,
(19) where argsort returns indices to sort a given vector and 1(·) is an indicator function. Note that
sˆ(i) = argsort P(i)⊤
gt s(i) and s ̃(i) = argsort P(i)⊤s(i) . (20)
We attempt to match the capacities of the Transformer-based models to the conventional CNNs. As described in Tables 1, 2, and 3, the capacities of the Transformer-Small models are smaller than or similar to the capacities of the CNNs in terms of FLOPs and the number of parameters.
5.1 SORTING MULTI-DIGIT IMAGES
Datasets. As steadily utilized in the previous work (Grover et al., 2019; Cuturi et al., 2019; Blondel et al., 2020; Petersen et al., 2021; 2022), we create a four-digit dataset by concatenating four images from the MNIST dataset (LeCun et al., 1998); see Figure 3 for some examples of the dataset. On the other hand, the SVHN dataset (Netzer et al., 2011) contains multi-digit numbers extracted from street view images and is therefore suitable for sorting.
6


Published as a conference paper at ICLR 2024
Table 2: Results on sorting the SVHN dataset. FLOPs is computed on the basis of a sequence length 3. All the values are averaged over 5 runs with different seeds.
Method Model Sequence Length FLOPs #Param.
3 5 7 9 15
Diffsort
Logistic
CNN
76.3 (83.2) 46.0 (72.7) 21.8 (63.9) 13.5 (61.7) 0.3 (45.9)
326M 1.226M
Logistic w/ ART 83.2 (88.1) 64.1 (82.1) 43.8 (76.5) 24.2 (69.6) 2.4 (56.8) Reciprocal 85.7 (89.8) 68.8 (84.2) 53.3 (80.0) 40.0 (76.3) 13.2 (66.0) Cauchy 85.5 (89.6) 68.5 (84.1) 52.9 (79.8) 39.9 (75.8) 13.7 (66.0) Optimal 86.0 (90.0) 67.5 (83.5) 53.1 (80.0) 39.1 (76.0) 13.2 (66.3)
Ours Error-Free DSFs
CNN 86.8 (90.6) 68.9 (84.5) 53.4 (80.4) 40.0 (77.0) 12.0 (65.3) 326M 1.226M Transformer-S 86.6 (90.2) 72.6 (85.7) 62.5 (83.5) 48.6 (79.3) 19.3 (69.6) 210M 1.223M Transformer-L 88.0 (91.2) 74.0 (86.3) 63.9 (83.8) 50.2 (80.1) 21.7 (71.2) 332M 3.475M
Table 3: Results on sorting image fragments of MNIST and CIFAR-10. 2 × 2 and 3 × 3 indicate the numbers of fragments, and 14 × 14, 9 × 9, 16 × 16, and 10 × 10 (in parentheses) indicate the sizes of image fragments. FLOPs is computed on the basis of the MNIST 2 × 2 (14 × 14) case and the CIFAR-10 2 × 2 (16 × 16) case. All the values are averaged over 5 runs with different seeds.
Method Model
MNIST CIFAR-10
2 × 2 3 × 3 FLOPs #Param. 2 × 2 3 × 3 FLOPs #Param.
(14 × 14) (9 × 9) (16 × 16) (10 × 10)
Diffsort
Logistic
CNN
98.5 (99.0) 5.3 (42.9)
1.498M 84K
56.9 (73.6) 0.8 (27.7)
1.663M 85K
Logistic w/ ART 98.4 (99.1) 5.4 (42.9) 56.7 (73.4) 0.7 (27.7) Reciprocal 98.4 (99.2) 5.3 (42.9) 56.7 (73.4) 0.7 (27.8) Cauchy 98.4 (99.2) 5.3 (42.9) 56.9 (73.6) 0.9 (27.9) Optimal 98.4 (99.1) 5.3 (43.0) 56.6 (73.4) 0.7 (27.7)
Ours Error-Free DSFs CNN 98.4 (99.2) 5.2 (42.6) 1.498M 84K 56.9 (73.6) 0.8 (28.0) 1.663M 85K
Transformer 98.6 (99.2) 5.6 (43.7) 946K 87K 58.1 (74.2) 0.9 (28.3) 1.111M 87K
Experimental Details. We conduct the experiments 5 times by varying random seeds to report the average of accem and accew, and use the optimal monotonic sigmoid function as DSFs. The performance of each model is measured by a test dataset. We use the AdamW optimizer (Loshchilov & Hutter, 2018), and train each model for 200,000 steps on the four-digit MNIST dataset and 300,000 steps on the SVHN dataset. Unless otherwise noted, we follow the same settings of the work (Petersen et al., 2022) for fair comparisons. Missing details are described in Section J.
Results. Tables 1 and 2 show the results of the previous work such as NeuralSort (Grover et al., 2019), Sinkhorn Sort (Cuturi et al., 2019), Fast Sort & Rank (Blondel et al., 2020), and Diffsort (Petersen et al., 2021; 2022), and our methods on the MNIST and SVHN datasets, respectively. When we use the conventional CNN as a permutation-equivariant network, our method shows better than or comparable to the previous methods. As we exploit more powerful models, i.e., the TransformerSmall and Transformer-Large permutation-equivariant models, our approaches show better results compared to other existing methods including our method with the conventional CNN.1
5.2 SORTING IMAGE FRAGMENTS
Datasets. For experiments on sorting image fragments, we use two datasets: the MNIST dataset (LeCun et al., 1998) and the CIFAR-10 dataset (Krizhevsky & Hinton, 2009). Similar to the work (Mena et al., 2018), we create multiple fragments or patches from a single-digit image of the MNIST dataset to utilize themselves as inputs – for example, 4 fragments of size 14 × 14 or 9 fragments of size 9 × 9 are created from a single image. Similarly, the image included in the CIFAR-10 dataset, which contains one of various objects, e.g., birds and cats, is split to multiple patches, and then is used to the experiments on sorting image fragments. See Table 3 for the details of the image fragments and their sizes.
1Thanks to many open-source projects, we can easily run the baseline methods. However, it is difficult to reproduce some results due to unknown random seeds. For this reason, we bring the results from the work (Petersen et al., 2022), and use fixed random seeds, i.e., 42, 84, 126, 168, 210, for our methods.
7


Published as a conference paper at ICLR 2024
Experimental Details. Similar to the experiments on sorting multi-digit images, an optimal monotonic sigmoid function is used as DSFs. Since the size of inputs is much smaller than the experiments on sorting multi-digit images, shown in Section 5.1, we modify the architectures of the CNNs and the Transformer-based models. We reduce the kernel size of convolution layers from 5 to 3 and make strides 2. Due to the small input sizes, we omit the results by the Transformer-Large model for these experiments. Additionally, max-pooling operations are removed. Similar to the experiments in Section 5.1, we use the AdamW optimizer (Loshchilov & Hutter, 2018). Moreover, each model is trained for 50,000 steps when the number of fragments is 2 × 2, i.e., when a sequence length is 4, and 100,000 steps for 3 × 3 fragments, i.e., when a sequence length is 9. Additional information including the details of neural architectures can be found in Sections I and J.
Results. Table 3 represents the experimental results on both datasets of image fragments, which are created from the MNIST and CIFAR-10 datasets. Similar to the experiments on sorting multidigit images, the more powerful architecture improves performance in this task.
According to the experimental results, we achieve satisfactory performance by applying the errorfree DSFs, combined loss, and Transformer-based models with multi-head attention. We provide detailed discussion on how they contribute to the performance gains in Section 7, and empirical studies on steepness, learning rate, and a balancing hyperparameter in Section 7 and the appendices.
6 RELATED WORK
Differentiable Sorting Algorithms. To allow us to differentiate a sorting algorithm, Grover et al. (2019) have proposed the continuous relaxation of argsort operator, which is named NeuralSort. In this work, the output of NeuralSort only satisfies the row-stochastic matrix property, although Grover et al. (2019) attempt to employ a gradient-based optimization strategy in learning a neural sorting algorithm. Cuturi et al. (2019) propose a smoothed ranking and sorting operator using optimal transport, which is the natural relaxation for assignments. To reduce the cost of the optimal transport, the Sinkhorn algorithm (Cuturi, 2013) is used. Then, Blondel et al. (2020) have proposed a differentiable sorting and ranking operator with O(n log n) time and O(n) space complexities, which is named Fast Rank & Sort, by constructing differentiable operators as projections on permutahedron. Petersen et al. (2021) have suggested a differentiable sorting network with relaxed conditional swap functions. Recently, the same authors analyze the characteristics of the relaxation of monotonic conditional swap functions, and propose several monotonic swap functions, e.g., the Cauchy and optimal monotonic functions (Petersen et al., 2022).
Permutation-Equivariant Networks. A seminal architecture, long short-term memory (Hochreiter & Schmidhuber, 1997) can be used in modeling a sequence without any difficulty, and a sequence-to-sequence model (Sutskever et al., 2014) can be employed to cope with a sequence. However, as discussed in the work by Vinyals et al. (2016), an unordered sequence can have good orderings, by analyzing the effects of permutation thoroughly. Zaheer et al. (2017) propose a permutation-invariant or permutation-equivariant network, named Deep Sets, and prove the permutation invariance and permutation equivariance of the proposed models. By utilizing the Transformer network (Vaswani et al., 2017), Lee et al. (2019) have proposed a permutation-equivariant network.
7 DISCUSSION
Numerical Analysis on Our Methods and Their Hyperparameters. We carry out numerical analyses on the effects of our methods, compared to a baseline method, i.e., Diffsort with the optimal monotonic sigmoid function. As reported in Table 4, we demonstrate that our methods better sorting performance compared to the baseline, which implies that our suggestions are effective in the sorting tasks. In these experiments, we follow the settings of the experiments described in Section 5.1. Moreover, we present numerical analyses on a balancing hyperparameter, steepness, and a learning rate in Sections K and L.
Analysis on Performance Gains. According to the results in Sections 5 and 7, we can argue that the error-free DSFs, our proposed loss, and the Transformer-based models contribute to better
8


Published as a conference paper at ICLR 2024
Table 4: Study on comparisons of Diffsort with the optimal monotonic sigmoid function and our methods in the experiments on sorting the four-digit MNIST dataset.
Method Model Sequence Length
3 5 7 9 15 32
Diffsort CNN 94.6 (96.3) 85.0 (93.3) 73.6 (90.7) 62.2 (88.5) 31.8 (82.3) 1.4 (67.9)
Ours 95.2 (96.7) 87.2 (94.2) 76.6 (91.6) 64.8 (89.2) 34.7 (83.3) 2.1 (69.2)
Diffsort Transformer-S 95.9 (97.1) 90.2 (95.4) 83.9 (94.2) 77.2 (92.9) 57.3 (89.7) 16.3 (81.7)
Ours 95.9 (97.1) 94.8 (97.5) 90.8 (96.5) 86.9 (95.7) 74.3 (93.6) 37.8 (87.7)
Diffsort Transformer-L 96.5 (97.5) 92.6 (96.4) 87.6 (95.3) 82.6 (94.3) 67.8 (92.0) 32.1 (85.7)
Ours 96.5 (97.5) 95.4 (97.7) 92.9 (97.2) 90.1 (96.5) 82.5 (95.0) 46.2 (88.9)
performance considerably compared to the baseline methods. As shown in Tables 1 and 4, the performance gains by the Transformer-based models are more substantial than the gains by the errorfree DSFs and our loss, since multi-head attention is effective for capturing long-term dependency (or dependency between multiple instances in our case) and reducing inductive biases. However, as will be discussed in the following, the hard permutation matrices can be used in the case that does not allow us to mix instances in X, e.g., sorting image fragments in Section 5.2.
Utilization of Hard Permutation Matrices. While the use of a soft permutation matrix Psoft makes given instances mixed, a hard permutation matrix Phard is instrumental in applying Phard
in a problem that requires swapping given instances exactly. More precisely, each row of P⊤
softX
is a linear combination of some column of Psoft and X, but one of P⊤
hardX corresponds to an
exact row in X. This property can be used to preserve input instances from sorting operations. The experiments in Section 5.2 can be considered as one of such cases, and it exhibits the strength of our method, not only the performance in accem and accew.
Effects of Multi-Head Attention in the Problem (3). We follow the model architecture used in the previous work (Grover et al., 2019; Cuturi et al., 2019; Petersen et al., 2021; 2022) for the CNNs. However, as shown in Tables 1, 2, and 3, the model is not enough to show the best performance. In particular, whereas the model capacity, i.e., FLOPs and the number of parameters, of the Transformer-Small models is almost matched to or less than the capacity of the CNNs, the results by the Transformer-Small models outperform the results by the CNNs. We presume that these performance gains are derived from a multi-head attention’s ability to capture long-term dependency and reduce inductive biases, as widely stated in many recent studies in diverse fields such as natural language processing (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2021; Liu et al., 2021), and 3D vision (Nash et al., 2020; Zhao et al., 2021). Especially, unlike the instance-wise CNNs, our permutation-equivariant Transformer architecture utilizes self-attention for given instances, so that our model can productively compare instances in a sequence and effectively learn the relative relationship between them.
Further Study of Differentiable Sorting Algorithms. Differentiable sorting encourages us to train a mapping from an abstract input to an ordinal score using supervision on permutation matrices. However, this line of studies is limited to a sorting problem of high-dimensional data with clear ordering information, e.g., multi-digit numbers. As the further study of differentiable sorting, we can expand this framework to sort more ambiguous data, which contains implicitly ordinal information.
8 CONCLUSION
In this paper, we defined a softening error, induced by a monotonic DSF, and demonstrated several evidences of the error accumulation problem. To resolve the error accumulation problem, an errorfree DSF is proposed, inspired by a straight-through estimator. Moreover, we provided the simple theoretical and empirical analyses that our error-free DSF successfully achieves a zero error and also holds a non-decreasing condition and differentiability. By combining all components, we suggested a generalized neural sorting network with the error-free DSF and multi-head attention. Finally, we showed that our methods are better than or comparable to other algorithms in diverse benchmarks.
9


Published as a conference paper at ICLR 2024
ETHICS STATEMENT
As discussed in Section 7, the hard permutation matrices produced by our methods allow us to swap instances exactly, not the linear combination of instances. This characteristic is required when we are given the final outcomes of sorting as supervision. This scenario is tested by the experiments presented in Section 5.2. In these experiments, we are supposed that original images are provided as supervision. Building on the advantages of neural network-based sorting networks, we expand their practical significance to the cases that need hard permutation matrices. On the other hand, the nature of neural sorting networks may yield a potential negative societal impact. If this line of research including our approaches is employed to sort controversial high-dimensional data such as beauty and intelligence, it can be considered as the unethical use cases of artificial intelligence.
ACKNOWLEDGMENTS
This work was supported by the IITP grants (2022-0-00290: Visual Intelligence for Space-Time Understanding and Generation based on Multi-layered Visual Common Sense, 2022-0-00264: Comprehensive Video Understanding and Generation with Knowledge-based Deep Logic Neural Network) funded by Ministry of Science and ICT, Republic of Korea.
REFERENCES
M. Ajtai, J. Komlo ́s, and E. Szemere ́di. An O(n log n) sorting network. In Proceedings of the Annual ACM Symposium on Theory of Computing (STOC), pp. 1–9, Boston, Massachusetts, USA, 1983.
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
Y. Bengio, N. Le ́onard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
L. Berrada, A. Zisserman, and M. P. Mudigonda. Smooth loss functions for deep top-k classification. In Proceedings of the International Conference on Learning Representations (ICLR), Vancouver, British Columbia, Canada, 2018.
M. Blondel, O. Teboul, Q. Berthet, and J. Djolonga. Fast differentiable sorting and ranking. In Proceedings of the International Conference on Machine Learning (ICML), pp. 950–959, Virtual, 2020.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1877–1901, Virtual, 2020.
Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the International Conference on Machine Learning (ICML), pp. 129–136, Corvallis, Oregon, USA, 2007.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to algorithms. MIT Press, 4 edition, 2022.
M. Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), volume 26, pp. 2292–2300, Lake Tahoe, Nevada, USA, 2013.
M. Cuturi, O. Teboul, and J.-P. Vert. Differentiable ranking and sorting using optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, Vancouver, British Columbia, Canada, 2019.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
10


Published as a conference paper at ICLR 2024
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), Virtual, 2021.
A. Grover, E. Wang, A. Zweig, and S. Ermon. Stochastic optimization of sorting networks via continuous relaxations. In Proceedings of the International Conference on Learning Representations (ICLR), New Orleans, Louisiana, USA, 2019.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, Las Vegas, Nevada, USA, 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.
D. E. Knuth. The art of computer programming, volume 3. Addison-Wesley Professional, 2 edition, 1998.
A. Krizhevsky and G. E. Hinton. Learning multiple layers of features from tiny images. Technical report, Computer Science Department, University of Toronto, 2009.
Y. LeCun, C. Cortes, and C. J. C. Burges. The MNIST database of handwritten digits. http: //yann.lecun.com/exdb/mnist/, 1998.
J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and Y. W. Teh. Set Transformer: A framework for attention-based permutation-invariant neural networks. In Proceedings of the International Conference on Machine Learning (ICML), pp. 3744–3753, Long Beach, California, USA, 2019.
T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval, 3(3):225–331, 2009.
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the International Conference on Computer Vision (ICCV), pp. 10012–10022, Virtual, 2021.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations (ICLR), Vancouver, British Columbia, Canada, 2018.
G. E. Mena, D. Belanger, S. Linderman, and J. Snoek. Learning latent permutations with GumbelSinkhorn networks. In Proceedings of the International Conference on Learning Representations (ICLR), Vancouver, British Columbia, Canada, 2018.
C. Nash, Y. Ganin, S. M. A. Eslami, and P. W. Battaglia. PolyGen: An autoregressive generative model of 3D meshes. In Proceedings of the International Conference on Machine Learning (ICML), pp. 7220–7229, Virtual, 2020.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In Neural Information Processing Systems Workshop on Deep Learning and Unsupervised Feature Learning, Granada, Spain, 2011.
F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen. Differentiable sorting networks for scalable sorting and ranking supervision. In Proceedings of the International Conference on Machine Learning (ICML), pp. 8546–8555, Virtual, 2021.
F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen. Monotonic differentiable sorting networks. In Proceedings of the International Conference on Learning Representations (ICLR), Virtual, 2022.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NeurIPS), volume 27, Montreal, Quebec, Canada, 2014.
11


Published as a conference paper at ICLR 2024
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, pp. 5998–6008, Long Beach, California, USA, 2017.
O. Vinyals, S. Bengio, and M. Kudlur. Order matters: Sequence to sequence for sets. In Proceedings of the International Conference on Learning Representations (ICLR), San Juan, Puerto Rico, 2016.
M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep sets. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, pp. 3391–3401, Long Beach, California, USA, 2017.
H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun. Point transformer. In Proceedings of the International Conference on Computer Vision (ICCV), pp. 16259–16268, Virtual, 2021.
12


Published as a conference paper at ICLR 2024
A OPTIMAL MONOTONIC SIGMOID FUNCTIONS
−15 −10 −5 0 5 10 15
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: An optimal monotonic sigmoid function, which is presented in (7).
We visualize an optimal monotonic sigmoid function in Figure 4.
B COMPARISONS OF DIFFERENTIABLE SWAP FUNCTIONS
10 20 30 40 50 #Swaps
0
1
2
3
4
maxk(x, y) − mink(x, y)
Logistic Logistic w/ ART Reciprocal Cauchy Optimal Error-Free DSF
50 100 150 200 250 #Swaps
0
2
4
6
8
maxk(x, y) − mink(x, y)
Figure 5: Comparisons of diverse DSFs in terms of the numbers of swap functions applied. Our error-free DSF does not change the original x and y, unlike other DSFs. We initially set x = 4, y = 0 for the left panel or x = 8, y = 0 for the right panel, where k = #Swaps.
As depicted in Figure 5, some sigmoid functions such as the logistic, logistic with ART, reciprocal, Cauchy, and optimal monotonic functions suffer from the error accumulation problem; see the work (Petersen et al., 2022) for the details of such sigmoid functions. For the case of the Cauchy function, two values are close enough at the 9th step in the left panel of Figure 5 and the 15th step in the right panel of Figure 5; we calculate the corresponding steps where a difference between two values becomes smaller than 0.001.
C COMPARISONS OF DIFFERENT SORTING NETWORKS
Figure 6 shows the comparisons of different sorting networks by varying sequence lengths. accem and accew are measured to assess the sorting networks.
D PROOF OF PROPOSITION 1
Proof. If two elements at indices i and j are swapped by a single swap function, [P]kk = 1 for k ∈ [n]\{i, j}, [P]kl = 0 for k ̸= l, k, l ∈ [n]\{i, j}, [P]ii = [P]jj = p, and [P]ij = [P]ji = 1 − p, where p is the output of a sigmoid function, i.e., p = σ(y − x) or p = ⌊σ(y − x)⌉. Since the multiplication of doubly-stochastic matrices is still doubly-stochastic, Proposition 1 is true.
13


Published as a conference paper at ICLR 2024
5 10 15 20 25 30 n
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
Logistic Logistic w/ ART Reciprocal Cauchy Optimal Error-Free DSF
Figure 6: accem (solid) and accew (dashed), versus sequence lengths. Without a mapping g from x to s, we uniformly sample n elements from [−10, 10], which are considered as ordinal scores s. Then, we sort them using the respective sorting networks and measure accuracy over the groundtruth permutation matrices computed by n elements. We repeat each experiment 10,000 times.
E PROOF OF PROPOSITION 2
Proof. Let mink(x, y) and mink(x, y) be minimum and maximum values where swap with min
and max is applied k times repeatedly. By Definition 1 and mini < mini+1 and maxi+1 < maxi, the following inequalities are satisfied:
min(x, y) < min1(x, y) < min2(x, y) < · · · < mink(x, y)
≤ maxk(x, y) < · · · < max2(x, y) < max1(x, y) < max(x, y), (21)
under the assumption that ∇xσ(x) > 0. By (21) and ∇xσ(x) > 0, we can obtain the following inequality:
0 ≤ maxk+1(x, y) − mink+1(x, y) < maxk(x, y) − mink(x, y) < maxk−1(x, y) − mink−1(x, y). (22) Therefore, limk→∞ maxk(x, y) − mink(x, y) = 0, and mink(x, y) = maxk(x, y) if k → ∞. To sum up, a softening error for k → ∞ is (max(x, y) − min(x, y))/2 since maxk(x, y) = (min(x, y) + max(x, y))/2 by (9). Note that the assumption ∇xσ(x) > 0 implies that σ(·) is a strictly monotonic sigmoid function.
F PROOF OF PROPOSITION 3
Proof. According to Definition 1, given x and y, the softening error x′ − min(x, y) is expressed as the following:
x′ − min(x, y) = min(x, y) − min(x, y) sg + min(x, y) − min(x, y)
= min(x, y) − min(x, y) + min(x, y) − min(x, y)
= 0, (23)
while a forward pass is applied. The proof for max(x, y) − y′ is omitted because it is obvious.
G DETAILS OF PERMUTATION-EQUIVARIANT NETWORKS WITH
MULTI-HEAD ATTENTION
Figure 7 illustrates the Transformer-based permutation-equivariant network, which is implemented with multi-head attention (Vaswani et al., 2017). For the sake of brevity, this illustration briefly depicts our permutation-equivariant network without detailing the specifics of the Transformer network. Each instance in a sequence is first processed by a feature extractor, i.e., a convolutional neural network. Then, a sequence of latent vectors is provided into the Transformer network without positional encoding. At each multi-head attention module, each latent vector is updated by the aggregation of the latent vectors given where the aggregation is determined by the operations explained in Section 4 and (16). After passing through multiple layers of multi-head attention and the
14


Published as a conference paper at ICLR 2024
Figure 7: Simple illustration of our permutation-equivariant network with multi-head attention. For the sake of brevity, we do not depict the details of multi-head attention, layer normalization, and feed-forward networks. Note that FE stands for a feature extractor.
corresponding components such as layer normalization and feed-forward neural networks, the final fully-connected layer is applied to transform the outputs of the Transformer network into a score vector s. The details of the Transformer network can be found in the work by Vaswani et al. (2017).
H SPLIT STRATEGY TO REDUCE THE NUMBER OF POSSIBLE PERMUTATIONS
Figure 8: A split process of P.
As presented in (14) and Proposition 1, the permutation matrix for the error-free DSF is a discretized doubly-stochastic matrix, which is denoted as Phard, in a forward pass, and is differentiable in a backward pass. Here, we show an interesting proposition of Phard:
Proposition 4. Let s ∈ Rn and Phard ∈ Rn×n be an unordered sequence and the corresponding permutation matrix to transform it to so, respectively. We are able to split s to two subsequences
s1 ∈ Rn1 and s2 ∈ Rn2 where s1 = [s]1:n1 and s2 = [s]n1+1:n1+n2 . Then, Phard is also split to
P1 ∈ Rn1×n1 and P2 ∈ Rn2×n2 , so that P1 and P2 are (discretized) doubly-stochastic.
Proof. A split does not change the relative order of elements in the same split and each entry in the permutation matrix is zero or one, so that a permutation matrix can be split as shown in Figure 8. Moreover, multiple splits are straightforwardly doable.
In contrast to Phard, it is impossible to split Psoft to sub-block matrices since such sub-block matrices cannot satisfy the property of doubly-stochastic matrix, which is discussed in Proposition 1. Importantly, Proposition 4 does not show a possibility of the recoverable decomposition of the permutation matrix, which implies that we cannot guarantee the recovery of decomposed matrices to
15


Published as a conference paper at ICLR 2024
the original matrix. Regardless of the existence of recoverable decomposition, we attempt to reduce the number of possible permutations with sub-block matrices, rather than holding the large number of possible permutations with the original permutation matrix. Therefore, by Proposition 4, relative relationships between instances with a smaller number of possible permutations are more distinctively learnable than the relationships with a larger number of possible permutations, preventing a sparse correct permutation among a large number of possible permutations.
I DETAILS OF ARCHITECTURES
We describe the details of the neural architectures used in our paper, as shown in Tables 5, 6, 7, 8, 9, 10, 11, 12, 13, and 14. For the experiments on sorting image fragments, we omit some of the architectures employed for particular fragmentation, since they follow the same architectures presented in Tables 11, 12, 13, and 14. Only differences are the sizes of inputs, and therefore the respective sizes of the first fully-connected layers change.
Table 5: Architecture of the convolutional neural networks for the four-digit MNIST dataset.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 1 × 32 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 32 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Fully-connected 12544 × 64 – ReLU – – Fully-connected 64 × 1 – 
Table 6: Architecture of the Transformer-Small models for the four-digit MNIST dataset.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 1 × 32 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 32 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Fully-connected 12544 × 16 – Transformer Encoder 16 × 16 #layers 6, #heads 8 ReLU – – Fully-connected 16 × 1 – 
Table 7: Architecture of the Transformer-Large models for the four-digit MNIST dataset.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 1 × 32 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 32 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Fully-connected 12544 × 64 – Transformer Encoder 64 × 64 #layers 8, #heads 8 ReLU – – Fully-connected 64 × 1 – 
J DETAILS OF EXPERIMENTS
As described in the main article, we use three public datasets: MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), and CIFAR-10 (Krizhevsky & Hinton, 2009). Unless otherwise spec
16


Published as a conference paper at ICLR 2024
ified, a learning rate 10−3.5 is used for the CNN architectures and a learning rate 10−4 is used for the Transformer-based architectures; see our implementation for the exact learning rates we utilize in the experiments. Learning rate decay is applied by multiplying 0.5 in every 50,000 steps for the experiments on sorting multi-digit images and every 20,000 steps for the experiments on sorting image fragments. Moreover, we balance two objectives for Phard and Psoft by multiplying 1, 0.1, 0.01, or 0.001; see our implementation for the respective values for all the experiments. For random seeds, we pick five random seeds 42, 84, 126, 168, and 210 for all the experiments; these values are picked without any trials. Other missing details can be found in our implementation. Furthermore, we employ several commercial NVIDIA GPUs, i.e., GeForce GTX Titan Xp, GeForce RTX 2080, and GeForce RTX 3090, in the experiments.
K STUDY ON BALANCING HYPERPARAMETER
We conduct a study on a balancing hyperparameter in the experiments on sorting the four-digit MNIST dataset, as shown in Table 15. For these experiments, we use steepness 2, 14, 23, 38, 25, and 124 for sequence lengths 3, 5, 7, 9, 15, and 32, respectively. Also, we use a learning rate 10−3 and 5 random seeds 42, 84, 126, 168, and 210.
L STUDY ON STEEPNESS AND LEARNING RATE
We present studies on steepness and learning rate for the experiments on sorting the multi-digit MNIST dataset, as shown in Tables 16, 17, 18, 19, 20, and 21. For these experiments, a random seed 42 is only used due to numerous experimental settings. Also, we use balancing hyperparameters λ as 1.0, 1.0, 0.1, 0.1, 0.1, and 0.1 for sequence lengths 3, 5, 7, 9, 15, and 32, respectively. Since there are many configurations of steepness, learning rate, and a balancing hyperparameter, we cannot include all the configurations here. The final configurations we use in the experiments are described in our implementation. As widely known in the deep learning community, a learning rate should be set as a value around 10−3. Moreover, according to our empirical analyses, steepness should generally be higher as a sequence length is longer.
M LIMITATIONS
While a sorting task is one of the most significant problems in computer science and mathematics (Cormen et al., 2022), our ideas, which are built on sorting networks (Knuth, 1998; Ajtai et al., 1983), can be limited to sorting algorithms. It implies that it is not easy to devise neural networkbased approaches to solving general problems in computer science, e.g., combinatorial optimization, which are inspired by our ideas.
In addition, while our proposed methods show the superior performance compared to the baseline methods, this line of research suffers from performance degradation for longer sequences as shown in Tables 1, 2, and 3. More precisely, for longer sequences, the element-wise accuracy does not decline dramatically, but the sequence-wise accuracy significantly drops due to the nature of sequences. Incorporating our contributions such as the error-free DSFs and the Transformer-based networks, we expect that the further progress of neural network-based sorting networks can be achieved. In particular, the consideration of more sophisticated neural networks, which are capable of handling longer sequences, might help improve performance. This will be left for future work.
Our frameworks successfully learn relationships between high-dimensional data with ordinal contents as shown in Section 5. However, we suppose that our methods might fail in sorting data without ordinal information; the elaborate discussion on this topic can be found in Section 7. In order to sort more ambiguous high-dimensional data, we can combine our work with part-based or segmentation-based approaches.
17


Published as a conference paper at ICLR 2024
Table 8: Architecture of the convolutional neural networks for the SVHN dataset.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 3 × 32 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 32 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 64 × 128 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 128 × 256 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Fully-connected 2304 × 64 – ReLU – – Fully-connected 64 × 1 – 
Table 9: Architecture of the Transformer-Small models for the SVHN dataset.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 3 × 32 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 32 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 64 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 64 × 128 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Fully-connected 1152 × 32 – Transformer Encoder 32 × 32 #layers 6, #heads 8 ReLU – – Fully-connected 32 × 1 – 
Table 10: Architecture of the Transformer-Large models for the SVHN dataset.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 3 × 32 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 32 × 64 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 64 × 128 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Convolutional 128 × 256 5 × 5 strides 1, padding 2 ReLU – – Max-pooling – – pooling 2, strides 2 Fully-connected 2304 × 64 – Transformer Encoder 64 × 64 #layers 8, #heads 8 ReLU – – Fully-connected 64 × 1 – 
18


Published as a conference paper at ICLR 2024
Table 11: Architecture of the convolutional neural networks for the MNIST dataset of 4 image fragments of size 14 × 14.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 1 × 32 3 × 3 strides 2, padding 1 ReLU – – Convolutional 32 × 64 3 × 3 strides 2, padding 1 ReLU – – Fully-connected 1024 × 64 – ReLU – – Fully-connected 64 × 1 – 
Table 12: Architecture of the Transformer models for the MNIST dataset of 4 image fragments of size 14 × 14.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 1 × 32 3 × 3 strides 2, padding 1 ReLU – – Convolutional 32 × 32 3 × 3 strides 2, padding 1 ReLU – – Fully-connected 512 × 16 – Transformer Encoder 16 × 16 #layers 1, #heads 8 ReLU – – Fully-connected 16 × 1 – 
Table 13: Architecture of the convolutional neural networks for the CIFAR-10 dataset of 4 image fragments of size 16 × 16.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 3 × 32 3 × 3 strides 2, padding 1 ReLU – – Convolutional 32 × 64 3 × 3 strides 2, padding 1 ReLU – – Fully-connected 1024 × 64 – ReLU – – Fully-connected 64 × 1 – 
Table 14: Architecture of the Transformer models for the CIFAR-10 dataset of 4 image fragments of size 16 × 16.
Layer Input & Output (Channel) Dimensions Kernel Size Details
Convolutional 3 × 32 3 × 3 strides 2, padding 1 ReLU – – Convolutional 32 × 32 3 × 3 strides 2, padding 1 ReLU – – Fully-connected 512 × 16 – Transformer Encoder 16 × 16 #layers 1, #heads 8 ReLU – – Fully-connected 16 × 1 – 
Table 15: Study on a balancing hyperparameter λ in the experiments on sorting the four-digit MNIST dataset.
λ Sequence Length
3 5 7 9 15 32
1.000 94.8 (96.4) 86.9 (94.1) 74.2 (90.9) 62.6 (88.6) 12.0 (69.4) 0.0 (38.9) 0.100 94.9 (96.5) 87.2 (94.2) 75.3 (91.3) 64.7 (89.2) 34.7 (83.3) 2.1 (69.2) 0.010 95.1 (96.6) 87.1 (92.7) 75.2 (91.2) 63.5 (88.8) 33.2 (82.7) 0.7 (60.7) 0.001 95.2 (96.7) 87.0 (94.1) 76.6 (91.6) 64.8 (89.2) 32.9 (82.7) 0.5 (61.5) 0.000 94.9 (96.5) 87.2 (94.2) 75.9 (91.5) 64.4 (89.1) 34.1 (83.2) 0.9 (60.6)
19


Published as a conference paper at ICLR 2024
Table 16: Study on steepness and learning rate for a sequence length 3. lr stands for learning rate.
log10 lr Steepness
2 4 6 8 10 12 14
-4.0 89.2 (92.6) 91.5 (94.2) 92.1 (94.6) 92.3 (94.7) 92.7 (95.0) 92.1 (94.6) 92.7 (95.0) -3.5 93.6 (95.5) 93.4 (95.5) 94.5 (96.2) 93.9 (95.9) 94.4 (96.1) 94.5 (96.2) 94.6 (96.3) -3.0 95.3 (96.8) 95.1 (96.6) 95.0 (96.5) 94.4 (96.2) 94.3 (96.1) 94.7 (96.4) 94.8 (96.5) -2.5 94.5 (96.2) 94.3 (96.1) 93.7 (95.6) 93.8 (95.7) 93.7 (95.7) 93.7 (95.6) 94.1 (95.9)
Table 17: Study on steepness and learning rate for a sequence length 5. lr stands for learning rate.
log10 lr Steepness
14 16 18 20 22 24 26
-4.0 78.3 (90.2) 76.3 (89.3) 79.5 (90.9) 78.7 (90.4) 78.4 (90.4) 77.3 (89.8) 79.0 (90.6) -3.5 85.1 (93.3) 83.6 (92.7) 84.8 (93.1) 85.5 (93.5) 83.4 (92.5) 85.6 (93.6) 84.1 (92.8) -3.0 86.9 (94.1) 85.2 (93.3) 86.2 (93.8) 85.7 (93.6) 85.4 (93.5) 85.0 (93.2) 85.0 (93.3) -2.5 83.1 (92.3) 83.2 (92.4) 83.1 (92.4) 81.7 (91.8) 83.3 (92.5) 82.4 (92.1) 82.5 (92.1)
Table 18: Study on steepness and learning rate for a sequence length 7. lr stands for learning rate.
log10 lr Steepness
23 25 27 29 31 33 35
-4.0 53.5 (82.9) 57.3 (84.6) 61.0 (86.0) 58.3 (85.1) 66.1 (88.0) 57.3 (84.6) 61.6 (86.4) -3.5 71.6 (90.0) 73.5 (90.8) 68.1 (88.6) 72.9 (90.5) 72.2 (90.2) 71.4 (89.9) 74.2 (91.0) -3.0 74.4 (90.9) 72.8 (90.4) 74.2 (90.8) 69.2 (89.0) 69.9 (89.3) 73.0 (90.4) 73.0 (90.5) -2.5 68.0 (88.6) 67.2 (88.1) 68.1 (88.6) 64.5 (86.9) 66.8 (88.0) 70.4 (89.3) 66.2 (87.9)
Table 19: Study on steepness and learning rate for a sequence length 9. lr stands for learning rate.
log10 lr Steepness
26 28 30 32 34 36 38
-4.0 34.0 (77.7) 37.8 (79.7) 37.3 (79.4) 45.9 (83.0) 46.4 (83.1) 36.2 (78.9) 45.3 (82.5) -3.5 51.8 (84.8) 59.7 (87.5) 58.3 (87.5) 61.0 (88.2) 60.2 (88.1) 54.6 (85.8) 56.3 (86.6) -3.0 62.5 (88.8) 60.5 (88.0) 61.8 (88.4) 63.2 (88.9) 61.5 (88.2) 62.2 (88.6) 65.0 (89.5) -2.5 57.4 (86.8) 58.7 (87.2) 56.5 (86.4) 55.3 (86.1) 52.8 (85.0) 46.9 (82.1) 52.1 (84.6)
Table 20: Study on steepness and learning rate for a sequence length 15. lr stands for learning rate.
log10 lr Steepness
19 21 23 25 27 29 31
-4.0 3.6 (62.7) 7.2 (67.6) 4.9 (64.8) 2.6 (60.9) 2.9 (61.5) 2.5 (60.2) 6.8 (68.3) -3.5 10.3 (71.5) 11.9 (73.0) 7.6 (68.6) 22.3 (78.7) 7.1 (68.7) 8.2 (69.6) 10.6 (71.8) -3.0 24.2 (79.8) 24.1 (80.1) 29.7 (81.9) 32.1 (82.7) 23.9 (79.9) 31.6 (82.1) 31.0 (82.0) -2.5 30.5 (81.4) 28.2 (80.0) 18.9 (76.8) 29.8 (80.9) 19.8 (77.8) 15.4 (75.4) 24.6 (79.6)
Table 21: Study on steepness and learning rate for a sequence length 32. lr stands for learning rate.
log10 lr Steepness
118 120 122 124 126 128 130
-4.0 0.0 (46.4) 0.0 (45.8) 0.0 (46.4) 0.0 (43.1) 0.0 (42.9) 0.0 (49.4) 0.0 (48.9) -3.5 0.2 (60.5) 0.3 (61.8) 0.7 (64.8) 0.3 (62.7) 0.0 (56.0) 0.3 (62.7) 0.2 (59.8) -3.0 0.6 (63.1) 0.5 (63.6) 0.4 (59.8) 0.8 (65.8) 0.2 (58.1) 0.1 (56.0) 0.1 (57.1) -2.5 0.5 (62.8) 0.5 (62.3) 0.5 (62.5) 0.1 (58.3) 0.3 (61.2) 0.1 (59.5) 0.2 (58.1)
20


Published as a conference paper at ICLR 2024
N ADDITIONAL DISCUSSION
It is challenging to directly sort a sequence of generic data instances without using auxiliary networks and explicit supervision. Unlike earlier sorting methods, this sorting network-based research (Petersen et al., 2021; 2022) including our work ensures that we can train a neural network that predicts numerical scores and eventually sorts them, even though we do not necessitate accessing explicit supervision such as exact numerical values of the contents in high-dimensional data. In this sense, the practical significance of our proposed methods can be highlighted by offering this possibility of solving a sorting problem with high-dimensional inputs. For example, as shown in Section 5, we can compare images of street view house numbers using the sorting network where our neural network is trained without exact house numbers.
Moreover, instead of using costly supervision, our networks allow us to sort high-dimensional instances in a sequence where information on comparisons between instances is only given. This scenario often occurs when we cannot obtain complete supervision. For example, if we would sort four-digit MNIST images, ordinary neural networks are designed to solve a classification task by predicting class probabilities each of which indicates one of all labels from “0000” to “9999”. If some labels are missing and further we do not know the exact number of labels, they might fail in predicting unseen data corresponding to those labels. Unlike these methods, it is possible to solve sorting problems using our networks in such a scenario.
Furthermore, this study can be applied in diverse deep learning tasks for learning to sort generic high-dimensional data, such as information retrieval (Cao et al., 2007; Liu, 2009) and top-k classification (Berrada et al., 2018).
21