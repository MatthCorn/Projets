\chapter{Dépliage du problème : une approche événementielle}

\section{Plan}
\begin{itemize}
\item Motivation
\item Formalisation du problème
\item Visualisation (vraiment utile)
\item Présentation CAID
\item Extension des essaies
	\begin{itemize}
	\item pré-traitement
	\item architecture
	\item fonction de perte
	\item résultats
	\end{itemize}
\item Conclusion
\end{itemize}
\; \\

Ce chapitre introduit une méthodologie alternative pour le traitement des signaux radar. En rupture avec les approches globales de type \textit{sequence-to-sequence} (Seq2Seq) abordées précédemment, nous proposons ici une reformulation du problème axée sur la causalité et le traitement flux-à-flux (\textit{stream processing}). Cette approche vise à s'affranchir des limitations de longueur de séquence tout en mimant plus fidèlement le comportement physique du système de Détection et Caractérisation des Impulsions (DCI).
\section{Motivation : Du traitement de séquence à la modélisation dynamique}
L'approche classique consistant à traiter l'intégralité d'une séquence d'impulsions en une seule passe se heurte à deux obstacles majeurs : un goulot d'étranglement computationnel et une dissonance conceptuelle avec le système réel.
\subsection{Le goulot d'étranglement des architectures Seq2Seq}
Le premier obstacle est la difficulté intrinsèque des architectures neuronales actuelles à maintenir des dépendances temporelles sur de longs horizons :
\begin{itemize}
\item \textbf{Architectures récurrentes (RNN/LSTM) :} Ces modèles peinent à conserver une mémoire contextuelle pertinente au-delà d'une dizaine d'éléments. La propagation du gradient à travers le temps (Backpropagation Through Time) sur de longues séquences entraîne des problèmes de disparition ou d'explosion du gradient, rendant l'apprentissage instable.
\item \textbf{Architectures attentionnelles (Transformers) :} Bien que plus robustes, elles présentent une complexité algorithmique quadratique $\mathcal{O}(N^2)$ en fonction de la longueur de la séquence. Au-delà de séquences de l'ordre de 1000 éléments, l'empreinte mémoire devient prohibitive.
\end{itemize}
Ces limitations imposent souvent des mécanismes de segmentation arbitraires (découpage de la scène en sous-blocs) qui brisent la continuité temporelle du signal et nuisent à la cohérence globale de la détection.
\subsection{Le DCI comme système dynamique réactif}
Le second obstacle est d'ordre conceptuel. Modéliser le DCI comme une fonction globale $f(\mathbf{X}_{total}) \to \mathbf{Y}_{total}$ est une approximation qui masque la réalité physique du module. Le DCI ne "voit" pas le futur ; il fonctionne intrinsèquement en mode événementiel.
C'est un système dynamique réactif régis par deux types d'événements :
\begin{enumerate}
\item \textbf{L'arrivée d'une impulsion incidente} déclenche une mise à jour des corrélations et de l'état des mémoires internes.
\item \textbf{L'émission d'une impulsion synthétisée} par le mécanisme de suivi provoque la libération de ressources mémoires.
\end{enumerate}
Il apparaît donc plus naturel de chercher à modéliser ce comportement "pas-à-pas". L'objectif est de transformer le problème de prédiction globale en un problème de prédiction locale causale : étant donné une impulsion incidente, le modèle doit prédire la cascade d'événements (émissions) qui surviennent avant l'arrivée de l'impulsion incidente suivante.

\section{Formalisation du problème "Flux-à-flux"}
Pour implémenter cette approche avec un réseau de neurones, nous devons transformer la structure des données. Cette transformation résulte de la confrontation entre la nature continue du flux radar et la nature discrète des réseaux de neurones. Nous détaillons ici les choix de conception nécessaires pour résoudre ce conflit.
\subsection{Contraintes architecturales et choix de conception}
Un réseau de neurones standard est une fonction $f: \mathcal{X} \to \mathcal{Y}$ qui associe un tenseur de sortie unique à un tenseur d'entrée unique. Cette définition rigide nous impose plusieurs contraintes pour le traitement d'un flux d'impulsions incidentes $\{I_k\}$ et d'impulsions émises (sorties) $\{O_k\}$ :
\paragraph{Contrainte 1 : Discrétisation de la réponse}
Le modèle ne peut pas "naturellement" générer un nombre variable d'impulsions de sortie pour une seule impulsion d'entrée sans modifier profondément son architecture.
\begin{itemize}
\item \textit{Choix de conception :} Nous adoptons une approche itérative. Nous interrogeons le réseau de manière répétée avec la même impulsion incidente tant qu'il reste des impulsions à générer pour ce créneau temporel.
\end{itemize}
\paragraph{Contrainte 2 : Condition d'arrêt}
Puisque le nombre de sorties est variable (potentiellement nul), le modèle doit posséder un moyen explicite de signaler la fin de la séquence d'émission associée à l'impulsion incidente courante.
\begin{itemize}
\item \textit{Choix de conception :} Nous introduisons un jeton de contrôle spécial, noté \texttt{NEXT}. La prédiction de ce jeton par le réseau signifie "Il n'y a plus d'émission à générer, passez à l'impulsion incidente suivante".
\end{itemize}
\paragraph{Contrainte 3 : Maintien du contexte}
Le modèle doit savoir où il se situe dans la séquence des émissions (doit-il prédire la première ou la troisième impulsion associée à $I_k$ ?).
\begin{itemize}
\item \textit{Choix de conception :} Nous optons pour une architecture à double entrée. Le modèle reçoit à chaque pas :
\begin{enumerate}
\item L'impulsion incidente courante $I_k$.
\item La dernière prédiction effectuée $P_{t-1}$ (ou un jeton d'initialisation).
\end{enumerate}
\end{itemize}
Ce dernier choix s'apparente à une approche auto-régressive guidée (Teacher Forcing), soulageant la mémoire interne du modèle en lui rappelant explicitement son état précédent.
\section{Protocole de transformation des données}
La combinaison de ces choix aboutit à un algorithme de "dépliage" des données. Une séquence temporelle d'événements est transformée en une série de couples $((\text{Entrées}), \text{Cible})$ indépendants.
\subsection{Illustration du mécanisme}
Considérons une chronologie mixte d'événements réels, triés par temps, impliquant 3 impulsions incidentes ($I$) et 5 impulsions émises ($O$) :
\begin{equation}
\text{Séquence Globale} : [I_1, \mathbf{O_1}, \mathbf{O_2}, \mathbf{O_3}, I_2, I_3, \mathbf{O_4}, \mathbf{O_5}]
\end{equation}
L'objectif est de prédire les événements en gras ($O$) en fonction des événements incidents ($I$). Le jeton \texttt{NEXT} est utilisé pour initialiser la boucle (entrée) et pour la clore (cible).
Le processus de génération des données d'entraînement se déroule comme suit :
\begin{enumerate}
\item \textbf{Traitement de $I_1$ (3 émissions associées) :}
\begin{itemize}
\item \textit{Entrée :} $(I_1, \texttt{NEXT})$ $\rightarrow$ \textit{Cible :} $O_1$ (Première émission).
\item \textit{Entrée :} $(I_1, O_1)$ $\rightarrow$ \textit{Cible :} $O_2$ (Le modèle sait qu'il a émis $O_1$).
\item \textit{Entrée :} $(I_1, O_2)$ $\rightarrow$ \textit{Cible :} $O_3$ (Le modèle sait qu'il a émis $O_2$).
\item \textit{Entrée :} $(I_1, O_3)$ $\rightarrow$ \textit{Cible :} \texttt{NEXT} (Fin des émissions pour $I_1$).
\end{itemize}
\item \textbf{Traitement de $I_2$ (0 émission associée) :}
\begin{itemize}
    \item L'impulsion suivante $I_3$ arrive avant toute émission.
    \item \textit{Entrée :} $(I_2, \texttt{NEXT})$ $\rightarrow$ \textit{Cible :} \texttt{NEXT} (Passage immédiat à la suite).
\end{itemize}

\item \textbf{Traitement de $I_3$ (2 émissions associées) :}
\begin{itemize}
    \item \textit{Entrée :} $(I_3, \texttt{NEXT})$ $\rightarrow$ \textit{Cible :} $O_4$.
    \item \textit{Entrée :} $(I_3, O_4)$ $\rightarrow$ \textit{Cible :} $O_5$.
    \item \textit{Entrée :} $(I_3, O_5)$ $\rightarrow$ \textit{Cible :} \texttt{NEXT}.
\end{itemize}
\end{enumerate}
Au final, la séquence temporelle initiale est convertie en un ensemble de 8 échantillons d'apprentissage. Cette transformation permet de ramener un problème de traitement de séquence complexe et long à une succession de tâches de régression locales de courte portée, parfaitement adaptées à l'entraînement d'un réseau de neurones récurrent supervisé.


\section{Validation de l'approche sur architectures récurrentes}
Dans cette section, nous présentons les résultats publiés lors de la conférence CAID. Cet article démontre que pour la modélisation du traitement radar, l'approche "flux-à-flux" (ici désignée par le terme Forecasting) est particulièrement pertinente pour capturer la causalité temporelle du système. L'étude établit également quelle variante d'architecture récurrente offre le meilleur compromis entre mémoire et stabilité dans ce contexte.
Cette section se concentre exclusivement sur les résultats obtenus en mode "prévision" (forecasting), qui correspond à la configuration opérationnelle cible. Nous omettrons ici la description détaillée de la transformation des données (le dépliage en séquences d'événements) ainsi que la métrique spécifique utilisée pour l'optimisation. Ces éléments méthodologiques sont détaillés dans l'article et une transformation analogue sera exposée en détail dans la section suivante consacrée aux extensions de ces travaux. La fonction de coût utilisée ici est comparable à l'équation (4) présentée précédemment, où le terme de normalisation est calculé sur la séquence événementielle dépliée.
\subsection{Les architectures}
Afin d'identifier la structure la plus adaptée, nous avons évalué un panel d'architectures récurrentes classiques. Ce choix est motivé par la nature séquentielle et causale du problème, où chaque prédiction dépend de l'historique des états internes. Les modèles comparés sont les suivants :
\begin{itemize} \item \textbf{RNN (Vanilla Recurrent Neural Network)} : Il constitue l'architecture de référence la plus simple. Bien que théoriquement capable de traiter des séquences, il souffre notoirement du problème de disparition du gradient, limitant sa capacité à retenir l'information sur de longues périodes. \item \textbf{LSTM (Long Short-Term Memory)} : Cette architecture introduit des portes logiques (entrée, oubli, sortie) pour réguler le flux d'information. Elle est conçue spécifiquement pour capturer les dépendances à long terme qui échappent aux RNN simples. \item \textbf{GRU (Gated Recurrent Unit)} : Variante simplifiée du LSTM, le GRU fusionne certaines portes pour réduire le nombre de paramètres tout en conservant une capacité de mémoire comparable. Il offre souvent une convergence plus rapide et une meilleure stabilité numérique. \item \textbf{LSTMAT (LSTM + Attention Additive)} : Il s'agit d'un décodeur LSTM augmenté d'un mécanisme d'attention de type Bahdanau. À chaque pas de temps, le modèle calcule un score additif pour pondérer l'importance des états cachés passés, permettant de "relire" l'historique pertinent. \item \textbf{LSTMAT-L (LSTM + Attention de Luong)} : Cette variante utilise un mécanisme d'attention bilinéaire (produit scalaire généralisé). Plus efficace calculatoirement, cette formulation tend à mieux passer l'échelle sur les longues séquences en offrant des gradients plus nets lors de l'alignement temporel. \end{itemize}
\subsection{Résultats expérimentaux}
L'évaluation concerne ici uniquement la configuration Forecasting, où le modèle ne dispose d'aucun accès au futur et génère les impulsions de manière purement causale.
Afin de garantir une comparaison équitable, chaque architecture a fait l'objet une optimisation rigoureuse de ses hyper-paramètres via un algorithme bayésien (TPE via Optuna). Cette étape a permis de s'assurer que les écarts de performance observés sont structurels et non liés à un mauvais réglage. La table ci-dessous résume la configuration géométrique optimale (nombre de couches $L$ et dimension latente $H$) retenue pour chaque modèle à l'issue de cette optimisation:
\begin{table}[h]
\centering
\begin{tabular}{l c c}
\hline
\textbf{Architecture} & \textbf{Dimension cachée ($H$)} & \textbf{Nombre de couches ($L$)} \\
\hline
RNN & 640 & 4 \\
GRU & 640 & 5 \\
LSTM & 768 & 4 \\
LSTMAT (Basic) & 768 & 4 \\
LSTMAT-L (Luong) & 768 & 6 \\
\hline
\end{tabular}
\caption{Configurations optimales des architectures évaluées (issues de l'annexe de l'article [ref]).}
\end{table}
Les entraînements ont été réalisés sur des ensembles de données synthétiques de 500 000 séquences, permettant de couvrir une grande diversité de scénarios d'interférence. La performance est mesurée par l'erreur quadratique moyenne normalisée (NRMSE) sur des horizons de prédiction croissants (10, 20 et 50 impulsions).

\begin{table}[h] 
\centering 
\begin{tabular}{l c c c} 
\hline 
\textbf{Modèle} & \textbf{10 impulsions} & \textbf{20 impulsions} & \textbf{50 impulsions} \\ 
\hline 
RNN & 0.1261 & 0.2109 & 0.2156 \\
GRU & 0.1260 & 0.1981 & 0.1945 \\ 
LSTM & 0.1146 & 0.2192 & 0.2001 \\ 
LSTMAT (Basic) & 0.1053 & 0.1757 & 0.1684 \\ 
\textbf{LSTMAT-L (Luong)} & \textbf{0.0744} & \textbf{0.1566} & \textbf{0.1542} \\ 
\hline 
\end{tabular} 
\caption{Comparaison des performances (NRMSE) en mode Forecasting en fonction de la longueur de la séquence.} 
\end{table}

Les résultats mettent en évidence plusieurs dynamiques caractéristiques : \begin{itemize} \item Limite des modèles simples : Le RNN et le LSTM standard montrent leurs limites sur la durée. Si le LSTM est performant à court terme (0.1146), il décroche significativement sur les horizons longs (0.2001 à 50 impulsions), victime de dérive temporelle (drift). Le GRU offre une stabilité légèrement supérieure sur le long terme mais reste insuffisant. \item Apport de l'attention : L'ajout de mécanismes d'attention améliore la robustesse globale. La variante avec attention additive (LSTMAT) réduit l'erreur à long terme (0.1684). \item Supériorité de Luong : L'architecture LSTMAT-L se distingue nettement. Elle offre non seulement la meilleure précision sur les courts horizons (0.0744), mais maintient surtout une erreur stable ($\approx 0.15$) lorsque la séquence s'allonge. Cela confirme que la capacité à "requêter" précisément l'historique via une attention bilinéaire est cruciale pour maintenir la cohérence du flux radar sur la durée. \end{itemize}
\subsection{Conclusion}
Cette étude valide expérimentalement la pertinence de l'approche par flux (Forecasting) couplée à une architecture LSTM avec attention de Luong. Ces travaux, présentés sous forme de poster à la conférence CAID, ont reçu un accueil favorable, confirmant l'intérêt de la communauté pour des modèles neuronaux capables de reproduire la causalité fine des chaînes de traitement signal. Ces résultats constituent le socle sur lequel nous bâtissons les extensions présentées dans la section suivante.


\section{Extension des essais : Généralisation à d'autres architectures}
Cette section étend la discussion sur la pertinence du dépliage des données initiée avec les travaux présentés à la conférence CAID [ref]. Si l'étude précédente se concentrait sur les architectures récurrentes, nous avons constaté que cette nouvelle formulation du problème permet en réalité l'utilisation de toute architecture de traitement de séquence, à condition qu'elle opère de manière auto-régressive et respecte donc la contrainte de causalité.
Nous réitérons les expériences en introduisant deux nouvelles familles d'architectures : les Réseaux Convolutifs et les Transformers. \\

Nous détaillons ici les adaptations nécessaires au pré-traitement des données pour maximiser l'efficacité de ces modèles, la sélection des architectures, ainsi que les métriques spécifiques définies pour guider l'apprentissage, avant de présenter les résultats et nos conclusions.

\subsection{Préparation des données et ingénierie des caractéristiques}
Le passage d'une modélisation globale (\textit{séquence vers séquence}) à une modélisation locale (\textit{flux-à-flux}) impose une adaptation de la représentation des données. D'une part, l'ingénierie des caractéristiques temporelles doit être repensée pour garantir que l'information fournie au modèle est localement exploitable, sans dépendre d'une vision globale de la séquence. D'autre part, la structure des données d'entraînement doit être adaptée via un système de masquage spécifique pour piloter l'apprentissage auto-régressif conditionnel.

\subsubsection{Encodage relatif local du temps}

Avant dépliage, les données brutes encodaient le temps d'arrivée relativement à la position dans la séquence afin d'éviter la croissance incontrôlée des valeurs. Cependant, dans le contexte du dépliage où les séquences d'entrée et de sortie sont entrelacées, la pertinence de cette représentation est remise en question.\\

Il est important de noter que, mathématiquement, l'encodage initial reste décodable avec la séquence dépliée car l'opération de pliage/dépliage est réversible. Toutefois, notre problématique ne porte pas sur la décodabilité théorique, mais sur la difficulté d'extraction de cette information par le réseau de neurones. Nous nous intéressons ici à la difficulté de décoder cette information temporelle dans le cadre d'une sous-séquence (typiquement avec un début tronqué). Ce scénario simule l'horizon limité que le modèle peut se représenter dans son espace latent et sa mémoire, en accord avec la philosophie de l'approche qui vise à reproduire les contraintes du matériel (DCI).\\

\begin{itemize}
\item Pour les PDW incidents ($(I_i)_{1 \leq i \leq N}$) : \\ Le décodage des temps d'arrivée est réalisé relativement au temps d'arrivée du premier PDW, que nous associons à l'origine des temps locale. Cependant, pour permettre au modèle de prédire, étant donné le PDW incident $I_i$ le bon nombre de PDW à générer avant le prochain PDW incident $I_{i+1}$, le modèle doit savoir cette prochaine date d'arrivée. Nous enrichissons ainsi la représentation de chaque PDW  $I_i$ en concaténant à ses caractéristiques la différence de temps d'arrivé avec l'impulsion suivante : 
$$\Delta T_{i} = TOA(I_{i+1}) - TOA(I_{i})$$
Cette information permet au modèle d'estimer la "fenêtre temporelle" disponible pour ses émissions. Dans le cas d'une fin de séquence, une valeur assez grande est utilisée pour signifier au modèle qu'il doit vider sa mémoire (générer toutes les réponses restantes).\\

\item Pour les impulsions interceptées ($(O_i)_{0 \leq i \leq M}$) : \\ Dans la configuration \textit{seq2seq}, l'encodage du temps d'émission est réalisé en soustrayant à cette valeur (information globale) réelle la position dans la séquence (information globale) [ref au calcul]. Bien que cette information soit toujours récupérable par inversion du dépliage, on considère que retrouver localement la position qu'une prédiction aurait eu dans la séquence non-dépliée n'est pas dans l'esprit de l'approche. La synchronisation des flux-à-flux permet d'adopter un encodage local plus pertinent : le temps d'arrivé d'un PDW intercepté $O_i$ (qu'il soit en entrée ou à prédire) est encodé relativement au temps d'arrivée de l'impulsion incidente courante $I_j$ qui la conditionne.
$$TOA(O_i) - TOA(I_j)$$
Par exemple dans le cadre de la séquence dépliée liée à \ref{tab:unfolding_example}, le troisième élément est constitué d'une entrée $(I_1, O_2)$ et d'une cible $O_3$. Alors le temps d'arrivé de $O_2$ sera encodé localement $TOA(O_2) - TOA(I_1)$ et le temps d'arrivé de $O_3$ sera encodé $TOA(O_3) - TOA(I_1)$.
\end{itemize}

\subsubsection{Gestion des masques et exemple de dépliage}
Outre les valeurs, la structure du dépliage nécessite la gestion de plusieurs masques pour piloter l'apprentissage supervisé et le calcul de la fonction de coût :
\begin{itemize}
\item MI (Mask Input) : Indique si l'entrée secondaire (Input 2) est le jeton spécial \texttt{NEXT} (1) ou une PDW valide (0).
\item MO (Mask Output) : Indique si la cible attendue est le jeton \texttt{NEXT} (1) ou une PDW valide (0).
\item MP (Mask Padding) : Indique si la position est valide (1) ou si elle relève du remplissage (padding) de fin de batch (0).
\end{itemize}
\: \\

Pour illustrer ce mécanisme, considérons une séquence courte avec 2 impulsions incidentes ($I_1, I_2$) générant respectivement 1 et 0 impulsion de sortie ($O_1$). La séquence dépliée de longueur 4 se construit ainsi :

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Pas de temps} & \textbf{t=1} & \textbf{t=2} & \textbf{t=3} & \textbf{t=4} \\
\hline
\textbf{Input 1} ($PDW_{in}$) & $I_1$ & $I_1$ & $I_2$ & Pad \\
\hline
\textbf{Input 2} (Contexte) & \texttt{NEXT} & $O_1$ & \texttt{NEXT} & Pad \\
\hline
\textbf{Target} (Sortie) & $O_1$ & \texttt{NEXT} & \texttt{NEXT} & Pad \\
\hline
\hline
\textbf{MI} (Input 2 is Next) & 1 & 0 & 1 & 0 \\
\hline
\textbf{MO} (Target is Next) & 0 & 1 & 1 & 0 \\
\hline
\textbf{MP} (Valid) & 1 & 1 & 1 & 0 \\
\hline
\end{tabular}
\caption{Exemple de construction des données et des masques pour une séquence courte dépliée.}
\label{tab:unfolding_example}
\end{table}

\subsection{Sélection des architectures}
Nous comparons trois architectures distinctes, choisies pour leur représentativité des différents paradigmes de traitement séquentiel.

\subsubsection{Le Transformer (Attention Mechanism)}
Le modèle utilisé est un Encodeur Transformer modifié, inspiré de l'architecture "Attention Is All You Need" [Vaswani et al., 2017]. Contrairement à l'encodeur standard qui voit tout le contexte, notre encodeur remplace les modules de Self-Attention [ref] par un module de Masked Self-Attention [ref] pour garantir la causalité. Cette architecture est particulièrement adaptée pour capturer des dépendances complexes et non-locales dans la séquence dépliée.

\subsubsection{Le TCN (Temporal Convolutional Network)}
Le TCN (Temporal Convolutional Network) est une architecture convolutionnelle adaptée aux séquences. Contrairement aux RNNs, il traite l'information en parallèle via des convolutions causales dilatées. Cette architecture est introduite pour évaluer si une modélisation hiérarchique locale, à champ réceptif fixe mais large, est plus efficace qu'une mémoire récurrente pour capturer la dynamique événementielle du DCI.

\subsubsection{Le GRU (Recurrent Network)}
Pour représenter la famille des réseaux récurrents, nous avons écarté les variantes à attention (LSTMAT) car leur mécanisme de "relecture" du passé les rapproche conceptuellement des Transformers, diluant l'intérêt de la comparaison "orthogonale". Au vu des résultats précédents [ref section précédente], nous retenons le GRU (Gated Recurrent Unit), qui s'est avéré être le compromis le plus performant et stable pour le traitement des longues séquences en mode déplié.

\subsubsection{Structure commune et hyper-paramètres}
Afin de garantir une comparaison rigoureuse, toutes les architectures évaluées partagent une topologie modulaire commune et un même principe d'inférence, structurés comme suit :\\

\begin{itemize}
\item Un vecteur d'état spécial $l_{next}$ : Il s'agit d'une représentation latente apprenable du jeton \texttt{NEXT}. Ce vecteur est destiné à être inséré dynamiquement dans la séquence pour signaler les changements de contexte.

\item Une couche d'embedding : Elle projette la séquence d'entrée brute vers l'espace latent du modèle, élément par élément, transformant les caractéristiques physiques des impulsions en une représentation vectorielle latente initiale.

\item Un mécanisme de substitution conditionnelle : À l'aide du masque binaire $MI$, les représentations latentes situées aux positions de contexte (Input 2) sont substituées par le vecteur $l_{next}$. Cette opération prépare la séquence pour le traitement auto-régressif.

\item Le corps du modèle : Selon l'architecture testée (GRU, TCN ou Transformer), ce bloc traite la séquence latente de manière strictement causale. Il enrichit progressivement les représentations par agrégation du contexte temporel passé, produisant en sortie une séquence latente à haute teneur sémantique.

\item Un décodeur (Tête de régression) : Il transforme chaque élément de la séquence latente enrichie en une prédiction dans l'espace des PDW (caractéristiques physiques) et retourne cette estimation.

\item Un comparateur (Tête de classification) : Ce module calcule la similarité (\textit{cosine similarity}) entre chaque représentation latente enrichie finale et le vecteur $e_{next}$. Ces scores, normalisés entre 0 et 1, quantifient la probabilité que chaque prédiction corresponde au jeton \texttt{NEXT}.
\end{itemize}
\; \\

Les hyper-paramètres de chaque architecture ont été optimisés via une recherche bayésienne (TPESampler d'Optuna) sur des entraînements courts.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Paramètre} & \textbf{Transformer} & \textbf{TCN} & \textbf{GRU} \\
\hline
Dimension Latente ($d_{model}$) & 128 & 128 & 256 \\
Nombre de couches ($L$) & 4 & 6 & 2 \\
Têtes d'attention / Taille Noyau & 4 têtes & $k=3$ & - \\
Dropout & 0.1 & 0.2 & 0.1 \\
\hline
\end{tabular}
\caption{Hyper-paramètres géométriques retenus pour chaque architecture.}
\label{tab:hyperparams}
\end{table}


\subsection{Fonction de perte et métriques d'évaluation}
L'apprentissage est guidé par une fonction de perte composite qui minimise simultanément l'erreur de reconstruction des PDW (régression) et l'erreur de détection du jeton \texttt{NEXT} (classification).

\subsubsection{Erreur de reconstruction pondérée ($\mathcal{L}_{reg}$)}
Cette composante évalue la qualité des caractéristiques prédites, mais uniquement pour les positions correspondant à une impulsion valide. Elle ignore donc les positions de padding ($MP=0$) ainsi que les positions où le jeton \texttt{NEXT} est attendu ($MO=1$).\\

Considérons un lot (batch) de $B$ séquences de longueur maximale $T$, où chaque impulsion est un vecteur de dimension $D$. L'ensemble des séquences cibles $\mathbf{Y}$ comme l'ensemble des séquences prédites $\hat{\mathbf{Y}}$ peuvent être considéré comme des tenseurs de dimension $B \times T \times D$. Les masques peuvent eux être associés à des tenseurs de dimensions $B \times T$. L'erreur brute est définie comme une distance euclidienne (RMSE), normalisée par le nombre total d'éléments scalaires valides dans le lot. Elle s'écrit :
\begin{equation}
\mathcal{L}{raw}(\hat{\mathbf{Y}}, \mathbf{Y}) = \frac{\| (\hat{\mathbf{Y}} - \mathbf{Y}) \odot (1 - \mathbf{MO}) \odot \mathbf{MP} \|_2}{\| ((1 - \mathbf{MO}) \odot \mathbf{MP} \|_2\sqrt{D}}
\end{equation}

Le terme au dénominateur assure que la perte est invariante à la taille du batch et au taux de remplissage (padding). Pour équilibrer l'apprentissage face à la diversité des séquences, nous utilisons la version pondérée par l'erreur de référence $\mathcal{L}_{ref}$ (calculée sur la séquence dépliée cible selon le protocole défini au chapitre précédent) :
\begin{equation}
\mathcal{L}{reg} = \frac{\mathcal{L}{raw}}{\mathcal{L}_{ref}}
\end{equation}

\subsubsection{Erreur de classification par MCC différentiable ($\mathcal{L}_{class}$)}
La seconde tête du réseau prédit un tenseur de probabilités $\mathbf{P}$ de dimension $B \times T$, où chaque élément $p_{k,t} \in [0, 1]$ représente la probabilité que la cible $t \in [|1; T|]$ de la séquence $k \in [|1; B|]$ soit un jeton \texttt{NEXT}.\\

Pour optimiser la performance sur ce problème de classification binaire déséquilibré, nous maximisons une version différentiable ("soft") du Coefficient de Corrélation de Matthews (MCC). 
\begin{align*}
TP &= \left\| \mathbf{MO} \odot \mathbf{P} \odot \mathbf{MP} \right\|_1 \\
TN &= \left\| (\mathbf{1} - \mathbf{MO}) \odot (\mathbf{1} - \mathbf{P}) \odot \mathbf{MP} \right\|_1 \\
FP &= \left\| (\mathbf{1} - \mathbf{MO}) \odot \mathbf{P} \odot \mathbf{MP} \right\|_1 \\
FN &= \left\| \mathbf{MO} \odot (\mathbf{1} - \mathbf{P}) \odot \mathbf{MP} \right\|_1
\end{align*}

On remarque que les quantités dans les tenseurs étant tous positifs ou nuls, ces calculs de norme correspondent simplement à la somme sur tous les éléments. Le score MCC différentiable est alors donné par :
\begin{equation}
MCC_{soft} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN) + \epsilon}}
\end{equation}

La perte de classification est définie pour tendre vers 0 lorsque la corrélation est parfaite ($MCC \to 1$) :
\begin{equation}
\mathcal{L}{class} = 1 - MCC{soft}
\end{equation}

La fonction de coût totale combinant régression et classification est finalement : 
$$\mathcal{L}_{total} = (1 - \lambda)\mathcal{L}_{reg} + \lambda \mathcal{L}_{class}$$









 