
\section{Méthodes de traitement : Séquences et structures spatiales}
\subsection{Typologie des données : De la causalité temporelle à la topologie spatiale}
Avant d'aborder les architectures neuronales, il est essentiel de définir formellement les objets mathématiques qu'elles manipulent et de distinguer leurs natures intrinsèques. Si l'apprentissage statistique classique suppose souvent des données indépendantes (i.i.d.), les signaux complexes se caractérisent par des structures de dépendance fortes. Nous distinguerons ici deux régimes fondamentaux : la séquence temporelle et la structure spatiale.
\subsubsection{La séquence temporelle et la causalité}
La séquence se définit par une dimension temporelle unidimensionnelle, régie par une flèche du temps irréversible. Elle constitue une collection indexée $X = \{x_1, x_2, \dots, x_T\}$ où l'indice $t$ porte une information causale déterminante : l'état présent est une fonction de l'histoire passée. C'est le fondement de la théorie de l'information de Shannon \cite{shannon_mathematical_1948}, où le langage est modélisé comme un processus stochastique causal. Dans cette vision, la probabilité d'un symbole dépend conditionnellement des précédents. Cette logique s'applique identiquement aux séries temporelles continues (physique, signaux capteurs). Des travaux sur les modèles ARIMA \cite{box_time_2015} montrent qu'une observation à l'instant $t$ est corrélée à ses prédécesseurs immédiats. Dans ce formalisme, la séquence impose une contrainte de traitement directionnelle : le futur n'est pas encore accessible au moment du traitement du présent.
\subsubsection{La donnée spatiale et la contiguïté}
À l'opposé, l'image ou le champ physique se définit comme une grille spatiale (bidimensionnelle ou tridimensionnelle) où la notion d'ordre séquentiel disparaît au profit de la contiguïté topologique. Ici, un pixel ou une cellule $x_{i,j}$ n'a pas de "passé" ou de "futur", mais un voisinage omnidirectionnel. La dépendance est régie par la proximité euclidienne : la valeur d'un point est corrélée à celle de ses voisins adjacents dans toutes les directions. Cette structure est modélisée mathématiquement par les Champs de Markov (Markov Random Fields). Contrairement à la séquence causale, l'intégralité de la donnée spatiale est disponible simultanément, permettant des interactions non-orientées. Bien que certaines architectures (comme les Transformers) puissent linéariser ces données pour les traiter, nous conserverons le terme de "structure spatiale" pour désigner ces signaux où la topologie prime sur la chronologie.
\subsubsection{Universalité et convergence des architectures}
Le défi des architectures que nous allons présenter (CNN, RNN, Transformer) est de modéliser la fonction de dépendance conditionnelle $P(x | \text{Contexte})$ adaptée à chaque type de donnée. Pour les séries temporelles, ce contexte est un historique causal $x_{<t}$. Pour les données spatiales, ce contexte est un voisinage topologique. L'enjeu de cette section est de montrer comment des architectures initialement conçues pour l'un de ces domaines (comme le CNN pour l'espace) ont pu être adaptées pour traiter l'autre (la séquence 1D), et comment des mécanismes unificateurs comme l'Attention tentent aujourd'hui de s'affranchir de cette distinction.
\subsection{Réseaux de convolution}
Les Réseaux de Neurones Convolutionnels (CNN) constituent la réponse architecturale par excellence au traitement des données structurées sur des grilles régulières. Initialement conçus pour exploiter la localité et la stationnarité des images, ils ont été adaptés avec succès au traitement de séquences temporelles en transposant la notion de voisinage spatial vers celle d'horizon temporel.
\subsubsection{Genèse et prédominance dans l'imagerie}
L'histoire des réseaux de convolution est indissociable de la vision par ordinateur et de la volonté de s'affranchir des descripteurs manuels (SIFT \cite{lowe_distinctive_2004}, HOG \cite{dalal_histograms_2005}). Inspiré par le fonctionnement du cortex visuel, le premier modèle LeNet-5 \cite{lecun_gradient-based_2002} a introduit les concepts de champ récepteur local et de partage des poids. L'avènement d'AlexNet \cite{krizhevsky_imagenet_2012} a marqué le point d'inflexion en démontrant la supériorité de l'apprentissage profond sur GPU, ouvrant la voie à des architectures comme GoogLeNet \cite{szegedy_going_2015} qui factorisent les convolutions pour traiter des motifs à différentes échelles spatiales.
\subsubsection{Mécanisme : Filtrage local et expansion hiérarchique}
L'interaction fondamentale d'un CNN repose sur l'opérateur de convolution (ou corrélation croisée). Contrairement à une couche dense, la convolution impose une contrainte de partage des poids sur une grille métrique régulière. Elle présuppose une fonction $p(\cdot)$ associant chaque élément à une position.
Le noyau de convolution $w$, défini sur un support fini $\mathcal{V}$, parcourt la structure de données. La sortie $h_u$ à la position $u$ est une somme pondérée des éléments du voisinage :
$$h_u = \sum_{v \in \mathcal{V}} w_{v} \cdot x_{u+v} + b$$
Cette formulation garantit l'invariance par translation : le même détecteur de motif est appliqué uniformément sur toute la donnée (image ou séquence).
La compréhension de la structure globale émerge de la composition hiérarchique. L'illustration \ref{convolution base} montre l'expansion du champ récepteur : une première couche de taille 3 voit un voisinage local, mais une seconde couche agissant sur la sortie de la première agrège indirectement une zone plus large. La profondeur du réseau agit comme un multiplicateur mécanique de l'horizon d'interaction.
\begin{figure}
\begin{center}
\includegraphics[scale=0.25]{BasicConvolution.png}
\end{center}
\caption{Illustration d'une convolution 1D standard et de l'expansion hiérarchique du champ récepteur}
\label{convolution base}
\end{figure}
Un point crucial pour la distinction entre espace et temps réside dans la géométrie du support. La figure \ref{convolution comparaison} (gauche) montre une convolution centrée : le calcul en $t$ dépend des voisins $t-k$ (passé) et $t+k$ (futur). Cette approche est valide pour une image (spatiale) ou une analyse de séquence a posteriori. En revanche, pour la modélisation de systèmes dynamiques causaux, on utilise la convolution causale (droite) : le support est décalé pour que la sortie en $t$ ne dépende que des indices $t' \le t$. Cette variante adapte l'outil spatial aux contraintes physiques de la temporalité.
\begin{figure}
\begin{center}
\includegraphics[scale=0.45]{ComparaisonConvolution.png}
\end{center}
\caption{Impact de la topologie du support sur la causalité : approche centrée spatiale (gauche) et approche causale temporelle (droite)}
\label{convolution comparaison}
\end{figure}
\subsubsection{La convolution dans l'image : Traitement spatial 2D}
Dans le contexte de l'image, le CNN opère sur une grille 2D. L'extraction est hiérarchique : les premières couches détectent des primitives (bords), les suivantes des motifs sémantiques. L'architecture VGG \cite{simonyan_very_2015} a standardisé l'usage de filtres $3\times3$ profonds. Pour pallier la disparition du gradient dans ces réseaux profonds, ResNet \cite{he_deep_2016} a introduit les connexions résiduelles. Pour les tâches de simulation physique sur grille (ex: champs de pression), l'architecture U-Net \cite{navab_u-net_2015} est la référence. Elle utilise des connexions latérales (skip connections) pour fusionner le contexte global et les détails locaux, indispensable pour ne pas perdre la précision spatiale lors de la reconstruction.
\subsubsection{La convolution pour les séquences (1D)}
Bien qu'issus du monde spatial, les CNN se révèlent performants pour les séquences temporelles 1D (Texte, Audio, Séries). L'architecture WaveNet \cite{van_den_oord_wavenet_2016} pour l'audio utilise des convolutions causales dilatées, permettant au champ récepteur de croître exponentiellement avec la profondeur pour capturer de longs historiques sans récurrence.
Cette approche s'est généralisée sous le nom de Temporal Convolutional Networks (TCN) \cite{bai_empirical_2018}. Les études montrent que pour de nombreuses tâches séquentielles, les TCN, grâce à leur stabilité et leur parallélisation, surpassent les RNN classiques (LSTM), prouvant que la localité hiérarchique est une inductive bias pertinent même pour la temporalité.
\subsubsection{Généralisation : Au-delà des grilles régulières}
Le principe de convolution a été étendu au-delà des images planes. Pour la vidéo (3D spatio-temporel), C3D \cite{tran_learning_2015} utilise des filtres volumétriques. De même, V-Net \cite{milletari_v-net_2016} applique ce principe aux données médicales volumétriques (voxels). Ces méthodes restent toutefois contraintes par la régularité de la grille euclidienne.
La rupture survient avec les données intrinsèquement irrégulières (maillages non structurés, molécules). Ici, le voisinage n'est plus matriciel mais topologique. Les Graph Convolutional Networks (GCN) \cite{kipf_semi-supervised_2016} redéfinissent la convolution comme une agrégation sur les nœuds voisins d'un graphe. Pour les nuages de points 3D (LiDAR), PointNet++ \cite{qi_pointnet_2017} applique des opérations hiérarchiques sur des coordonnées continues, s'affranchissant de la voxelisation.

\subsection{Réseaux de neurones récurrents et Espaces d'États (RNN et SSM)}
Si les réseaux de convolution abordent les données par une fenêtre glissante spatiale ou temporelle, une autre famille d'architectures adopte une approche intrinsèquement liée à la définition stricte de la séquence causale : la modélisation récursive. Qu'il s'agisse des Réseaux de Neurones Récurrents (RNN) historiques ou des récents Modèles d'Espaces d'États (SSM), le principe fondateur est la persistance de l'information le long de la flèche du temps. Le modèle maintient un état caché interne $h_t$, agissant comme une mémoire compressée de l'historique passé, mise à jour à chaque nouvelle observation.
\subsubsection{Mécanisme : Récurrence et persistance causale}
L'interaction fondamentale de ces architectures repose sur la causalité. Contrairement à l'agrégation statique d'un voisinage dans un CNN, l'opération centrale est une mise à jour dynamique. À chaque pas de temps $t$, l'état courant est recalculé en fonction de l'état précédent et de la nouvelle observation, selon une équation de transition générique $h_t = f(h_{t-1}, x_t; \theta)$. Cette formulation implique que $h_t$ dépend indirectement de toute la trajectoire passée $\{x_0, \dots, x_t\}$. L'illustration \ref{RNN base} permet de visualiser cette propagation : le flux d'information transporte l'état caché horizontalement, créant un lien causal ininterrompu. Contrairement au CNN dont le champ récepteur est fini, le cône d'influence du RNN s'étend théoriquement à l'infini vers le passé.
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{BasicRNN.png}
\end{center}
\caption{Mécanisme fondamental de la récurrence et propagation de l'état mémoire}
\label{RNN base}
\end{figure}
L'histoire de ces modèles reflète la lutte pour stabiliser cette mémoire sur de longues durées. Les RNN simples \cite{elman_finding_1990} souffraient de la disparition du gradient, empêchant l'apprentissage des causes lointaines. Le LSTM (Long Short-Term Memory) \cite{hochreiter_long_1997} a résolu ce problème par des portes logiques régulant l'oubli et la rétention d'information. Plus récemment, les Modèles d'Espaces d'États (SSM) comme S4 \cite{gu_efficiently_2022} ou Mamba \cite{gu_mamba_2024} ont modernisé cette approche en discrétisant des équations différentielles continues, permettant de modéliser des dépendances sur des milliers de pas de temps avec une complexité linéaire, défiant ainsi les architectures à attention sur les séquences très longues.
\subsubsection{Topologies de traitement : Flux vs Traduction}
La persistance de l'état caché permet deux modes de fonctionnement distincts. Le mode "Flux à Flux" (gauche, figure \ref{RNN comparaison}) aligne la sortie sur l'entrée : à chaque $t$, une prédiction est émise. C'est la configuration naturelle pour le filtrage en ligne ou le contrôle de systèmes dynamiques, respectant une causalité physique stricte. Le second mode, "Séquence vers Séquence" (Seq2Seq, droite), opère en deux phases : un encodeur comprime toute la séquence source dans un vecteur d'état final, puis un décodeur génère la séquence cible. Ce paradigme a révolutionné la traduction automatique \cite{sutskever_sequence_2014} mais impose un goulot d'étranglement (le vecteur contexte) difficile à tenir pour des signaux denses.
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{ComparaisonRNN.png}
\end{center}
\caption{Topologies d'application : traitement de flux causal (gauche) et traduction globale (droite)}
\label{RNN comparaison}
\end{figure}
\subsection{L'architecture Transformer}
Si les RNN sont conçus pour la séquence temporelle et les CNN pour la grille spatiale, l'architecture Transformer \cite{vaswani_attention_2017} propose un changement de paradigme radical. Elle postule que l'interaction ne doit être contrainte ni par la proximité temporelle (récurrence), ni par la proximité spatiale (convolution), mais pilotée uniquement par le contenu via un mécanisme d'attention globale.
\subsubsection{Architecture Macroscopique et adaptation aux modalités}
Le Transformer se définit d'abord comme une architecture modulaire composée d'Encodeurs et de Décodeurs.
L'Encodeur construit une représentation riche où chaque élément interagit avec tous les autres simultanément. Le Décodeur génère la sortie séquentiellement. Cette structure, initialement conçue pour le texte, a dû être adaptée pour traiter d'autres types de données. Pour les séquences temporelles strictes, l'architecture utilise des masques de causalité dans le mécanisme d'attention, interdisant au modèle de "voir" le futur, recréant ainsi artificiellement la flèche du temps. Pour les données spatiales (images), dépourvues d'ordre intrinsèque, le Vision Transformer (ViT) \cite{dosovitskiy_image_2020} procède par découpage de l'image en "patchs" (carrés de pixels) qui sont ensuite linéarisés. C'est ici que la distinction sémantique est cruciale : le Transformer ne traite pas l'image comme une grille, mais force la donnée spatiale à entrer dans un format séquentiel d'ensemble, perdant la topologie locale au profit d'une interaction globale.
\subsubsection{Mécanisme interne : Attention et Indépendance de position}
Au niveau microscopique, le cœur du système est l'Attention Scalaire Produit. Pour chaque élément, le modèle génère une Requête ($Q$), une Clé ($K$) et une Valeur ($V$). Le score d'attention est calculé par la compatibilité entre $Q$ et $K$ :
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
Ce mécanisme traite l'entrée comme un "sac d'éléments" sans ordre. Contrairement au RNN qui "sait" que $x_t$ suit $x_{t-1}$ par construction, le Transformer est invariant par permutation. Pour traiter des séquences (où l'ordre compte) ou des images (où la position des patchs compte), il est donc impératif d'injecter une information explicite : l'Encodage Positionnel (Positional Encoding). C'est cet ajout qui permet à une architecture basée sur l'ensemble de traiter la structure ordonnée.
\subsubsection{Applications : Limites sur le continu et le spatial}
Dans le domaine du texte, cette approche a permis l'émergence des grands modèles de langage (LLM). Pour les séries temporelles continues, l'application est plus controversée. Bien que capable de capturer des dépendances à long terme, l'attention globale en $O(N^2)$ est coûteuse et tend parfois à sur-interpréter le bruit local, conduisant certains auteurs à préférer des modèles linéaires simples (DLinear) ou des variantes segmentées (PatchTST) \cite{nie_time_2023}. En vision, bien que ViT ait prouvé que l'on peut se passer de convolution avec assez de données, des architectures hybrides comme Swin Transformer \cite{liu_swin_2021} réintroduisent une notion de fenêtre locale pour retrouver l'efficacité du traitement spatial hiérarchique.
\subsection{Ancrage dans la problématique}
L'analyse de ces architectures à l'aune de notre problématique de simulation radar révèle des compromis clairs entre la nature des données et les mécanismes de traitement.
Les réseaux convolutionnels (CNN), par leur biais inductif de localité, sont naturellement adaptés aux aspects "spatiaux" ou "images" de nos données, comme les cartes de densité ou les matrices temps-fréquence. Ils excellent à modéliser les interactions physiques locales (interférences immédiates). Cependant, leur structure à fenêtre glissante limite leur capacité à lier des événements distants dans le temps, une lacune critique pour le pistage de scénarios longs.
Les architectures récurrentes (RNN/SSM) épousent parfaitement la nature "séquentielle causale" du flux radar. En mimant la flèche du temps et la persistance de l'état, ils correspondent à la physique du capteur et aux algorithmes de filtrage. Néanmoins, leur usage en mode génératif global (Seq2Seq) se heurte au goulot d'étranglement du vecteur contexte : compresser une simulation complexe en un seul état latent est souvent impossible sans perte d'information majeure.
Enfin, le Transformer offre une capacité de modélisation puissante grâce à son mécanisme d'attention globale, s'affranchissant des limitations de mémoire des RNN et de la localité des CNN. Toutefois, son invariance par permutation impose une vigilance extrême : pour simuler un système radar causal, nous devrons forcer cette structure (via masques et encodages positionnels) à respecter rigoureusement la temporalité, et gérer sa complexité quadratique face à la densité des impulsions. Le choix de l'architecture devra donc arbitrer entre le respect de la physique causale (RNN) et la puissance de modélisation des interactions complexes (Transformer).



\section{Méthode traitement de séquence}

\subsection{Le concept de séquence : De l'ensemble à la structure ordonnée}
Avant d'aborder les architectures neuronales spécifiques, il est essentiel de définir formellement l'objet mathématique qu'elles manipulent : la séquence. Dans l'apprentissage statistique classique, les données sont souvent supposées être indépendantes et identiquement distribuées (hypothèse i.i.d.). Le traitement de séquence rompt fondamentalement avec cette hypothèse en introduisant une notion d'ordre et de dépendance. Une séquence n'est pas un simple ensemble non ordonné, mais une collection indexée $X = \{x_1, x_2, \dots, x_T\}$ où l'indice $t$ porte une information sémantique ou causale déterminante. La valeur d'un élément $x_t$ n'a de sens que relativement à son contexte, c'est-à-dire son voisinage ou son historique. 
Cette propriété de dépendance locale, formalisée par les processus de Markov pour le temps ou les Champs de Markov pour l'espace, constitue le fondement théorique qui nous permet de poser une définition élargie de la séquentialité. Dans la suite de cette étude, nous considérerons comme séquentielle toute structure de données régie par ces dépendances contextuelles, qu'elles soient causales ou topologiques. Cette définition transcende les domaines d'application, unifiant sous un même formalisme le texte, les séries temporelles et l'image.


\subsubsection{séquence temporelle et la causalité}
La manifestation la plus intuitive de la séquence est temporelle et unidimensionnelle. Dans ce cadre, la notion de proximité est dictée par la causalité : l'état présent est une fonction de l'histoire passée. C'est le fondement de la théorie de l'information de Shannon \cite{shannon_mathematical_1948}, où le langage est modélisé comme un processus stochastique discret. Dans cette vision, la probabilité d'apparition d'un symbole (lettre ou mot) dépend conditionnellement de la séquence des symboles précédents, définissant la notion d'entropie d'une source d'information. Cette logique s'applique identiquement aux séries temporelles continues. Des travaux sur les modèles ARIMA \cite{box_time_2015} ont montrés qu'une observation à l'instant $t$ est mathématiquement corrélée à ses prédécesseurs immédiats et aux termes d'erreur passés. Dans ce formalisme statistique, la séquence est définie par une dépendance directionnelle irréversible vers le futur.

\subsubsection{La séquence spatiale et la contiguïté}
L'extension du concept de séquence à l'image est moins immédiate mais tout aussi fondamentale pour l'apprentissage profond moderne. Une image statique est une grille spatiale bidimensionnelle, mais elle peut être conceptualisée comme une séquence par deux approches distinctes. La première est la linéarisation explicite : on peut dérouler les pixels ligne par ligne pour former une longue chaîne unidimensionnelle. Des travaux comme PixelRNN \cite{van_den_oord_pixel_2016} ont montré qu'en traitant les pixels ainsi, comme une séquence autorégressive où chaque pixel dépend de ceux situés "avant" lui (en haut et à gauche), on pouvait modéliser la distribution conjointe des pixels d'une image et générer des structures visuelles cohérentes. La seconde approche définit la séquence par la contiguïté spatiale locale, 
s'affranchissant de la causalité temporelle au profit d'un voisinage omnidirectionnel propre aux Champs de Markov. C'est cette vision topologique qui sous-tend les opérations de convolution, où la notion d'ordre chronologique est remplacée par celle de proximité euclidienne.


\subsubsection{Universalité de la modélisation séquentielle}
Finalement, le défi central des architectures de traitement que nous allons présenter (CNN, RNN, Transformer) est de modéliser cette fonction de dépendance conditionnelle $P(x_t | \text{Voisinage})$. La nature de ce voisinage varie selon le domaine : il est strictement causal et orienté vers le passé pour les séries temporelles et la génération de texte, tandis qu'il est bidirectionnel et spatial pour l'image ou la compréhension sémantique globale. Cependant, l'objectif mathématique reste identique : capturer les corrélations à courte et longue portée qui structurent la donnée, transformant une collection de valeurs isolées en une entité cohérente.


\subsection{Réseaux de convolution}
Bien que les données séquentielles soient intuitivement associées à une dimension temporelle linéaire, le traitement de l'information repose fondamentalement sur l'extraction de motifs locaux et de relations de voisinage. C'est dans cette optique que les Réseaux de Neurones Convolutionnels (CNN) se positionnent comme une méthode incontournable. Initialement conçus pour la grille spatiale de l'image, ils formalisent une approche du traitement de séquence fondée sur la localité, l'invariance par translation et la hiérarchie des caractéristiques.

\subsubsection{Genèse et prédominance dans l'imagerie}
L'histoire des réseaux de convolution est indissociable de la vision par ordinateur et de la volonté de s'affranchir des descripteurs manuels  (SIFT \cite{lowe_distinctive_2004}, SURF \cite{bay_surf_2006} et HOG \cite{dalal_histograms_2005}). Inspirée par les travaux biologiques sur le cortex visuel, le premier modèle \cite{lecun_gradient-based_2002} a introduit les concepts fondateurs de champ récepteur local et de partage des poids pour la reconnaissance de caractères manuscrits. Cependant, c'est l'avènement d'AlexNet \cite{krizhevsky_imagenet_2012} qui a marqué le véritable point d'inflexion en démontrant la supériorité de l'apprentissage profond sur GPU pour l'extraction de caractéristiques. Cette percée a ouvert la voie à des architectures plus profondes et plus efficientes. Par exemple, l'architecture GoogLeNet \cite{szegedy_going_2015} factorise les convolutions pour réduire le coût de calcul tout en augmentant la largeur du réseau, permettant de traiter des motifs à différentes échelles simultanément.


\subsubsection{Mécanisme d'interaction : Filtrage local et expansion hiérarchique}
L'interaction fondamentale d'un réseau de convolution avec une séquence repose sur l'application répétée d'un opérateur de filtrage caractérisé par un noyau $w$ de support fini. Contrairement à une couche dense qui apprendrait un poids spécifique pour chaque élément de la séquence globale, la convolution impose une contrainte de partage des poids qui nécessite que les données soient structurées dans un espace métrique régulier. En effet, l'opération présuppose l'existence d'une fonction $p(\cdot)$ permettant d'associer à chaque élément $x$ sa position sur une grille sous-jacente, qu'elle soit unidimensionnelle pour des données temporelles ou multidimensionnelle pour des données spatiales.\\

Mathématiquement, le noyau $w$ est défini sur un support $\mathcal{V}$ centré à l'origine. Ainsi, le noyau définit pour tout élément cible $x_t$ un voisinage d'interaction $\mathcal{V}_t$, correspondant à la translation du support $\mathcal{V}$ en la position $p(x_t)$. L'opération de convolution consiste alors à calculer une somme pondérée des éléments appartenant à ce voisinage, où les poids sont déterminés exclusivement par la position relative entre les éléments et le centre. La sortie $h_t$ (avant activation) s'exprime par l'équation :
$$h_t = \sum_{x_j \in \mathcal{V}_t} w_{\Delta(x_j, x_t)} \cdot x_j + b$$
Dans cette expression, $\Delta(x_j, x_t) = p(x_j) - p(x_t)$ représente le vecteur de position relative du voisin $x_j$ par rapport au centre $x_t$, $b$ est un biais. Une conséquence directe de la structure en grille des données est que ce vecteur de différence correspond systématiquement à un n-uplet d'entiers ($n$ étant la dimension de la grille). Cette propriété discrète est fondamentale pour l'implémentation neuronale : elle implique que la fonction $w$ n'a pas besoin d'être modélisée comme une fonction continue, mais se réduit à un ensemble fini de paramètres scalaires (les poids du filtre) qu'il suffit d'apprendre pour chaque décalage entier possible dans le support. Cette formulation garantit l'invariance par translation, assurant que le même motif de poids est appliqué uniformément sur toute la structure. Par ailleurs, l'application de ce voisinage aux bornes d'une grille finie nécessite une gestion des effets de bord, typiquement résolue par l'ajout de valeurs nulles (zero-padding) en périphérie afin de conserver la dimension de la séquence traitée.\\

Ce mécanisme permet l'extraction robuste de motifs locaux, mais la compréhension de la structure globale de la séquence émerge de la composition hiérarchique de ces opérations. L'illustration \ref{convolution base} permet de visualiser comment l'empilement de couches induit une expansion mathématique du champ récepteur. Considérons une première couche définie par un filtre $w$ de taille 3. Pour un instant $t$, ce filtre induit un voisinage immédiat. L'équation locale est, en notant $\phi$ la fonction d'activation :
\begin{equation*}
\begin{split}
	h^{(1)}_t &= w_{1}x_{t-1} + w_{2}x_{t} + w_{3}x_{t+1} \\
	x^{(1)}_t &= \phi(h^{(1)}_t)
\end{split}
\end{equation*}
La sortie $x^{(1)}_t$ est une fonction des entrées $x_{t-1}$ à $x_{t+1}$.
Lorsqu'une seconde couche définie par un filtre $v$ de même support est appliquée sur cette représentation intermédiaire, elle opère selon le même principe d'invariance en translation :
\begin{equation*}
\begin{split}
	h^{(2)}_t & = v_{1}x^{(1)}_{t-1} + v_{2}x^{(1)}_{t} + v_{3}x^{(1)}_{t+1} \\
	x^{(2)}_t &= \phi(h^{(2)}_t)
\end{split}
\end{equation*}
La sortie $x^{(2)}_t$ est une fonction des entrées $x_{t-2}$ à $x_{t+2}$. Ainsi, par simple composition algébrique, l'horizon d'interaction s'est étendu de 3 éléments (couche 1) à 5 éléments (couche 2). La profondeur du réseau agit donc comme un multiplicateur mécanique de l'horizon d'interaction, permettant de reconstruire des dépendances causales à longue portée à partir de règles de construction strictement locales et invariantes.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.25]{BasicConvolution.png}
\end{center}
\caption{Illustration d'une convolution 1D standard et de l'expansion hiérarchique du champ récepteur}
\label{convolution base}
\end{figure}

Au-delà de l'expansion de l'horizon d'interaction, la géométrie du support de convolution détermine la nature causale ou non du traitement, une caractéristique nécessaire pour la modélisation de systèmes dynamiques. La partie gauche de la figure \ref{convolution comparaison} la configuration standard, dite convolution centrée. Pour calculer un élément de sortie $y_4$ à l'instant $t=4$, le champ récepteur effectif (cône violet) agrège les informations d'un voisinage symétrique de l'entrée $x$, de $x_2$ à $x_6$. Cette approche est naturelle pour l'analyse de données statiques (comme une image) ou le traitement de séquences complètes a posteriori, mais n'est pas adapté à la modélisation d'un système dynamique. Par exemple dans le cas de filtrage en ligne, le système ne peut physiquement pas accéder aux mesures futures pour débruiter les données du présent. Pour adapter l'architecture à ces contraintes temporelles strictes, on recourt à la convolution causale. Cette variante consiste à décaler le support du filtre de manière à ce que le voisinage d'interaction au temps $t$ ne contienne aucun indice supérieur à $t$. L'exemple illustré sur la partie droite de la figure \ref{convolution comparaison} assure que le cône d'influence de la sortie $y_5$ est alors strictement orienté vers le passé ($x_1$ à $x_5$). Dans cette configuration, le réseau conserve sa capacité d'extraction de motifs et de parallélisation, mais adopte une topologie d'interaction compatible avec la physique, simulant le comportement d'un système causal sans recourir à la mémoire récurrente.

\begin{figure}
\begin{center} 
\includegraphics[scale=0.45]{ComparaisonConvolution.png}
\end{center}
\caption{Impact de la topologie du support de convolution sur la causalité temporelle : approche centrée (gauche) et approche causale (droite)}
\label{convolution comparaison}
\end{figure}


\subsubsection{La convolution dans l'image : Une séquence spatiale 2D}
Dans le contexte de l'image, la séquence est bidimensionnelle et le CNN y opère une extraction hiérarchique. Les premières couches détectent des primitives simples comme des bords ou des textures, qui sont ensuite combinées pour former des motifs sémantiques complexes. Cette capacité d'abstraction a été poussée à son paroxysme par l'architecture VGG \cite{simonyan_very_2015}, qui a standardisé l'usage de filtres de très petite taille ($3\times3$) empilés en grande profondeur. Les auteurs ont démontré qu'une séquence de petites convolutions est plus efficace pour capturer des non-linéarités complexes qu'une seule grande convolution. Cependant, l'augmentation de la profondeur a engendré des problèmes de disparition du gradient, résolus avec ResNet \cite{he_deep_2016}. L'introduction de connexions résiduelles a permis d'entraîner des réseaux dépassant la centaine de couches, essentiels pour capturer les dépendances à très longue portée dans des images haute résolution.
Pour les tâches de "traduction" d'image vers image, cruciales en simulation (par exemple, passer d'une carte de densité à un champ de pression), il est impératif de ne pas perdre l'information spatiale lors de la compression. L'architecture U-Net \cite{navab_u-net_2015}, initialement pour la segmentation biomédicale combine un chemin de contraction et un chemin d'expansion reliés par des connexions latérales (skip connections). Cette structure permet de générer une sortie de même résolution que l'entrée en fusionnant le contexte sémantique global et les détails locaux. Cette architecture est aujourd'hui une référence pour les modèles de substitution en physique. D'autres variantes comme DenseNet \cite{iandola_densenet_2014} ont poussé cette logique plus loin en connectant chaque couche à toutes les suivantes pour maximiser le flux d'information, bien que cela se fasse au prix d'une consommation mémoire accrue.


\subsubsection{La convolution dans les séquences 1D (Texte, Audio, Séries Temporelles)}
Bien que souvent associés à l'image, les CNN se sont révélés extrêmement performants pour traiter des séquences unidimensionnelles, surpassant parfois les réseaux récurrents grâce à leur capacité de parallélisation. Dans le traitement du signal audio, l'architecture WaveNet \cite{van_den_oord_wavenet_2016} a marqué une rupture en utilisant des convolutions causales dilatées. Ce mécanisme permet au champ récepteur du réseau de croître exponentiellement avec la profondeur sans augmenter le nombre de paramètres, capturant ainsi des dépendances temporelles sur des milliers de pas de temps, ce qui est impossible pour un RNN standard \textcolor{red}{REF}.
Dans le domaine du traitement du langage naturel (NLP), cette logique a été appliquée avec succès à la traduction automatique. L'architecture ConvS2S \cite{gehring_convolutional_2017} entièrement convolutionnelle pour la séquence à séquence, utilise des mécanismes d'attention multi-pas pour pondérer l'importance des mots sources. De même, ByteNet \cite{kalchbrenner_neural_2017}, réalise la traduction en temps linéaire en empilant des convolutions dilatées. Ces travaux ont démontré que l'induction de biais locaux propres aux convolutions est pertinente pour la syntaxe et la sémantique locale.
Cette approche a été généralisée aux séries temporelles génériques sous le nom de Temporal Convolutional Networks (TCN) \cite{bai_empirical_2018}. L'étude comparative démontre que sur une vaste gamme de tâches séquentielles, comme la prédiction de charge énergétique ou la modélisation de séquences symboliques, les TCN surpassent souvent les réseaux récurrents (LSTM/GRU) \textcolor{red}{REF} tout en offrant une stabilité d'entraînement supérieure. Une étude récente \cite{tay_are_2022} prolonge ce constat en suggérant que des architectures convolutionnelles modernes pré-entraînées peuvent rivaliser avec les Transformers sur certaines tâches textuelles, soulignant la pertinence continue de ce paradigme.

\subsubsection{Généralisation : De la grille volumétrique aux topologies irrégulières}Le principe de convolution, initialement restreint aux images planes, a fait l'objet d'extensions successives pour traiter des structures de données de plus en plus complexes. Une première étape de généralisation concerne les données volumétriques (3D) et spatio-temporelles, qui conservent toutefois une structure de grille régulière (euclidienne). Pour l'analyse de vidéos, C3D \cite{tran_learning_2015} déploie des filtres tridimensionnels ($x, y, t$) capturant conjointement l'espace et le mouvement. De même, dans le domaine médical, l'architecture V-Net \cite{milletari_v-net_2016} étend le principe du U-Net à la 3D en opérant sur des voxels. Bien que performantes, ces méthodes restent contraintes par la régularité de la grille : elles imposent une voxelisation des données qui induit une complexité cubique et une perte de résolution, les rendant inadaptées aux géométries éparses.

La généralisation la plus significative pour la simulation scientifique concerne donc les données non-euclidiennes, intrinsèquement irrégulières. Dans une simulation lagrangienne ou un système moléculaire, le "voisinage" n'est plus défini par une position dans une matrice, mais par la topologie d'un graphe. Les Graph Convolutional Networks (GCN) \cite{kipf_semi-supervised_2016} redéfinissent alors la convolution comme une agrégation spectrale ou spatiale des caractéristiques des nœuds connectés, indépendamment de leur disposition géométrique absolue. Cette approche a été enrichie par GraphSAGE \cite{hamilton_inductive_2017}, proposant une convolution inductive capable de généraliser à des nœuds invisibles durant l'entraînement. Enfin, pour traiter des nuages de points 3D bruts sans passer par la case voxelisation, des architectures comme PointNet++ \cite{qi_pointnet_2017} appliquent des opérations hiérarchiques directement sur des ensembles continus, comblant le fossé entre convolution discrète et géométrie continue.


\subsection{Réseaux de neurones récurrents et Espaces d'Etats (RNN et SSM)}
Si les réseaux de convolution abordent la séquence par une fenêtre glissante locale, une autre famille d'architectures adopte une approche intrinsèquement temporelle : la modélisation récursive. Qu'il s'agisse des Réseaux de Neurones Récurrents (RNN) historiques ou des récents Modèles d'Espaces d'États (SSM), le principe fondateur reste la persistance de l'information. Le modèle maintient un état caché interne $h_t$ qui agit comme une mémoire compressée de tout l'historique passé, mise à jour à chaque nouvelle observation. Cette formulation est particulièrement naturelle pour la simulation physique, car elle mime la dynamique des systèmes causaux où l'état futur dépend de l'état présent et des forces appliquées.

\subsubsection{Genèse et mécanismes d'interaction : De la boucle simple aux portes logiques}
L'histoire de cette approche débute avec les RNN classiques \cite{elman_finding_1990} qui introduisent une boucle de rétroaction permettant au réseau de maintenir une trace du contexte temporel. Cependant, bien que ces réseaux parviennent à générer des séquences continues complexes comme de l'écriture manuscrite, ils souffrent d'une instabilité critique lors de l'entraînement : le problème de la disparition ou de l'explosion du gradient \cite{graves_generating_2014}. Sur de longues séquences, le signal d'erreur se dilue, empêchant l'apprentissage des causes lointaines d'un événement. Pour y remédier, le LSTM (Long Short-Term Memory) \cite{hochreiter_long_1997} propose des "cellules" mémoires protégées par des portes logiques, et peut choisir de retenir ou d'effacer une information sur des milliers de pas de temps. Cette capacité a été affinée par l'introduction du GRU \cite{cho_properties_2014}, \cite{chung_empirical_2014}, une variante plus économe.


\subsubsection{Mécanisme d'interaction : Récurrence et Mémoire d'État}
L'interaction fondamentale des architectures récurrentes (RNN) et des modèles d'espaces d'états (SSM) avec la séquence repose sur un principe de persistance de l'information, radicalement différent de la localité spatiale des convolutions. Au lieu d'agréger un voisinage statique, ces modèles introduisent une variable latente dynamique, l'état caché $h_t$, qui agit comme une mémoire compressée de l'historique causal. L'opération centrale n'est plus un filtrage, mais une mise à jour récursive : à chaque pas de temps, l'état courant est recalculé en fonction de l'état précédent et de la nouvelle observation. Mathématiquement, pour une séquence d'entrée $x$, cette dynamique s'exprime par une équation de transition d'état générique :
$$h_t = f(h_{t-1}, x_t; \theta)$$
où $f$ est une fonction paramétrée par $\theta$ (matrices de poids dans un RNN, matrices d'état dans un SSM). Cette formulation implique que $h_t$ ne dépend pas seulement de l'entrée locale $x_t$, mais indirectement de toute la trajectoire passée $\{x_0, \dots, x_t\}$ accumulée dans $h_{t-1}$.
En pratique, cette dynamique de mise à jour est régie par un ensemble de paramètres apprenables qui sont partagés sur toute la longueur de la séquence, garantissant l'invariance temporelle du traitement. Ces paramètres se composent d'une matrice $W_{ih}$ (Input-to-Hidden) qui projette l'entrée courante dans l'espace latent, d'une matrice $W_{hh}$ (Hidden-to-Hidden) qui contrôle l'évolution de la mémoire interne, et d'un vecteur de biais $b$. En notant $\phi$ la fonction d'activation non-linéaire, l'équation de récurrence s'écrit pour notre exemple : \\
$$h_{t+1} = \phi(W_{ih}x_t + W_{hh}h_t + b)$$

L'illustration \ref{RNN base} permet de visualiser la propagation de la dépendance temporelle à travers le réseau. Le flux d'information, matérialisé par les flèches horizontales, transporte l'état caché d'un pas de temps à l'autre, agissant comme une mémoire cumulative. Ainsi, le calcul de l'état caché $h_3$ (représenté par le carré vert) intègre non seulement l'information de l'entrée courante, mais également celle de l'état caché précédent. Par récurrence, cette mémoire transporte déjà les traces des observations passées ($x_1, x_2$), créant un lien causal ininterrompu.
\begin{equation*}
\begin{split}
	h_3 & = \phi(W_{2}x_2 + W_{1}h_2) \\
	&= \phi(W_{2}x_2 + W_{1}\phi(W_{2}x_1 + W_{1}h_1))
\end{split}
\end{equation*}
Cette formulation met en évidence la différence fondamentale avec la convolution : alors que le champ récepteur d'un CNN s'élargit progressivement par empilement de couches, ici le cône d'influence (zone violette) s'étend horizontalement vers le passé de manière théoriquement infinie, capturant la totalité de l'historique causal disponible.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.35]{BasicRNN.png}
\end{center}
\caption{Mécanisme fondamental de la récurrence et propagation de l'état mémoire}
\label{RNN base}
\end{figure}

La persistance de l'état caché offre une flexibilité architecturale concernant la topologie des entrées-sorties et permet de découpler la lecture de l'écriture selon deux paradigmes distincts. Le mode "Flux à Flux" (ou Many-to-Many synchronisé), illustré à gauche dans la figure \ref{RNN comparaison}, aligne la production de la sortie sur la réception de l'entrée. À chaque pas de temps $t$, l'état caché $h_t$ est utilisé immédiatement pour prédire une sortie $y_t$. Cette configuration, où la causalité est stricte et le délai minimal, est caractéristique des systèmes de filtrage en ligne ou de contrôle, où la réaction doit être instantanée. Le second mode, correspondant au paradigme "Séquence vers Séquence" (Seq2Seq), à droite dans la figure \ref{RNN comparaison}, nécessite d'opérer en deux phases distinctes. La phase d'encodage représenté en violet, ingurgite toutes les informations et les condenses dans l'état caché $h_5$. La phase de décodage récupère ce contexte pour générer la séquence de sortie. Ce mécanisme permet de transformer une séquence en une autre de longueur différente et de modéliser des dépendances non-monotones, mais impose que toute l'information pertinente soit compressée dans un goulot d'étranglement. C'est cette distinction topologique, plus que la nature des données, qui différencie fondamentalement l'usage des RNN pour le suivi de signal (mode flux) de leur usage pour la traduction (mode Seq2Seq).

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{ComparaisonRNN.png}
\end{center}
\caption{Topologies d'application des architectures récurrentes : traitement de flux (gauche) et traduction globale (droite)}
\label{RNN comparaison}
\end{figure}

\subsubsection{Renouveau architectural : Les Modèles d'Espaces d'Etats (SSM)}
Malgré leur robustesse, les LSTM conservent une limitation structurelle majeure : leur traitement séquentiel interdit la parallélisation sur GPU. C'est pour lever ce verrou qu'une nouvelle classe de modèles a émergé : les State Space Models (SSM). Ces modèles puisent leur origine théorique dans le papier HiPPO \cite{gu_hippo_2020}, qui formalise mathématiquement comment compresser optimalement une histoire continue dans un vecteur de taille fixe via des projections polynomiales orthogonales. Cette base a permis de développer S4 (Structured State Space sequence model) \cite{gu_efficiently_2022}, capable de modéliser des dépendances sur plus de 10 000 pas de temps en résolvant une équation différentielle continue discrétisée.
Cependant, les premiers SSM souffraient d'une rigidité dynamique, peinant à sélectionner l'information pertinente en fonction du contexte ("Content-based selection"). Cette limite a été adressée par l'architecture Mamba \cite{gu_mamba_2024}. En rendant les matrices d'état dépendantes de l'entrée, Mamba atteint des performances comparables aux premiers Transformers \textcolor{red}{REF} tout en conservant une complexité linéaire. Toutefois, ces modèles restent délicats à stabiliser sur des dynamiques hautement chaotiques où la discrétisation numérique peut introduire des dérives.

\subsubsection{Application au texte : L'ère du Sequence-to-Sequence et de la Traduction}
Dans le traitement du langage, l'approche récurrente a connu son apogée avec le paradigme Seq2Seq \cite{sutskever_sequence_2014}. En utilisant deux LSTM (un encodeur et un décodeur), cette architecture a permis de traiter des séquences de longueurs variables. Cette avancée a transformé l'industrie de la traduction avec le déploiement du Google’s Neural Machine Translation System (GNMT) \cite{wu_googles_2016} en 2016, réduisant les erreurs de traduction de près de 60 \% par rapport aux systèmes statistiques. Au-delà de la traduction, ce paradigme a permis des avancées dans la modélisation prédictive de parcours complexes, comme l'illustré par le modèle Doctor AI \cite{choi_doctor_2016}. Ce modèle utilise des RNN pour prédire les futurs diagnostics médicaux et la durée avant la prochaine visite à partir de l'historique clinique des patients, démontrant la capacité des RNN à capturer des dynamiques temporelles irrégulières et multivariées dans des données réelles bruitées.

\subsubsection{Application aux systèmes temporels, physiques et créatifs}
Au-delà du texte, les RNN se sont imposés comme l'outil naturel pour la modélisation de systèmes dynamiques continus, un domaine crucial pour la simulation. Une étude sur la prévision de systèmes chaotiques \cite{vlachas_data-driven_2018} a mis en avant que les LSTM pouvaient apprendre la dynamique de l'attracteur de Lorenz ou de l'équation de Kuramoto-Sivashinsky mieux que les modèles physiques simplifiés, en capturant les propriétés non-linéaires de l'évolution temporelle à court terme. Cette capacité à modéliser le chaos déterministe fait des RNN des candidats sérieux pour accélérer les simulations de mécanique des fluides turbulents.
Dans l'industrie, cette robustesse est exploitée pour la prévision probabiliste avec DeepAR \cite{salinas_deepar_2020}, utilisé par Amazon pour sa chaîne logistique. Ce modèle apprend une distribution de probabilité future à chaque pas de temps, permettant de quantifier l'incertitude via des simulations de Monte Carlo. Enfin, la capacité "générative constructive" des RNN a été pionnière dans la création artistique. Le modèle Performance RNN \cite{oore_this_2020}, développé par Google Magenta, a montré qu'un LSTM pouvait générer des performances de piano expressives (avec nuances de vélocité et de timing) en traitant la musique non pas comme une partition rigide, mais comme une séquence temporelle continue d'événements, prouvant que les RNN peuvent capturer des structures hiérarchiques globales (phrasé musical) tout en gérant des détails micro-temporels.


\subsection{Transformer}
Si les réseaux récurrents ont introduit la mémoire et les réseaux convolutionnels la localité, l'architecture Transformer a proposé un changement de paradigme radical en postulant que l'interaction entre les éléments d'une séquence doit être modélisée par une relation directe de contenu à contenu, et non par une contrainte de proximité spatiale ou temporelle. Cette architecture, devenue l'épine dorsale de l'IA générative moderne, repose sur le mécanisme d'attention.

\subsubsection{Histoire : De l'alignement au "Pointer Network" et à l'Attention pure}
L'émergence du Transformer est le fruit d'une lente maturation visant à résoudre le goulot d'étranglement des architectures Encodeur-Décodeur récurrentes (RNN) \textcolor{red}{REF}. Dans le paradigme Seq2Seq classique \cite{sutskever_sequence_2014}, toute l'information de la phrase source devait être compressée dans un unique vecteur de contexte de taille fixe, entraînant une perte d'information critique sur les longues séquences. Une première solution \cite{bahdanau_neural_2014} introduit un mécanisme d'attention additive permettant au décodeur de "chercher" (search) et d'aligner (align) les parties pertinentes de la phrase source à chaque étape de la génération. Ici, l'attention n'était encore qu'un module auxiliaire greffé sur des RNN.
Une seconde étape conceptuelle fut franchie avec les Pointer Networks \cite{vinyals_pointer_2015} où le réseau de neurones peut apprendre à résoudre des problèmes combinatoires (comme l'enveloppe convexe) en utilisant l'attention comme un pointeur pour sélectionner des éléments de l'entrée comme sortie, plutôt que de générer des symboles abstraits. Cela a ancré l'idée que le mécanisme de sélection basé sur le contenu ("Content-based addressing") était suffisamment puissant pour structurer la sortie.
La rupture définitive survient avec l'article Attention Is All You Need \cite{vaswani_attention_2017}. Les auteurs ont démontré que la récurrence, jugée jusqu'alors indispensable pour encoder l'ordre séquentiel, était en réalité superflue et limitante pour la parallélisation. En ne conservant que le mécanisme d'attention (devenu Self-Attention), ils ont permis un traitement parallèle massif des séquences, réduisant la distance de propagation de l'information entre deux mots quelconques à une constante $O(1)$, contre $O(N)$ pour un RNN.

\subsubsection{Mécanisme d'interaction et complexité}
Contrairement aux architectures précédentes qui traitent la séquence par voisinage spatial ou récursivité temporelle, le Transformer repose sur un mécanisme d'interaction directe et globale : l'attention. Ce processus permet à chaque élément de la séquence de construire sa propre représentation en agrégeant l'information de tous les autres éléments, pondérée par leur pertinence contextuelle. Cette interaction est formalisée par le mécanisme "Query-Key-Value". Pour chaque élément d'entrée $x_i$, trois vecteurs sont générés par projection linéaire via des matrices de poids apprenables $W^Q, W^K, W^V$ : une Requête $q_i = x_i W^Q$, une Clé $k_i = x_i W^K$ et une Valeur $v_i = x_i W^V$.\\

Le cœur du calcul réside dans la mesure de compatibilité entre ces vecteurs, appelé score d'attention. Pour construire une nouvelle représentation d'un élément $x_i$ dans son contexte (c'est-à-dire le reste de la séquence), la requête associé $q_i$ est comparée aux clés de chaque élément de la séquence $(k_j)_j$, formant une séquence de score d'attention $(a_{i,j})_j$. Cette comparaison s'effectue souvent par un produit scalaire, mis à l'échelle par la racine de la dimension des clés $d_{att}$ pour stabiliser les gradients. Le score d'attention brute $a_{i,j}$ est généralement donné par:
$$a_{i,j} = \frac{\langle q_i, k_j \rangle}{\sqrt{d_{att}}} $$
Ces scores bruts sont ensuite convertis en une distribution de probabilité $(p_{i,j})_j$ par l'application d'une fonction Softmax :
$$p_{i,j} = \text{softmax}((a_{i,k})_k)_j = \frac{\exp(a_{i,j})}{\sum_{k} \exp(a_{i,k})}$$
La nouvelle représentation de $x_i$, notée $y_i$, est calculée comme une somme des vecteurs de Valeur $(v_j)_j$ pondérés par les poids d'attention $(p_{i,j})_j$ :
$$y_i = \sum_{j} p_{i,j} v_j$$
Il est intéressant de noter que l'opération d'attention est invariante par permutation (elle traite la séquence comme un ensemble, un "sac de mots"). Il est donc nécessaire d'injecter explicitement d'informations de position (Positional Encodings) a priori dans la séquence $(x_i)_i$ pour reconstruire la topologie temporelle ou spatiale de la donnée si elle n'y est pas présent initialement.\\

L'illustration \ref{self attention} détaille le processus du calcul de la représentation contextuelle d'un élément cible $x_4$ (représenté en vert). Comme expliqué à l'instant, les éléments de la séquence sont projetés dans l'espace des requêtes, clés et valeurs, les scores d'attention sont calculés puis la pondération d'attention en est déduite et la nouvelle représentation de $x_4$ que nous notons $y_4$, est calculée en effectuant une somme des valeurs pondérées par les poids d'attention :
\begin{align*}
	q_4 &= x_4 W^Q, & k_j &= x_j W^K, & v_i &= x_i W^V \\
	a_{4,j} &= \frac{\langle q_4, k_j \rangle}{\sqrt{d_{att}}}, & p_{4,j} &= \frac{\exp(a_{4,j})}{\sum_{k} \exp(a_{4,k})}, & y_4 &= \sum_{j} p_{4,j} v_j
\end{align*}
Ainsi, le vecteur résultant $y_4$ est une synthèse dynamique du contenu de la séquence.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{self_attention.png}
\end{center}
\caption{Illustration du mécanisme d'Auto-Attention par produit scalaire}
\label{self attention}
\end{figure}

Le mécanisme canonique d'auto-attention se décline en deux variantes fondamentales pour répondre à des contraintes structurelles spécifiques : le respect de la causalité temporelle et l'intégration d'informations exogènes. La première variante, l'Attention Masquée (Masked Self-Attention), est indispensable aux tâches de génération séquentielle ou de simulation, où la prédiction de l'état présent ne doit physiquement pas dépendre du futur. Cette causalité est induite par une modification de la matrice des scores d'attention avant l'étape de normalisation : en forçant vers $-\infty$ les scores associés aux indices futurs, on garantit que la fonction Softmax leur attribue une probabilité strictement nulle. La partie gauche de la figure \ref{other attention} illustre ce mécanisme : lors du calcul de la représentation pour la position 4 (carré vert), l'accès aux positions ultérieures 5 et 6 est bloqué par le masque. Le vecteur de sortie $y_4$ est ainsi construit exclusivement par l'agrégation des valeurs passées et présentes ($v_1$ à $v_4$), préservant l'intégrité causale du flux de données.

La seconde variante, l'Attention Croisée (Cross-Attention), permet le transfert d'information entre deux séquences distinctes, une opération centrale pour les tâches de traduction ou de reconstruction conditionnelle. Cette architecture repose sur une distribution asymétrique des rôles : la séquence source (qui détient l'information) projette les Clés ($K$) et les Valeurs ($V$), tandis que la séquence cible (qui cherche à s'enrichir ou se construire) émet les Requêtes ($Q$). La partie droite de la figure \ref{other attention} détaille cette interaction : la séquence du bas représente le flux cible, dont le troisième élément émet une requête $q_3$. Celle-ci est comparée à l'ensemble des clés $k$ issues de la séquence source (au milieu), permettant de pondérer les valeurs $v_1$ à $v_6$ correspondantes. Le vecteur résultant est donc une injection dynamique du contexte source pertinent au sein de la trajectoire cible, pilotée par les besoins de cette dernière.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{other_attention.png}
\end{center}
\caption{Adaptations du mécanisme d'attention : restriction causale (gauche) et interaction inter-séquences (droite)}
\label{other attention}
\end{figure}

L'expressivité du Transformer repose sur la parallélisation du mécanisme d'attention unitaire et son intégration dans différents blocs. Pour capturer des relations de natures variées (syntaxiques, sémantiques, ou causales par exemple) à différentes échelles, le modèle utilise l'Attention Multi-Têtes (Multi-Head Attention). Au lieu de calculer une unique matrice d'attention sur la dimension totale du modèle $d_{model}$, l'entrée est projetée linéairement $h$ fois dans des sous-espaces de dimension réduite $d_{att} = d_{model}/h$. Chaque "tête" calcule sa propre attention indépendamment, permettant au réseau de se focaliser simultanément sur différents aspects de la séquence. Les sorties de ces $h$ têtes sont ensuite concaténées et reprojetées par une matrice linéaire finale $W^O$ pour restaurer la dimension originale. Mathématiquement, cela permet de recombiner les informations extraites de chaque sous-espace pour former une représentation unifiée.\\

La illustration \ref{encoder transformer} présente l'architecture de l'Encodeur, dédiée à l'analyse et à la construction d'une représentation contextuelle robuste de la séquence d'entrée. Elle est constituée d'un empilement de $N$ blocs identiques. Chaque bloc s'articule autour de deux sous-modules fonctionnels : l'Attention Multi-Têtes, qui capture les interactions globales entre les différentes positions, et un réseau de neurones dense (Feed-Forward Network - FFN) appliqué indépendamment à chaque position pour traiter les caractéristiques. Pour permettre de la profondeur au réseau, chaque sous-module est systématiquement encapsulé par une connexion résiduelle - qui additionne l'entrée du module à sa sortie pour préserver le flux de gradient - suivie d'une normalisation de couche (Layer Norm) assurant stabilité et une bonne propagation du gradient dans toutes les couches. Enfin, l'architecture étant invariante par permutation, l'ajout dès l'entrée d'un Encodage Positionnel est indispensable pour injecter la topologie temporelle ou spatiale dans les représentations vectorielles.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{enc_transformer.png}
\end{center}
\caption{Architecture du bloc Encodeur du Transformer}
\label{encoder transformer}
\end{figure}

L'illustration \ref{decoder transformer} détaille l'architecture du Décodeur, conçue pour la génération autorégressive. Bien qu'elle hérite de la structure modulaire stratifiée de l'encodeur, elle s'en distingue par l'intégration dans chaque bloc de mécanismes spécifiques dédiés à la prédiction. Le premier module est une Attention Multi-Têtes Masquée (Masked Self-Attention). Ce composant permet aux éléments de la séquence cible d'assimiler exclusivement les informations des états antérieurs, garantissant le respect de la causalité temporelle nécessaire à la génération. Le décodeur se singularise ensuite par l'insertion d'un troisième sous-module, intercalé avant le FFN : l'Attention Croisée (Cross-Attention). Ce module est l'interface de conditionnement du modèle ; il incorpore à la séquence cible (qui fournit les requêtes $Q$) l'information contextuelle extraite d'une source externe (l'encodeur, qui fournit les clés $K$ et les valeurs $V$). Chacun de ces trois sous-modules (Masked Self-Attention, Cross-Attention, FFN) est soumis au même schéma de stabilisation par connexion résiduelle et normalisation que l'encodeur, assurant une cohérence dynamique à travers tout le réseau.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{dec_transformer.png}
\end{center}
\caption{Architecture du bloc Décodeur de Transformer}
\label{decoder transformer}
\end{figure}

Dans une configuration complète de type "Séquence vers Séquence" (Seq2Seq), telle que celle utilisée pour la traduction automatique ou la simulation physique, la sortie de la pile d'encodeurs est connectée à l'entrée source de la pile de décodeurs. Cette architecture bipartite, illustrée figure \ref{full transformer}, permet de transformer une séquence d'entrée complexe (scénario tactique, signal bruité) en une représentation latente continue. À partir de cet espace, le décodeur reconstruit pas à pas la séquence de sortie cible (signal reconstruit), agissant ainsi comme un traducteur universel capable de modéliser des fonctions de transfert hautement non-linéaires entre des signaux de natures variées.

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{full_transformer.png}
\end{center}
\caption{Architecture Transformer complète pour le paradigme Séquence-vers-Séquence}
\label{full transformer}
\end{figure}

\subsubsection{Le Transformer dans le texte : La divergence des architectures}
Dans le traitement du langage naturel (NLP), le Transformer a provoqué une véritable explosion cambrienne des modèles, se scindant en trois familles distinctes.
La première, celle des Encodeurs, est incarnée par BERT \cite{devlin_bert_2019}. Utilisant une attention bidirectionnelle, ces modèles excellent dans la compréhension et la classification, car chaque mot a accès au contexte passé et futur simultanément.
La seconde, celle des Décodeurs, est dominée par la lignée GPT \cite{radford_improving_2018}, \cite{brown_language_2020}. Ici, l'attention est causale (masquée vers le futur), optimisée pour la génération autorégressive. C'est cette branche qui a mis en évidence les "lois d'échelle" (Scaling Laws), montrant que la performance de prédiction du prochain token suit une loi de puissance en fonction du nombre de paramètres et de données, ouvrant la voie aux gros modèles de langage (Large Language Model - LLM) actuels.
La troisième famille, Encodeur-Décodeur, reste fidèle à l'architecture originale \cite{vaswani_attention_2017} pour les tâches de traduction ou de résumé. Le modèle T5 \cite{raffel_exploring_2020} a poussé ce paradigme à son extrême en reformulant toute tâche NLP (y compris la classification) comme un problème de génération de texte-vers-texte.

\subsubsection{Le Transformer dans les systèmes temporels : Promesses et controverses}
L'application des Transformers aux séries temporelles continues (consommation énergétique, trafic, météo) a fait l'objet de recherches intenses \cite{wen_transformers_2023}. L'attrait principal réside dans la capacité théorique de l'attention à capturer des corrélations à très long terme et des saisonnalités complexes que les RNN peinent à retenir.
Des architectures spécifiques ont été proposées pour briser la complexité quadratique. Informer  \cite{zhou_informer_2021} introduit une attention "ProbSparse" pour sélectionner uniquement les interactions dominantes, réduisant la complexité à $O(N \log N)$. Autoformer \cite{wu_autoformer_2021} va plus loin en remplaçant le produit scalaire par une auto-corrélation pour mieux capturer les périodicités.
Cependant, l'efficacité réelle des Transformers sur des signaux continus est contestée. Une étude \cite{zeng_are_2023} assure qu'un simple modèle linéaire bien calibré (DLinear) surpassait souvent des Transformers complexes sur les benchmarks standards. La raison invoquée est que l'attention, conçue pour la sémantique discrète, tend à sur-interpréter le bruit dans les signaux continus et perd l'information d'ordre temporel cruciale, malgré les encodages positionnels. Néanmoins, des approches récentes comme PatchTST \cite{nie_time_2023}, qui segmentent le signal en patchs (comme en vision) avant d'appliquer l'attention, semblent redonner l'avantage aux Transformers en traitant des dynamiques locales plutôt que des points isolés.

\subsubsection{Le Transformer dans l'image : Patchs et hiérarchie}
L'hégémonie des CNN en vision a été remise en cause par le Vision Transformer (ViT) \cite{dosovitskiy_image_2020}. En découpant l'image en une séquence de patchs carrés traités comme des mots, ViT a prouvé qu'un Transformer pur, sans biais inductif de convolution, pouvait atteindre l'état de l'art, à condition d'être pré-entraîné sur des volumes de données massifs (JFT-300M, \cite{sun_revisiting_2017}).
Pour pallier le coût quadratique sur les images haute résolution et le manque de localité, l'architecture Swin Transformer \cite{liu_swin_2021} a réintroduit une structure hiérarchique similaire aux CNN. En calculant l'attention uniquement à l'intérieur de fenêtres locales glissantes (Shifted Windows), Swin combine la modélisation globale des Transformers avec l'efficacité locale des convolutions, devenant le standard actuel pour la segmentation et la détection d'objets.

\subsubsection{Généralisation : Physique et Prise de décision}
La capacité du Transformer à modéliser des graphes d'interaction arbitraires en fait un outil puissant pour la physique et la biologie. L'exemple le plus spectaculaire est AlphaFold 2 \cite{jumper_highly_2021}, qui a résolu le problème du repliement des protéines. Son module central, l'Evoformer, est une variante du Transformer qui traite la protéine comme un graphe dynamique, mettant à jour itérativement la représentation de la séquence d'acides aminés et la matrice de distances 3D par des mécanismes d'attention triangulaire.
Enfin, dans le domaine du contrôle et de la simulation, le Decision Transformer \cite{chen_decision_2021} a reformulé l'apprentissage par renforcement comme un problème de modélisation de séquence. Au lieu d'estimer des fonctions de valeur ou des gradients de politique, ce modèle prédit simplement l'action suivante conditionnée par les états passés et la récompense désirée (le "Return-to-go"). Cette approche "générative" du contrôle permet d'appliquer les techniques de pré-entraînement des LLM à la robotique ou à la navigation d'agents autonomes, unifiant ainsi perception, prédiction physique et prise de décision sous une même architecture.


\subsection{Ancrage dans la problématique}
L'exploration des architectures de traitement de séquence met en lumière un éventail de mécanismes complémentaires pour la modélisation de notre simulateur de capteur, dont la pertinence doit être pondérée par les contraintes spécifiques des signaux radar. Les réseaux convolutionnels, par leur biais inductif de localité, offrent une approche adaptée pour modéliser les interactions à courte portée, telles que les interférences immédiates entre impulsions proches au sein d'un même train. Cependant, leur architecture à fenêtre glissante impose une limitation structurelle majeure : la difficulté à maintenir un état mémoire persistant sur des horizons temporels arbitrairement longs, ce qui peut s'avérer insuffisant pour reproduire fidèlement les processus de pistage temporel qui nécessitent de lier des événements très distants.

De leur côté, les architectures récurrentes (RNN) et les modèles d'espaces d'états (SSM) présentent une affinité naturelle avec la causalité physique du capteur, mimant le comportement des algorithmes de traitement du signal qui mettent à jour des pistes au gré des réceptions. Néanmoins, leur usage impliquerait un changement de paradigme par rapport à notre approche orientée "traduction". Ces modèles excellent dans le traitement séquentiel flux à flux, mais leur application à une tâche de transformation globale de séquence (Seq2Seq) sur de très longs scénarios est complexe. Le goulot d'étranglement du vecteur de contexte, censé compresser toute l'information de la séquence d'entrée avant la génération, devient rapidement prohibitif face à la densité des données radar, limitant leur capacité à reconstruire fidèlement l'ensemble du scénario en une seule passe.

Enfin, l'architecture Transformer et le mécanisme d'attention apportent une capacité de modélisation contextuelle globale, permettant à chaque impulsion d'interagir directement avec l'ensemble de la séquence. Cette propriété est puissante pour capturer des corrélations complexes non-locales et apprendre la fonction de transfert globale du simulateur sans les contraintes de compression des RNN. Toutefois, l'application de ce modèle exige une vigilance particulière quant à sa complexité quadratique, qui peut devenir prohibitive face à la haute densité des flux d'impulsions radar, ainsi qu'à la nécessité d'adapter l'encodage positionnel pour traiter le temps continu irrégulier des PDW plutôt que des indices discrets.


