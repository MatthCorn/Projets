
@article{lowe_distinctive_2004,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {1573-1405},
	url = {https://doi.org/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	pages = {91--110},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	urldate = {2025-11-26},
	date = {2004-11-01},
	langid = {english},
	keywords = {image matching, invariant features, object recognition, scale invariance},
}

@inproceedings{bay_surf_2006,
	location = {Berlin, Heidelberg},
	title = {{SURF}: Speeded Up Robust Features},
	isbn = {978-3-540-33833-8},
	doi = {10.1007/11744023_32},
	shorttitle = {{SURF}},
	abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined {SURF} (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.},
	pages = {404--417},
	booktitle = {Computer Vision – {ECCV} 2006},
	publisher = {Springer},
	author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	date = {2006},
	langid = {english},
	keywords = {Hessian Matrix, Integral Image, Interest Point, Robust Feature, Viewpoint Change},
	file = {Full Text PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\F5N64SZI\\Bay et al. - 2006 - SURF Speeded Up Robust Features.pdf:application/pdf},
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/1467360},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear {SVM} based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient ({HOG}) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original {MIT} pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	eventtitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	pages = {886--893 vol. 1},
	booktitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	urldate = {2025-11-26},
	date = {2005-06},
	note = {{ISSN}: 1063-6919},
	keywords = {High performance computing, Histograms, Humans, Image databases, Image edge detection, Object detection, Object recognition, Robustness, Support vector machines, Testing},
	file = {Snapshot:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\5ML459NY\\1467360.html:text/html;Version soumise:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\U363T6IM\\Dalal et Triggs - 2005 - Histograms of oriented gradients for human detection.pdf:application/pdf},
}

@article{lecun_gradient-based_2002,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	url = {https://ieeexplore.ieee.org/abstract/document/726791/},
	pages = {2278--2324},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	author = {{LeCun}, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	urldate = {2025-11-26},
	date = {2002},
	note = {Publisher: Ieee},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\3FEW4CCR\\LeCun et al. - 2002 - Gradient-based learning applied to document recognition.pdf:application/pdf},
}

@article{krizhevsky_imagenet_2012,
	title = {Imagenet classification with deep convolutional neural networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	journaltitle = {Advances in neural information processing systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2025-11-26},
	date = {2012},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\PYCWHHN4\\Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional neural networks.pdf:application/pdf},
}

@misc{simonyan_very_2015,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	number = {{arXiv}:1409.1556},
	publisher = {{arXiv}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2025-11-26},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\UMNX5VVP\\Simonyan et Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep residual learning for image recognition},
	url = {http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	pages = {770--778},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2025-11-26},
	date = {2016},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\NK6SITTW\\He et al. - 2016 - Deep residual learning for image recognition.pdf:application/pdf},
}

@incollection{navab_u-net_2015,
	location = {Cham},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
	shorttitle = {U-Net},
	pages = {234--241},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	urldate = {2025-11-26},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-24574-4_28},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\GAS6KD7Q\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf},
}

@article{van_den_oord_wavenet_2016,
	title = {Wavenet: A generative model for raw audio},
	volume = {12},
	url = {https://www.academia.edu/download/61836013/WAVENET_-_A_GENERATIVE_MODEL_FOR_RAW_AUDIO_-_1609.0349920200120-19152-1e964lf.pdf},
	shorttitle = {Wavenet},
	pages = {1},
	journaltitle = {{arXiv} preprint {arXiv}:1609.03499},
	author = {Van Den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2025-11-26},
	date = {2016},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\GUAIA67U\\Van Den Oord et al. - 2016 - Wavenet A generative model for raw audio.pdf:application/pdf},
}

@inproceedings{gehring_convolutional_2017,
	title = {Convolutional sequence to sequence learning},
	url = {https://proceedings.mlr.press/v70/gehring17a},
	pages = {1243--1252},
	booktitle = {International conference on machine learning},
	publisher = {{PMLR}},
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\66EKYV2X\\Gehring et al. - 2017 - Convolutional sequence to sequence learning.pdf:application/pdf},
}

@misc{kalchbrenner_neural_2017,
	title = {Neural Machine Translation in Linear Time},
	url = {http://arxiv.org/abs/1610.10099},
	doi = {10.48550/arXiv.1610.10099},
	abstract = {We present a novel neural network for processing sequences. The {ByteNet} is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The {ByteNet} uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The {ByteNet} decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The {ByteNet} also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German {WMT} translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
	number = {{arXiv}:1610.10099},
	publisher = {{arXiv}},
	author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
	urldate = {2025-11-26},
	date = {2017-03-15},
	eprinttype = {arxiv},
	eprint = {1610.10099 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\KFHPXHG8\\Kalchbrenner et al. - 2017 - Neural Machine Translation in Linear Time.pdf:application/pdf},
}

@article{bai_empirical_2018,
	title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	journaltitle = {{arXiv} preprint {arXiv}:1803.01271},
	author = {Bai, Shaojie},
	date = {2018},
}

@misc{tay_are_2022,
	title = {Are Pre-trained Convolutions Better than Pre-trained Transformers?},
	url = {http://arxiv.org/abs/2105.03322},
	doi = {10.48550/arXiv.2105.03322},
	abstract = {In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or {CNN}, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on 8 datasets/tasks, we find that {CNN}-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.},
	number = {{arXiv}:2105.03322},
	publisher = {{arXiv}},
	author = {Tay, Yi and Dehghani, Mostafa and Gupta, Jai and Bahri, Dara and Aribandi, Vamsi and Qin, Zhen and Metzler, Donald},
	urldate = {2025-11-26},
	date = {2022-01-30},
	eprinttype = {arxiv},
	eprint = {2105.03322 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\JG7S6YPH\\Tay et al. - 2022 - Are Pre-trained Convolutions Better than Pre-trained Transformers.pdf:application/pdf},
}

@inproceedings{tran_learning_2015,
	title = {Learning spatiotemporal features with 3d convolutional networks},
	url = {http://openaccess.thecvf.com/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html},
	pages = {4489--4497},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	urldate = {2025-11-26},
	date = {2015},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\VMXNXEV2\\Tran et al. - 2015 - Learning spatiotemporal features with 3d convolutional networks.pdf:application/pdf},
}

@inproceedings{milletari_v-net_2016,
	title = {V-net: Fully convolutional neural networks for volumetric medical image segmentation},
	url = {https://ieeexplore.ieee.org/abstract/document/7785132/},
	shorttitle = {V-net},
	pages = {565--571},
	booktitle = {2016 fourth international conference on 3D vision (3DV)},
	publisher = {Ieee},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	urldate = {2025-11-26},
	date = {2016},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\XVJ7G4HK\\Milletari et al. - 2016 - V-net Fully convolutional neural networks for volumetric medical image segmentation.pdf:application/pdf},
}

@article{kipf_semi-supervised_2016,
	title = {Semi-supervised classification with graph convolutional networks},
	url = {https://bibbase.org/service/mendeley/bfbbf840-4c42-3914-a463-19024f50b30c/file/25dbdd06-4704-a33f-23d9-c626b08adc1e/160902907.pdf.pdf},
	journaltitle = {{arXiv} preprint {arXiv}:1609.02907},
	author = {Kipf, T. N.},
	urldate = {2025-11-26},
	date = {2016},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\A2MJQ498\\Kipf - 2016 - Semi-supervised classification with graph convolutional networks.pdf:application/pdf},
}

@article{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
	journaltitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\LWICFZLN\\Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf},
}

@article{qi_pointnet_2017,
	title = {Pointnet++: Deep hierarchical feature learning on point sets in a metric space},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html},
	shorttitle = {Pointnet++},
	journaltitle = {Advances in neural information processing systems},
	author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\CQUKVHP8\\Qi et al. - 2017 - Pointnet++ Deep hierarchical feature learning on point sets in a metric space.pdf:application/pdf},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
	pages = {1--9},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2025-11-26},
	date = {2015},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\EMM3FDYM\\Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf},
}

@article{iandola_densenet_2014,
	title = {Densenet: Implementing efficient convnet descriptor pyramids},
	url = {https://arxiv.org/abs/1404.1869},
	shorttitle = {Densenet},
	journaltitle = {{arXiv} preprint {arXiv}:1404.1869},
	author = {Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
	urldate = {2025-11-26},
	date = {2014},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\35D7U5X6\\Iandola et al. - 2014 - Densenet Implementing efficient convnet descriptor pyramids.pdf:application/pdf},
}

@article{grieves_digital_2014,
	title = {Digital twin: manufacturing excellence through virtual factory replication},
	volume = {1},
	shorttitle = {Digital twin},
	pages = {1--7},
	number = {2014},
	journaltitle = {White paper},
	author = {Grieves, Michael},
	date = {2014},
}

@article{negri_review_2017,
	title = {A review of the roles of digital twin in {CPS}-based production systems},
	volume = {11},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978917304067},
	pages = {939--948},
	journaltitle = {Procedia manufacturing},
	author = {Negri, Elisa and Fumagalli, Luca and Macchi, Marco},
	urldate = {2025-11-26},
	date = {2017},
	note = {Publisher: Elsevier},
}

@inproceedings{dosovitskiy_carla_2017,
	title = {{CARLA}: An open urban driving simulator},
	url = {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
	shorttitle = {{CARLA}},
	pages = {1--16},
	booktitle = {Conference on robot learning},
	publisher = {{PMLR}},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\CVET4HXL\\Dosovitskiy et al. - 2017 - CARLA An open urban driving simulator.pdf:application/pdf},
}

@article{tao_digital_2018,
	title = {Digital twin in industry: State-of-the-art},
	volume = {15},
	url = {https://ieeexplore.ieee.org/abstract/document/8477101/},
	shorttitle = {Digital twin in industry},
	pages = {2405--2415},
	number = {4},
	journaltitle = {{IEEE} Transactions on industrial informatics},
	author = {Tao, Fei and Zhang, He and Liu, Ang and Nee, Andrew {YC}},
	urldate = {2025-11-26},
	date = {2018},
	note = {Publisher: {IEEE}},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\IXDYLWN5\\Tao et al. - 2018 - Digital twin in industry State-of-the-art.pdf:application/pdf},
}

@book{sherman_understanding_2018,
	title = {Understanding virtual reality: Interface, application, and design},
	url = {https://books.google.com/books?hl=fr&lr=&id=D-OcBAAAQBAJ&oi=fnd&pg=PP1&dq=Understanding+Virtual+Reality+:+Interface,+Application,+and+Design&ots=QT0icdfR4U&sig=7NVNO2ZhXIV7ehae-by0E-2w5u0},
	shorttitle = {Understanding virtual reality},
	publisher = {Morgan Kaufmann},
	author = {Sherman, William R. and Craig, Alan B.},
	urldate = {2025-11-26},
	date = {2018},
}

@book{fritzson_principles_2015,
	title = {Principles of object-oriented modeling and simulation with Modelica 3.3: a cyber-physical approach},
	url = {https://books.google.com/books?hl=fr&lr=&id=wgIaBgAAQBAJ&oi=fnd&pg=PR13&dq=Principles+of+Object-Oriented+Modeling+and+Si-+mulation+with+Modelica&ots=cZ60scKEkN&sig=SxTVWzN56d47byaUV6l_Qq0vZoE},
	shorttitle = {Principles of object-oriented modeling and simulation with Modelica 3.3},
	publisher = {John Wiley \& Sons},
	author = {Fritzson, Peter},
	urldate = {2025-11-26},
	date = {2015},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\NWYY4QR5\\Fritzson - 2015 - Principles of object-oriented modeling and simulation with Modelica 3.3 a cyber-physical approach.pdf:application/pdf},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} Gym},
	url = {http://arxiv.org/abs/1606.01540},
	doi = {10.48550/arXiv.1606.01540},
	abstract = {{OpenAI} Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
	number = {{arXiv}:1606.01540},
	publisher = {{arXiv}},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	urldate = {2025-11-26},
	date = {2016-06-05},
	eprinttype = {arxiv},
	eprint = {1606.01540 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\JHAPSSDU\\Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf},
}

@article{mildenhall_nerf_2022,
	title = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
	volume = {65},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3503250},
	doi = {10.1145/3503250},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (
              x
              ,
              y
              ,
              z
              ) and viewing direction (
              θ, ϕ
              )) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
	pages = {99--106},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2025-11-26},
	date = {2022-01},
	langid = {english},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\ZDFIBZNN\\Mildenhall et al. - 2022 - NeRF representing scenes as neural radiance fields for view synthesis.pdf:application/pdf},
}

@article{muller_instant_2022,
	title = {Instant neural graphics primitives with a multiresolution hash encoding},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern {GPUs}. We leverage this parallelism by implementing the whole system using fully-fused {CUDA} kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	pages = {1--15},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	urldate = {2025-11-26},
	date = {2022-07},
	langid = {english},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\YSHDXC6M\\Müller et al. - 2022 - Instant neural graphics primitives with a multiresolution hash encoding.pdf:application/pdf},
}

@article{kerbl_3d_2023,
	title = {3D Gaussian splatting for real-time radiance field rendering.},
	volume = {42},
	url = {https://sgvr.kaist.ac.kr/~sungeui/ICG_F23/Students/[CS482]%203D%20Gaussian%20Splatting%20for%20Real-Time%20Radiance%20Field%20Rendering.pdf},
	pages = {139--1},
	number = {4},
	journaltitle = {{ACM} Trans. Graph.},
	author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
	urldate = {2025-11-26},
	date = {2023},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\QDLFIIKF\\Kerbl et al. - 2023 - 3D Gaussian splatting for real-time radiance field rendering..pdf:application/pdf},
}

@misc{poole_dreamfusion_2022,
	title = {{DreamFusion}: Text-to-3D using 2D Diffusion},
	url = {http://arxiv.org/abs/2209.14988},
	doi = {10.48550/arXiv.2209.14988},
	shorttitle = {{DreamFusion}},
	abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a {DeepDream}-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or {NeRF}) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
	number = {{arXiv}:2209.14988},
	publisher = {{arXiv}},
	author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
	urldate = {2025-11-26},
	date = {2022-09-29},
	eprinttype = {arxiv},
	eprint = {2209.14988 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\II6VQLEP\\Poole et al. - 2022 - DreamFusion Text-to-3D using 2D Diffusion.pdf:application/pdf},
}

@article{wang_prolificdreamer_2023,
	title = {Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1a87980b9853e84dfb295855b425c262-Abstract-Conference.html},
	shorttitle = {Prolificdreamer},
	pages = {8406--8441},
	journaltitle = {Advances in neural information processing systems},
	author = {Wang, Zhengyi and Lu, Cheng and Wang, Yikai and Bao, Fan and Li, Chongxuan and Su, Hang and Zhu, Jun},
	urldate = {2025-11-26},
	date = {2023},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\RWG3YPV9\\Wang et al. - 2023 - Prolificdreamer High-fidelity and diverse text-to-3d generation with variational score distillation.pdf:application/pdf},
}

@inproceedings{sanchez-gonzalez_learning_2020,
	title = {Learning to simulate complex physics with graph networks},
	url = {https://proceedings.mlr.press/v119/sanchez-gonzalez20a},
	pages = {8459--8468},
	booktitle = {International conference on machine learning},
	publisher = {{PMLR}},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
	urldate = {2025-11-26},
	date = {2020},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\WRGMNQXI\\Sanchez-Gonzalez et al. - 2020 - Learning to simulate complex physics with graph networks.pdf:application/pdf},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	shorttitle = {Physics-informed neural networks},
	pages = {686--707},
	journaltitle = {Journal of Computational physics},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E.},
	urldate = {2025-11-26},
	date = {2019},
	note = {Publisher: Elsevier},
}

@article{greydanus_hamiltonian_2019,
	title = {Hamiltonian neural networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/26cd8ecadce0d4efd6cc8a8725cbd1f8-Abstract.html},
	journaltitle = {Advances in neural information processing systems},
	author = {Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
	urldate = {2025-11-26},
	date = {2019},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\5PMDZC86\\Greydanus et al. - 2019 - Hamiltonian neural networks.pdf:application/pdf},
}

@misc{li_fourier_2021,
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations ({PDEs}), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of {PDEs}, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first {ML}-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional {PDE} solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	number = {{arXiv}:2010.08895},
	publisher = {{arXiv}},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	urldate = {2025-11-26},
	date = {2021-05-17},
	eprinttype = {arxiv},
	eprint = {2010.08895 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\D8TP6ZDX\\Li et al. - 2021 - Fourier Neural Operator for Parametric Partial Differential Equations.pdf:application/pdf},
}

@article{silver_mastering_2017,
	title = {Mastering the game of go without human knowledge},
	volume = {550},
	url = {https://idp.nature.com/authorize/casa?redirect_uri=https://www.nature.com/articles/nature24270&casa_token=uxVzaLwHPRIAAAAA:ABf8yG3mit_-tnl5ZCgrjrpH2A_BCl8nsu5zAdIGdvnCQ2HUA0cQqPyuGJEWgDg4MSMrH4DvTYUcfmSITqw},
	pages = {354--359},
	number = {7676},
	journaltitle = {nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian},
	urldate = {2025-11-26},
	date = {2017},
	note = {Publisher: Nature Publishing Group {UK} London},
}

@article{demay_alphadogfight_2022,
	title = {Alphadogfight trials: Bringing autonomy to air combat},
	volume = {36},
	url = {https://secwww.jhuapl.edu/techdigest/Content/techdigest/pdf/V36-N02/36-02-DeMay.pdf},
	shorttitle = {Alphadogfight trials},
	pages = {154--163},
	number = {2},
	journaltitle = {Johns Hopkins {APL} Technical Digest},
	author = {{DeMay}, Christopher R. and White, Edward L. and Dunham, William D. and Pino, Johnathan A.},
	urldate = {2025-11-26},
	date = {2022},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\7ZHRHXEX\\DeMay et al. - 2022 - Alphadogfight trials Bringing autonomy to air combat.pdf:application/pdf},
}

@inproceedings{tobin_domain_2017,
	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
	url = {https://ieeexplore.ieee.org/abstract/document/8202133/},
	pages = {23--30},
	booktitle = {2017 {IEEE}/{RSJ} international conference on intelligent robots and systems ({IROS})},
	publisher = {{IEEE}},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\YRLBM6PG\\Tobin et al. - 2017 - Domain randomization for transferring deep neural networks from simulation to the real world.pdf:application/pdf},
}

@misc{wang_paired_2019,
	title = {Paired Open-Ended Trailblazer ({POET}): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions},
	url = {http://arxiv.org/abs/1901.01753},
	doi = {10.48550/arXiv.1901.01753},
	shorttitle = {Paired Open-Ended Trailblazer ({POET})},
	abstract = {While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer ({POET}) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like {POET} to continue to create novel and increasingly complex capabilities without bound. Our results show that {POET} produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that {POET} will inspire a new push towards open-ended discovery across many domains, where algorithms like {POET} can blaze a trail through their interesting possible manifestations and solutions.},
	number = {{arXiv}:1901.01753},
	publisher = {{arXiv}},
	author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
	urldate = {2025-11-26},
	date = {2019-02-21},
	eprinttype = {arxiv},
	eprint = {1901.01753 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\BSJIMDLW\\Wang et al. - 2019 - Paired Open-Ended Trailblazer (POET) Endlessly Generating Increasingly Complex and Diverse Learning.pdf:application/pdf},
}

@article{kingma_auto-encoding_2013,
	title = {Auto-encoding variational bayes},
	url = {https://indico.math.cnrs.fr/event/11377/attachments/4589/6915/18012024_Kingma-and-Welling-2022%20Auto-Encoding%20Variational%20Bayes.pdf},
	journaltitle = {{arXiv} preprint {arXiv}:1312.6114},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2025-11-26},
	date = {2013},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\3WQ3P6DC\\Kingma et Welling - 2013 - Auto-encoding variational bayes.pdf:application/pdf},
}

@article{sohn_learning_2015,
	title = {Learning structured output representation using deep conditional generative models},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
	journaltitle = {Advances in neural information processing systems},
	author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
	urldate = {2025-11-26},
	date = {2015},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\SWZENBXH\\Sohn et al. - 2015 - Learning structured output representation using deep conditional generative models.pdf:application/pdf},
}

@article{razavi_generating_2019,
	title = {Generating diverse high-fidelity images with vq-vae-2},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html},
	journaltitle = {Advances in neural information processing systems},
	author = {Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
	urldate = {2025-11-26},
	date = {2019},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\TIIKWQR6\\Razavi et al. - 2019 - Generating diverse high-fidelity images with vq-vae-2.pdf:application/pdf},
}

@article{ha_world_2018,
	title = {World models},
	volume = {2},
	url = {https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2024_2025/presentation/S6/WM_Edmund.pdf},
	number = {3},
	journaltitle = {{arXiv} preprint {arXiv}:1803.10122},
	author = {Ha, David and Schmidhuber, Jürgen},
	urldate = {2025-11-26},
	date = {2018},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\6DHH6PMG\\Ha et Schmidhuber - 2018 - World models.pdf:application/pdf},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the
              generative modeling
              problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks ({GANs}) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but {GANs} are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). {GANs} have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	pages = {139--144},
	number = {11},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2025-11-26},
	date = {2020-10-22},
	langid = {english},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\RDERHZGK\\Goodfellow et al. - 2020 - Generative adversarial networks.pdf:application/pdf},
}

@misc{mohamed_learning_2017,
	title = {Learning in Implicit Generative Models},
	url = {http://arxiv.org/abs/1610.03483},
	doi = {10.48550/arXiv.1610.03483},
	abstract = {Generative adversarial networks ({GANs}) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of {GANs} with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame {GANs} within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by {GANs}, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the {GAN} literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
	number = {{arXiv}:1610.03483},
	publisher = {{arXiv}},
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	urldate = {2025-11-26},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1610.03483 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\K9LNEWD9\\Mohamed et Lakshminarayanan - 2017 - Learning in Implicit Generative Models.pdf:application/pdf},
}

@inproceedings{isola_image--image_2017,
	title = {Image-to-image translation with conditional adversarial networks},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html},
	pages = {1125--1134},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\SLG7Q9I6\\Isola et al. - 2017 - Image-to-image translation with conditional adversarial networks.pdf:application/pdf},
}

@article{xie_tempogan_2018,
	title = {{tempoGAN}: a temporally coherent, volumetric {GAN} for super-resolution fluid flow},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3197517.3201304},
	doi = {10.1145/3197517.3201304},
	shorttitle = {{tempoGAN}},
	abstract = {We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate adverted quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.},
	pages = {1--15},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Xie, You and Franz, Aleksandra and Chu, Mengyu and Thuerey, Nils},
	urldate = {2025-11-26},
	date = {2018-08-31},
	langid = {english},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\Y54KVDNT\\Xie et al. - 2018 - tempoGAN a temporally coherent, volumetric GAN for super-resolution fluid flow.pdf:application/pdf},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep unsupervised learning using nonequilibrium thermodynamics},
	url = {http://proceedings.mlr.press/v37/sohl-dickstein15.html},
	pages = {2256--2265},
	booktitle = {International conference on machine learning},
	publisher = {pmlr},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	urldate = {2025-11-26},
	date = {2015},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\ZKAFGW3P\\Sohl-Dickstein et al. - 2015 - Deep unsupervised learning using nonequilibrium thermodynamics.pdf:application/pdf},
}

@article{ho_denoising_2020,
	title = {Denoising diffusion probabilistic models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	pages = {6840--6851},
	journaltitle = {Advances in neural information processing systems},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2025-11-26},
	date = {2020},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\HZM27DQ4\\Ho et al. - 2020 - Denoising diffusion probabilistic models.pdf:application/pdf},
}

@misc{song_denoising_2022,
	title = {Denoising Diffusion Implicit Models},
	url = {http://arxiv.org/abs/2010.02502},
	doi = {10.48550/arXiv.2010.02502},
	abstract = {Denoising diffusion probabilistic models ({DDPMs}) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models ({DDIMs}), a more efficient class of iterative implicit probabilistic models with the same training procedure as {DDPMs}. In {DDPMs}, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that {DDIMs} can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to {DDPMs}, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
	number = {{arXiv}:2010.02502},
	publisher = {{arXiv}},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	urldate = {2025-11-26},
	date = {2022-10-05},
	eprinttype = {arxiv},
	eprint = {2010.02502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{graves_generating_2014,
	title = {Generating Sequences With Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1308.0850},
	doi = {10.48550/arXiv.1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	number = {{arXiv}:1308.0850},
	publisher = {{arXiv}},
	author = {Graves, Alex},
	urldate = {2025-11-26},
	date = {2014-06-05},
	eprinttype = {arxiv},
	eprint = {1308.0850 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\H7DFKZ5M\\Graves - 2014 - Generating Sequences With Recurrent Neural Networks.pdf:application/pdf},
}

@misc{reed_generalist_2022,
	title = {A Generalist Agent},
	url = {http://arxiv.org/abs/2205.06175},
	doi = {10.48550/arXiv.2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	number = {{arXiv}:2205.06175},
	publisher = {{arXiv}},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and Freitas, Nando de},
	urldate = {2025-11-26},
	date = {2022-11-11},
	eprinttype = {arxiv},
	eprint = {2205.06175 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\IQGSSTVA\\Reed et al. - 2022 - A Generalist Agent.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	journaltitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2025-11-26},
	date = {2017},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\HP9VNUD2\\Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf},
}

@article{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	url = {https://www.mikecaptain.com/resources/pdf/GPT-1.pdf},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	urldate = {2025-11-26},
	date = {2018},
	note = {Publisher: San Francisco, {CA}, {USA}},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\JRA5RHCN\\Radford et al. - 2018 - Improving language understanding by generative pre-training.pdf:application/pdf},
}

@article{dosovitskiy_image_2020,
	title = {An image is worth 16x16 words: Transformers for image recognition at scale},
	url = {https://files.ryancopley.com/Papers/2010.11929v2.pdf},
	shorttitle = {An image is worth 16x16 words},
	journaltitle = {{arXiv} preprint {arXiv}:2010.11929},
	author = {Dosovitskiy, Alexey},
	urldate = {2025-11-26},
	date = {2020},
	file = {Available Version (via Google Scholar):C\:\\Users\\Matth\\Documents\\Projets\\Biblio\\Zotero\\storage\\7ANP3QQK\\Dosovitskiy - 2020 - An image is worth 16x16 words Transformers for image recognition at scale.pdf:application/pdf},
}
