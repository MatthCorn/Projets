\chapter{Problématique (plan)}

Le chapitre sur la problématique contiendra les éléments suivants:
\begin{itemize}
\item Description du fonctionnement du capteur ESM dans l'environnement (dans la limite de ce qu'on peut dire), intégré dans la chaine algorithmique de traitement de l'information.
\item Description du fonctionnement de l'environnement virtuel, avec l'explication des modélisations de chaque traitement.
\item Spécification du goulot d'étranglement et commentaire sur les données I/O.
\item Commentaire sur le complexité du problème pour l'apprentissage automatique : double problématique génération et traitement de séquence.
\item \textcolor{red}{Penser à ajouter des références de traitement avec ce principe de mesureur (brevet ?) pour montrer qu'on ne révèle pas des secrets}
\end{itemize}

\chapter{Problématique}
\section{Introduction}
L'objectif central de ces travaux de recherche est d'accélérer par intelligence artificielle la simulation d'un environnement virtuel modélisant l'interaction entre des impulsions RADAR et un capteur de Mesures de Soutien Électronique (ESM - Electronic Support Measures). Ce chapitre s'attache à définir précisément le périmètre et les enjeux de cette problématique. Dans un premier temps, nous détaillerons le fonctionnement opérationnel du capteur ESM et son intégration au sein de la chaîne de traitement de l'information tactique. Cette analyse contextuelle permettra de justifier la nécessité de disposer d'un environnement virtuel de simulation capable de reproduire fidèlement le comportement du capteur pour des besoins de validation. Nous exposerons ensuite l'architecture de cet environnement virtuel afin d'identifier les verrous technologiques qui limitent actuellement ses performances, en localisant spécifiquement le goulot d'étranglement computationnel. Cette analyse conduira finalement à la formalisation du problème d'accélération sous l'angle de l'apprentissage automatique, en mettant en évidence sa double nature de traitement de séquence et de génération conditionnelle.
\section{Description de l'intégration du capteur de Mesures de Soutien Électronique et son fonctionnement}

\subsection{Contexte opérationnel : La maîtrise du spectre électromagnétique} 
Dans le cadre d'un scénario de guerre électronique (GE), la survie de l'aéronef dépend de sa capacité à percevoir et comprendre son environnement électromagnétique (EM). L'appareil évolue dans un espace abondant d'émissions provenant de RADAR adverses ou civils, au sol ou aéroportés, cherchant eux-mêmes à détecter leur cible. Les caractéristiques techniques de ces signaux, telles que la fréquence, la largeur d'impulsion ou la période de répétition, constituent une signature unique permettant d'identifier l'émetteur et d'en déduire ses intentions tactiques (veille, poursuite, engagement). La mission des capteurs de ESM, illustrée dans la partie droite de la figure \ref{fig:apport_tactic}, est alors d'assurer une écoute passive et discrète du spectre pour détecter, de caractériser et de localiser ces menaces potentielles. Ils fournissent ainsi les données critiques à la décision stratégique et aux contre-mesures. 

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{apport_tactic.png}
\end{center}
\caption{Visualisation de l'apport tactique. Le système transforme un environnement EM dense (gauche) en une situation tactique intelligible (droite), permettant la localisation et la classification des forces en présence.}
\label{fig:apport_tactic}
\end{figure}

\subsection{Chaîne de traitement de l'information} 
L'intégration du capteur s'inscrit dans une architecture de traitement séquentielle visant à transformer un signal physique brut en renseignement tactique exploitable. Le champ EM incident est initialement capté par le capteur ESM, qui opère la première conversion fondamentale : il détecte les impulsions radar et construit en temps réel des descripteurs numériques, les PDW (Pulse Description Word - Mot de Description d'Impulsion). Chaque PDW synthétise les paramètres mesurés de l'impulsion : Date d'arrivée (ToA), Largeur d'impulsion (LI), Fréquence, Niveau de puissance et Direction d'arrivée (DoA).
Ce flux continu de PDW alimente ensuite les algorithmes de traitement de haut niveau. Une étape de désentrelacement regroupe d'abord les impulsions par émetteur sur un horizon temporel court, isolant les trains d'impulsions cohérents. Ces regroupements élémentaires sont ensuite consolidés par un processus de pistage (tracking) qui suit l'évolution des émetteurs sur le long terme pour en caractériser la cinématique et le mode de fonctionnement. Finalement, ces pistes enrichies sont confrontées à des bases de données de signatures pour identifier formellement le système d'arme associé et évaluer le niveau de menace immédiat.

La figure \ref{fig:env} présente la chaîne de traitement d’un capteur réel (haut) ainsi que deux approches de simulation correspondant à un environnement virtuel (milieu) et un environnement virtuel avec IA (bas).

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{env.png}
\end{center}
\caption{Illustration des chaînes de transformation des informations pour un capteur réel (haut), un environnement virtuel (milieu) et un environnement virtuel avec IA (bas). \textbf{Haut :} La vérité terrain dicte l'émission d'impulsions, le signal physique est intercepté et les PDW sont déterminées par le capteur (en pointillés noirs). Ces informations sont agrégées (désentrelacement, pistage) et analysées (identification) pour produire un renseignement exploitable sur la vérité terrain initiale. \textbf{Milieu :} Un environnement virtuel (périmètre en pointillés bleus) modélise la physique, à partir de l'émission des impulsions jusqu'à la caractérisation des PDW. La simulation des blocs est comportementale. \textbf{Droite :} Le goulot d'étranglement (en pointillés rouges) est remplacé par un module d'IA. L'enchainement des blocs de traitement constitue encore un environnement virtuel mais avec IA.}
\label{fig:env}
\end{figure}

\subsection{Architecture et fonctionnement du capteur} 
Le fonctionnement interne du capteur ESM ne se limite pas à une conversion analogique-numérique transparente ; il constitue une chaîne de traitements physiques et logiques qui conditionne structurellement la qualité des données produites. Les traitements décrits ci-après, et illustré de manière simplifié au milieu de la figure \ref{fig:env}, constituent le socle architectural de la plupart des récepteurs numériques modernes, bien que des variantes d'implémentation, dictées par les contraintes matérielles des systèmes temps réel, puissent exister selon les constructeurs.

Le traitement débute par la conversion du champ EM incident en signal électrique analogique. L'objectif est ensuite d'extraire, sur des fenêtres temporelles successives, les raies spectrales significatives. La mission de surveillance de bandes passantes instantanées de plusieurs gigahertz (typiquement $2 - 18 GHz$) impose l'usage de bancs de convertisseurs analogique-numérique (au moins $3$) fonctionnant en parallèle à des cadences inférieures à la fréquence de Nyquist (ex: $1 GHz$, $1.2 GHz$, $1.4 GHz $.) \cite{tsui_digital_2004}. Ce choix architectural induit un repliement spectral systématique sur chaque voie d'acquisition, mais permet la détermination de la fréquence réelle grâce à la résolution d'un système de congruences entre les différentes voies repliées, principe connu sous le nom de théorème des restes chinois \cite{vaidyanathan_sparse_2010}, \cite{li_robust_2009}. Cependant, sur chaque canal d'acquisition, des phénomènes de masquage peuvent intervenir, liés à la saturation des convertisseurs, aux harmoniques et\slash ou à l'intermodulation provoquée par des impulsions de haute énergie, ou à la proximité fréquentielle entre signaux repliés. De plus, la résolution d'ambiguïté s'appuie généralement sur une sélection restreinte des $N$ pics spectraux les plus énergétiques par canal, liée aux contraintes matérielles. La conjonction du bruit thermique sur des canaux critiques et de cette sélection limitative peut conduire à l'échec de la levée d'ambiguïté, entraînant la perte d'impulsions sur la fenêtre temporelle considérée. Les fréquences non-ambiguës identifiées, associées leur énergie, sont qualifiées de détections non-ambiguës (DNA).

Une fois les DNA identifiées sur la fenêtre d'analyse courante, elles sont transmises collectivement au système de suivi temporel. Ce module reçoit ainsi un paquet de détections simultanées qu'il doit associer aux ressources mémoires actives, qualifiées de "pistes" \cite{mardia_new_1989}. Le processus d'association est séquentiel et hiérarchisé : les DNA sont traitées une à une, par ordre de priorité croissante selon leur amplitude, afin de privilégier le suivi des signaux les plus énergétiques. Pour chaque détection candidate, le système recherche une correspondance parmi les pistes actives sur la base de critères de tolérance prédéfinis (proximité fréquentielle, cohérence de niveau). Une contrainte stricte d'unicité s'applique alors : une piste ne peut être mise à jour qu'une seule fois par fenêtre temporelle. En cas de corrélation valide avec une piste disponible (non encore mise à jour sur ce cycle), celle-ci intègre les nouveaux paramètres et prolonge sa durée de vie. Si aucune association n'est possible, ou si la piste candidate a déjà été servie par une détection prioritaire, une nouvelle piste est ouverte pour le signal, sous réserve qu'une ressource mémoire soit libre. La clôture d'une piste et la génération du PDW final \cite{adamy_ew_2001} s'opèrent finalement lorsqu'elle n'a pas reçu de mise à jour durant une période seuil ou lorsque l'impulsion dépasse sa limite de durée opérationnelle.

La limitation matérielle du nombre de ces mémoires, conjuguée à leur logique d'allocation, engendre des artefacts de segmentation spécifiques. Premièrement, une contrainte de latence maximale impose de segmenter artificiellement les impulsions très longues ou continues pour assurer des mises à jour périodiques, générant une série de PDW contigus. Deuxièmement, les échecs de résolution d'ambiguïté peuvent provoquer une rupture de suivi prématurée. Si le masquage est transitoire, la piste reprend après une interruption, scindant l'impulsion en plusieurs entités. Si le masquage persiste jusqu'à la fin de l'émission, le suivi s'arrête définitivement, tronquant la fin du signal. Troisièmement, la saturation des ressources mémoires en environnement dense impacte directement la détection : si aucune piste n'est disponible à l'apparition du signal, il sera ignoré sur l'instant. Cela conduit soit à une acquisition tardive dès la libération d'une ressource, amputant alors le début du signal, soit à une perte totale de l'impulsion si aucune ressource ne se libère à temps. À l'inverse de ces phénomènes de fragmentation ou de perte, la résolution temporelle finie des bancs de filtres peut conduire à l'amalgame de deux impulsions brèves et rapprochées en un seul descripteur.

Ainsi, la séquence de PDW produite ne doit pas être considérée comme une simple mesure dégradée de la réalité, mais comme une reconstruction interprétée, portant intrinsèquement la signature des limitations fréquentielles et des heuristiques de gestion de ressources du capteur.

\section{Description de l'environnement virtuel}
\subsection{Motivations}
La phase d'Intégration, Vérification, Validation et Qualification (IVVQ) des algorithmes de GE, tels que le désentrelacement, le pistage et l'identification, exige de disposer de jeux de données rigoureusement contrôlés. Pour valider la chaîne de traitement, il est nécessaire de confronter les données d'entrée des algorithmes - le flux de PDW en sortie du capteur ESM - aux données de sortie attendues, c'est-à-dire la situation tactique restituée. Or, l'obtention de ces données par des essais en vol réels se heurte à des contraintes majeures. En effet, la connaissance exacte et exhaustive de la situation tactique (la position et l'activité de tous les émetteurs environnants) est souvent impossible à garantir, empêchant d'établir une "vérité terrain" fiable pour qualifier les algorithmes. De plus, la réalisation d'essais en vol dédiés représente un défi logistique et financier considérable. La constitution d'un scénario réaliste implique le déploiement de moyens conséquents, tels que des avions plastrons ou des stations radars au sol, dont la disponibilité est limitée. Par ailleurs, ces essais physiques ne permettent de couvrir qu'un spectre restreint de configurations géométriques et EM, limitant la diversité des données récoltées. Face à ces obstacles, le recours à un simulateur d'impulsions s'impose comme la solution de référence. En permettant de générer le flux exact d'impulsions que les capteurs ESM auraient intercepté pour un scénario prédéfini, l'environnement virtuel offre une maîtrise totale des paramètres d'entrée. Cette approche autorise la simulation d'une quantité massive de données et la construction de scénarios d'une complexité arbitraire, incluant des cas limites difficilement reproductibles en vol. Cette capacité de tests intensifs est indispensable pour analyser finement la réaction de la chaîne algorithmique et procéder aux itérations nécessaires à son amélioration.

\subsection{Architecture et fonctionnement de l'environnement virtuel}

L'environnement virtuel est structuré en quatre modules fonctionnels séquentiels. Il repose sur une approche de modélisation comportementale, dont l'objectif n'est pas de reproduire le traitement du signal échantillon par échantillon, mais de simuler l'effet macroscopique des traitements physiques et logiques sur le flux d'impulsions incident, afin de prédire les PDW que le capteur aurait effectivement générés.

Le premier module assure la génération de la vérité terrain EM. Il ingère les fichiers de description cinématique (trajectoires du porteur et des radars environnants) ainsi que les séquences d'émission théoriques de chaque radar. À partir de ces données, il construit une liste chronologique d'impulsions émises, triées par date d'arrivée (ToA), où chaque impulsion est décrite par un PDW idéal contenant ses caractéristiques physiques natives. Le second module modélise la chaîne de propagation et de réception analogique. Sa fonction est double : premièrement, il applique l'équation du bilan de liaison pour déterminer l'amplitude du signal arrivant au voisinage du porteur, en tenant compte de la position relative émetteur-récepteur et de la direction d'émission. Deuxièmement, il simule la fonction de transfert des antennes réceptrices. En exploitant les diagrammes de gain et l'angle d'incidence du signal, ce module calcule l'atténuation ou l'amplification subie par le signal électrique en sortie d'antenne. La sortie de ce bloc est une liste de PDW dont l'amplitude a été ajustée pour refléter la puissance réellement disponible à l'entrée du récepteur numérique.\\

Les deux derniers modules simulent le cœur du traitement numérique : la détection spectrale et la caractérisation temporelle. Pour s'abstraire de la simulation coûteuse du temps continu, ces modules opèrent sur une échelle temporelle discrétisée nommée "palier". Un palier définit un intervalle de temps au cours duquel l'environnement EM est considéré comme stationnaire : il est délimité par l'apparition ou la disparition d'une impulsion quelconque. Cette hypothèse de stationnarité permet de considérer que les fréquences détectables par le capteur restent constantes sur toute la durée du palier, autorisant un calcul unique par intervalle.\\

Le troisième module se charge de la modélisation spectrale à l'échelle de ce palier. Il reproduit les effets de l'architecture sous-Nyquist en calculant, pour chaque impulsion présente, ses fréquences repliées sur les différents canaux d'acquisition. En comparant les niveaux relatifs des signaux concurrents et en appliquant les seuils de sensibilité matériels, le module détermine la visibilité de chaque impulsion. Une impulsion est considérée comme "détectée" — c'est-à-dire que son ambiguïté fréquentielle aurait été levée avec succès — si elle reste visible et non masquée sur au moins trois canaux simultanément, elle est alors qualifiée de DNA. \\

Enfin, le quatrième module reproduit le mécanisme de suivi temporel. Son fonctionnement mime la logique interne du capteur décrite précédemment, mais la cadence de mise à jour est ici dictée par les paliers et non par les fenêtres d'échantillonnage. À chaque palier, les DNA sont comparées aux pistes actives. En cas de corrélation, la piste est maintenue ; sinon, une nouvelle piste est allouée sous réserve de disponibilité mémoire. Ce module gère également les clôtures de pistes : si une piste n'est pas mise à jour pendant une durée critique, elle est fermée. De même, pour simuler la segmentation des signaux continus ou longs, une logique de coupure forcée est implémentée : lorsqu'une piste dépasse une durée maximale d'ouverture, elle est close (génération d'un PDW), puis immédiatement rouverte pour poursuivre le suivi, reproduisant ainsi fidèlement les artefacts de segmentation du capteur réel.\\

En conclusion, l'environnement virtuel articule une chaîne de transformation cohérente qui mime le cycle de vie complet de l'information EM. En élaborant successivement une séquence de PDW idéaux issue de la vérité terrain, en leur appliquant les modulations radiométriques propres à la chaîne d'acquisition, puis en soumettant ce flux aux logiques de sélection spectrale et de suivi temporel du récepteur, le simulateur parvient à reproduire la séquence de PDW qu'un capteur aurait effectivement interceptée pour un scénario donné. Bien que cette modélisation comportementale constitue par essence une approximation par rapport à une simulation physique du signal au niveau de l'échantillon, elle offre un compromis entre la précision des phénomènes reproduits et la charge de calcul. La représentativité des artefacts générés — incluant les effets de masquage, de fragmentation et de saturation — s'avère suffisante pour garantir la pertinence des données synthétiques, permettant ainsi de répondre aux exigences de diversité et de volume nécessaires à la validation robuste des algorithmes de traitement de l'information sans dépendre exclusivement des essais en vol.

\section{Identification du goulot d'étranglement et limites opérationnelles}
\subsection{L'impératif de temps réel et le coût de la simulation}
Si l'architecture modulaire de l'environnement virtuel garantit une fidélité satisfaisante des données produites, son exploitation opérationnelle se heurte à une contrainte majeure de performance temporelle. Actuellement, la simulation détaillée des traitements du capteur présente un coefficient d'expansion temporel moyen de l'ordre de 100 : la simulation d'une seule seconde de scénario nécessite environ cent secondes de calcul. Cette latence prohibe l'utilisation du simulateur pour des applications nécessitant une interaction en temps réel, telles que la formation des pilotes et des opérateurs de GE. Par ailleurs, même dans le cadre de la validation algorithmique hors ligne, ce coût calculatoire devient un obstacle à la génération massive de données. La validation statistique robuste des algorithmes de désentrelacement ou d'identification exige de couvrir des milliers de variations de scénarios, une tâche qui, avec le coefficient d'expansion actuel, nécessiterait des temps de calcul incompatibles avec les cycles de développement industriels, particulièrement pour les scénarios denses.

\subsection{Analyse de la complexité et localisation du verrou}
L'analyse de profilage de l'environnement virtuel permet de localiser précisément l'origine de cette latence au niveau des troisième et quatrième modules fonctionnels, responsables de la détection spectrale, du pistage et de la caractérisation des impulsions. Une distinction structurelle fondamentale sépare ces modules des étages amont. Les modules 1 et 2 (génération et propagation) appliquent des transformations physiques indépendantes sur chaque impulsion ; ils se prêtent donc naturellement à une parallélisation sur CPU voire GPU. À l'inverse, les modules 3 et 4 opèrent intrinsèquement de manière séquentielle : l'état du système à l'instant $t$ (les pistes actives, les masquages en cours) dépend de l'histoire du traitement, empêchant toute parallélisation temporelle simple.
De surcroît, le nombre de paliers temporels à traiter croît linéairement avec la densité d'impulsions du scénario. Or, au sein de chaque palier, le module de détection doit effectuer des comparaisons croisées entre toutes les impulsions présentes pour résoudre les masquages et les ambiguïtés, induisant une complexité quadratique par rapport au nombre d'impulsions locales. Cette combinaison d'une structure séquentielle rigide et d'une complexité locale quadratique crée un goulot d'étranglement dès que la densité du scénario augmente. Ce verrou, situé au cœur de la logique de traitement du capteur, constitue l'obstacle technologique qu'il est nécessaire de lever. L'objectif de la thèse est donc d'accélérer l'environnement virtuel en substituant ces deux modules, que nous désignerons conjointement comme le système de détection et de caractérisation des impulsions (DCI), par un modèle d'intelligence artificielle optimisé.

\section{Formalisation et complexité du problème d'apprentissage}
\subsection{Nature des données et définition formelle de la tâche}
L'objectif est de substituer au DCI un modèle appris capable d'approximer sa dynamique de fonctionnement. Du point de vue des données, cette tâche se formalise comme un problème de transduction de séquence s'opérant dans un espace continu. L'entrée du modèle, notée $X$, est la séquence de PDW "modulés" issue du module de propagation (module 2). Elle représente l'information physique brute incidente aux bornes des convertisseurs analogique-numérique. La cible à prédire, notée $Y$, est la séquence de PDW "capteur" (sortie du module 4), correspondant aux impulsions effectivement construites et publiées par le système.\\

Ces données présentent deux caractéristiques structurelles qui font leurs particularités. Premièrement, elles appartiennent à un espace continu multidimensionnel. Chaque élément constitutif des séquences $X$ et $Y$ est un vecteur de valeurs réelles (Date, Fréquence, Largeur, Amplitude, Azimut) définissant les propriétés physiques de l'impulsion. Deuxièmement, les séquences se caractérisent par une cardinalité variable et asynchrone. La topologie temporelle diffère entre l'entrée et la sortie en raison des mécanismes internes du capteur (masquages, fusions, scissions). Ainsi, la longueur $N$ de la séquence d'entrée et la longueur $M$ de la séquence de sortie sont variables et ne respectent pas de règle de proportionnalité stricte ou de synchronisation élément par élément.

\subsection{Une double problématique : Traitement de séquence et Génération}
Cette caractérisation ancre le problème à l'intersection de deux paradigmes de l'apprentissage profond, induisant une spécificité qui le distingue des applications classiques. \\

D'une part, il s'agit fondamentalement d'un problème de traitement de séquence (Sequence Processing), imposé par la nature variable et décorrélée des dimensions temporelles en jeu. Contrairement à des tâches de régression à taille fixe, le modèle doit traiter une série d'entrée de longueur arbitraire $N$ pour produire une série de sortie de longueur $M$, sans qu'il existe de règle de proportionnalité simple entre $N$ et $M$. Cette structure contraint à l'utilisation de mécanismes capables de mapper une séquence de taille variable vers une représentation latente fixe ou dynamique. De plus, la tâche se trouve complexifiée par la nature continue des données. Contrairement aux tâches de Traitement du Langage Naturel (NLP) où les séquences sont formées de mots issus d'un dictionnaire fini, les PDW exigent une précision numérique fine sur plusieurs variables continues simultanément. L'espace de travail est donc continue au lieu d'être fini.\\

D'autre part, il s'agit d'un problème de génération conditionnelle. Le modèle ne doit pas simplement filtrer ou altérer les impulsions incidentes, mais construire intégralement une séquence de sortie dont la structure événementielle diffère fondamentalement de l'entrée. La logique de suivi temporel du capteur brise la relation bijective : un événement physique unique peut se trouver traduit par une succession de plusieurs PDW distincts, ou inversement, plusieurs événements peuvent être fusionnés. Le modèle doit donc apprendre à générer une nouvelle série de descripteurs qui constituent une interprétation synthétique de la réalité physique. Dans cette reconstruction, la séquence de sortie devient une entité structurellement autonome plutôt qu'une version simplement dégradée de l'entrée.\\ 

Cette dualité, combinée à l'exigence de précision, écarte les solutions sur étagère et nécessite la conception d'une architecture capable de marier les mécanismes propres à la compréhension de contexte et les capacités de régression fine nécessaires à la reconstruction physique du signal.

