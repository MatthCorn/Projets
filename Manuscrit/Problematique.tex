\chapter{Problématique (plan)}

Le chapitre sur la problématique contiendra les éléments suivants:
\begin{itemize}
\item Description du fonctionnement du capteur ESM dans l'environnement (dans la limite de ce qu'on peut dire), intégré dans la chaine algorithmique de traitement de l'information.
\item Description du fonctionnement de l'environnement numérique, avec l'explication des modélisations de chaque traitement.
\item Spécification du goulot d'étranglement et commentaire sur les données I/O.
\item Commentaire sur le complexité du problème pour l'apprentissage automatique : double problématique génération et traitement de séquence.
\item \textcolor{red}{Penser à ajouter des références de traitement avec ce principe de mesureur (brevet ?) pour montrer qu'on ne révèle pas des secrets}
\end{itemize}

\chapter{Problématique}
\section{Introduction}
\textcolor{red}{
Comme abordé dans le chapitre introductif, notre mission est d'accélérer la simulation d'un environnement numérique modélisant l'interception d'impulsion RADAR par des capteurs ESM. Ce chapitre va nous permettre de revenir sur cette problématique. Nous commencerons par l'explication du fonctionnement du capteur ESM et son intégration dans le chaine de traitement de l'information. Nous verrons ensuite pourquoi il est nécessaire de disposer d'un environnement numérique modélisant le fonctionnement du capteur et nous reviendrons sur cette modélisation. APrès nous identifierons le goulot d'étranglement et précierons alors la nature exacte du problème d'accélération que nous aurons à résoudre. Nous porterons une attention particulière à la lecture de se problème sous le spectre de l'apprentissage automatique en notant que l'aspect est à la fois traitement de séquence et génération. Finalement, nous exposerons les références des problèmes où ce type de traitement apparaît, des brevets existants, etc
}
\section{Description de l'intégration du capteur ESM et son fonctionnement}

\subsection{Contexte opérationnel : La maîtrise du spectre électromagnétique} 
Dans le cadre d'un scénario de guerre électronique, la survie de l'aéronef dépend de sa capacité à percevoir et comprendre son environnement électromagnétique. L'appareil évolue dans un espace abondant d'émissions provenant de RADAR adverses ou civils, au sol ou aéroportés, cherchant eux-mêmes à détecter leur cible. Les caractéristiques techniques de ces signaux, telles que la fréquence, la largeur d'impulsion ou la période de répétition, constituent une signature unique permettant d'identifier l'émetteur et d'en déduire ses intentions tactiques (veille, poursuite, engagement). C'est la mission des capteurs de Mesures de Soutien Électronique (ESM - Electronic Support Measures) : assurer une écoute passive et discrète du spectre pour détecter, caractériser et localiser ces menaces potentielles, fournissant ainsi les données critiques à la décision stratégique et aux contre-mesures.

\subsection{Chaîne de traitement de l'information} 
L'intégration du capteur s'inscrit dans une architecture de traitement séquentielle visant à transformer un signal physique brut en renseignement tactique exploitable. Le champ électromagnétique incident est initialement capté par le capteur ESM, qui opère la première conversion fondamentale : il détecte les impulsions radar et construit en temps réel des descripteurs numériques, les PDW (Pulse Description Word - Mot de Description d'Impulsion). Chaque PDW synthétise les paramètres mesurés de l'impulsion : Date d'arrivée (ToA), Largeur d'impulsion (LI), Fréquence, Niveau de puissance et Direction d'arrivée (DoA).

Ce flux continu de PDW alimente ensuite les algorithmes de traitement de haut niveau. Une étape de désentrelacement regroupe d'abord les impulsions par émetteur sur un horizon temporel court, isolant les trains d'impulsions cohérents. Ces regroupements élémentaires sont ensuite consolidés par un processus de pistage (tracking) qui suit l'évolution des émetteurs sur le long terme pour en caractériser la cinématique et le mode de fonctionnement. Finalement, ces pistes enrichies sont confrontées à des bases de données de signatures pour identifier formellement le système d'arme associé et évaluer le niveau de menace immédiat.


\subsection{Architecture et fonctionnement du capteur} 
Le fonctionnement interne du capteur ESM ne se limite pas à une conversion analogique-numérique transparente ; il constitue une chaîne complexe de traitements physiques et logiques qui conditionne structurellement la qualité des données produites. Les traitements décrits ci-après constituent le socle architectural de la plupart des récepteurs numériques modernes, bien que des variantes d'implémentation, dictées par les contraintes matérielles des systèmes temps réel, puissent exister selon les constructeurs.

Le traitement débute par la conversion du champ électromagnétique incident en signal électrique analogique. L'objectif est ensuite d'extraire, sur des fenêtres temporelles successives, les raies spectrales significatives. La mission de surveillance de bandes passantes instantanées de plusieurs gigahertz (typiquement $2 - 18 GHz$) impose l'usage de bancs de convertisseurs analogique-numérique (au moins $3$) fonctionnant en parallèle à des cadences inférieures à la fréquence de Nyquist (ex: $1 GHz$, $1.2 GHz$, $1.4 GHz $.) \cite{tsui_digital_2004}. Ce choix architectural induit un repliement spectral systématique sur chaque voie d'acquisition, mais permet la détermination de la fréquence réelle grâce à la résolution d'un système de congruences entre les différentes voies repliées, principe connu sous le nom de théorème des restes chinois \cite{vaidyanathan_sparse_2010}, \cite{li_robust_2009}. Cependant, sur chaque canal d'acquisition, des phénomènes de masquage peuvent intervenir, liés à la saturation des convertisseurs, aux harmoniques et\slash ou à l'intermodulation provoquée par des impulsions de haute énergie, ou à la proximité fréquentielle entre signaux repliés. De plus, la résolution d'ambiguïté s'appuie généralement sur une sélection restreinte des $N$ pics spectraux les plus énergétiques par canal, liée aux contraintes matérielles. La conjonction du bruit thermique sur des canaux critiques et de cette sélection limitative peut conduire à l'échec de la levée d'ambiguïté, entraînant la perte d'impulsions sur la fenêtre temporelle considérée. Les fréquences non-ambiguës identifiées, associées leur énergie, sont qualifiées de détections non-ambiguës (DNA).

Une fois les DNA identifiées sur la fenêtre d'analyse courante, elles sont transmises collectivement au système de suivi temporel. Ce module reçoit ainsi un paquet de détections simultanées qu'il doit associer aux ressources mémoires actives, qualifiées de "pistes" \cite{mardia_new_1989}. Le processus d'association est séquentiel et hiérarchisé : les DNA sont traitées une à une, par ordre de priorité croissante selon leur amplitude, afin de privilégier le suivi des signaux les plus énergétiques. Pour chaque détection candidate, le système recherche une correspondance parmi les pistes actives sur la base de critères de tolérance prédéfinis (proximité fréquentielle, cohérence de niveau). Une contrainte stricte d'unicité s'applique alors : une piste ne peut être mise à jour qu'une seule fois par fenêtre temporelle. En cas de corrélation valide avec une piste disponible (non encore mise à jour sur ce cycle), celle-ci intègre les nouveaux paramètres et prolonge sa durée de vie. Si aucune association n'est possible, ou si la piste candidate a déjà été servie par une détection prioritaire, une nouvelle piste est ouverte pour le signal, sous réserve qu'une ressource mémoire soit libre. La clôture d'une piste et la génération du PDW final \cite{adamy_ew_2001} s'opèrent finalement lorsqu'elle n'a pas reçu de mise à jour durant une période seuil ou lorsque l'impulsion dépasse sa limite de durée opérationnelle.

La limitation matérielle du nombre de ces mémoires, conjuguée à leur logique d'allocation, engendre des artefacts de segmentation spécifiques. Premièrement, une contrainte de latence maximale impose de segmenter artificiellement les impulsions très longues ou continues pour assurer des mises à jour périodiques, générant une série de PDW contigus. Deuxièmement, les échecs de résolution d'ambiguïté peuvent provoquer une rupture de suivi prématurée. Si le masquage est transitoire, la piste reprend après une interruption, scindant l'impulsion en plusieurs entités. Si le masquage persiste jusqu'à la fin de l'émission, le suivi s'arrête définitivement, tronquant la fin du signal. Troisièmement, la saturation des ressources mémoires en environnement dense impacte directement la détection : si aucune piste n'est disponible à l'apparition du signal, il sera ignoré sur l'instant. Cela conduit soit à une acquisition tardive dès la libération d'une ressource, amputant alors le début du signal, soit à une perte totale de l'impulsion si aucune ressource ne se libère à temps. À l'inverse de ces phénomènes de fragmentation ou de perte, la résolution temporelle finie des bancs de filtres peut conduire à l'amalgame de deux impulsions brèves et rapprochées en un seul descripteur.

Ainsi, la séquence de PDW produite ne doit pas être considérée comme une simple mesure dégradée de la réalité, mais comme une reconstruction interprétée, portant intrinsèquement la signature des limitations fréquentielles et des heuristiques de gestion de ressources du capteur.

\section{Description de l'environnement numérique}
\subsection{Motivations}
La phase d'Intégration, Vérification, Validation et Qualification (IVVQ) des algorithmes de guerre électronique, tels que le désentrelacement, le pistage et l'identification, exige de disposer de jeux de données rigoureusement contrôlés. Pour valider la chaîne de traitement, il est nécessaire de confronter les données d'entrée des algorithmes - le flux de PDW en sortie du capteur ESM - aux données de sortie attendues, c'est-à-dire la situation tactique restituée. Or, l'obtention de ces données par des essais en vol réels se heurte à des contraintes majeures. En effet, la connaissance exacte et exhaustive de la situation tactique (la position et l'activité de tous les émetteurs environnants) est souvent impossible à garantir, empêchant d'établir une "vérité terrain" fiable pour qualifier les algorithmes. De plus, la réalisation d'essais en vol dédiés représente un défi logistique et financier considérable. La constitution d'un scénario réaliste implique le déploiement de moyens conséquents, tels que des avions plastrons ou des stations radars au sol, dont la disponibilité est limitée. Par ailleurs, ces essais physiques ne permettent de couvrir qu'un spectre restreint de configurations géométriques et électromagnétiques, limitant la diversité des données récoltées. Face à ces obstacles, le recours à un simulateur d'impulsions s'impose comme la solution de référence. En permettant de générer le flux exact d'impulsions que les capteurs ESM auraient intercepté pour un scénario prédéfini, l'environnement numérique offre une maîtrise totale des paramètres d'entrée. Cette approche autorise la simulation d'une quantité massive de données et la construction de scénarios d'une complexité arbitraire, incluant des cas limites difficilement reproductibles en vol. Cette capacité de tests intensifs est indispensable pour analyser finement la réaction de la chaîne algorithmique et procéder aux itérations nécessaires à son amélioration.

\subsection{Architecture et fonctionnement de l'environnement numérique}

L'environnement numérique est structuré en quatre modules fonctionnels séquentiels. Il repose sur une approche de modélisation comportementale, dont l'objectif n'est pas de reproduire le traitement du signal échantillon par échantillon, mais de simuler l'effet macroscopique des traitements physiques et logiques sur le flux d'impulsions incident, afin de prédire les PDW que le capteur aurait effectivement générés.

Le premier module assure la génération de la vérité terrain électromagnétique. Il ingère les fichiers de description cinématique (trajectoires du porteur et des radars environnants) ainsi que les séquences d'émission théoriques de chaque radar. À partir de ces données, il construit une liste chronologique d'impulsions émises, triées par date d'arrivée (ToA), où chaque impulsion est décrite par un PDW idéal contenant ses caractéristiques physiques natives. Le second module modélise la chaîne de propagation et de réception analogique. Sa fonction est double : premièrement, il applique l'équation du bilan de liaison pour déterminer l'amplitude du signal arrivant au voisinage du porteur, en tenant compte de la position relative émetteur-récepteur et de la direction d'émission. Deuxièmement, il simule la fonction de transfert des antennes réceptrices. En exploitant les diagrammes de gain et l'angle d'incidence du signal, ce module calcule l'atténuation ou l'amplification subie par le signal électrique en sortie d'antenne. La sortie de ce bloc est une liste de PDW dont l'amplitude a été ajustée pour refléter la puissance réellement disponible à l'entrée du récepteur numérique.\\

Les deux derniers modules simulent le cœur du traitement numérique : la détection spectrale et la caractérisation temporelle. Pour s'abstraire de la simulation coûteuse du temps continu, ces modules opèrent sur une échelle temporelle discrétisée nommée "palier". Un palier définit un intervalle de temps au cours duquel l'environnement électromagnétique est considéré comme stationnaire : il est délimité par l'apparition ou la disparition d'une impulsion quelconque. Cette hypothèse de stationnarité permet de considérer que les fréquences détectables par le capteur restent constantes sur toute la durée du palier, autorisant un calcul unique par intervalle.\\

Le troisième module se charge de la modélisation spectrale à l'échelle de ce palier. Il reproduit les effets de l'architecture sous-Nyquist en calculant, pour chaque impulsion présente, ses fréquences repliées sur les différents canaux d'acquisition. En comparant les niveaux relatifs des signaux concurrents et en appliquant les seuils de sensibilité matériels, le module détermine la visibilité de chaque impulsion. Une impulsion est considérée comme "détectée" — c'est-à-dire que son ambiguïté fréquentielle aurait été levée avec succès — si elle reste visible et non masquée sur au moins trois canaux simultanément, elle est alors qualifiée de DNA. \\

Enfin, le quatrième module reproduit le mécanisme de suivi temporel. Son fonctionnement mime la logique interne du capteur décrite précédemment, mais la cadence de mise à jour est ici dictée par les paliers et non par les fenêtres d'échantillonnage. À chaque palier, les DNA sont comparées aux pistes actives. En cas de corrélation, la piste est maintenue ; sinon, une nouvelle piste est allouée sous réserve de disponibilité mémoire. Ce module gère également les clôtures de pistes : si une piste n'est pas mise à jour pendant une durée critique, elle est fermée. De même, pour simuler la segmentation des signaux continus ou longs, une logique de coupure forcée est implémentée : lorsqu'une piste dépasse une durée maximale d'ouverture, elle est close (génération d'un PDW), puis immédiatement rouverte pour poursuivre le suivi, reproduisant ainsi fidèlement les artefacts de segmentation du capteur réel.\\

En conclusion, l'environnement numérique articule une chaîne de transformation cohérente qui mime le cycle de vie complet de l'information électromagnétique. En élaborant successivement une séquence de PDW idéaux issue de la vérité terrain, en leur appliquant les modulations radiométriques propres à la chaîne d'acquisition, puis en soumettant ce flux aux logiques de sélection spectrale et de suivi temporel du récepteur, le simulateur parvient à reproduire la séquence de PDW qu'un capteur aurait effectivement interceptée pour un scénario donné. Bien que cette modélisation comportementale constitue par essence une approximation par rapport à une simulation physique du signal au niveau de l'échantillon, elle offre un compromis optimal entre la précision des phénomènes reproduits et la charge de calcul. La représentativité des artefacts générés — incluant les effets de masquage, de fragmentation et de saturation — s'avère suffisante pour garantir la pertinence des données synthétiques, permettant ainsi de répondre aux exigences de diversité et de volume nécessaires à la validation robuste des algorithmes de traitement de l'information sans dépendre exclusivement des essais en vol.


\section{Toujours trop lent}
\subsection{L'impératif de temps réel et le coût de la simulation} 
Si l'architecture modulaire de l'environnement numérique garantit une fidélité satisfaisante des données produites, son exploitation opérationnelle se heurte à une contrainte majeure de performance temporelle. Actuellement, la simulation détaillée des traitements du capteur présente un coefficient d'expansion temporel moyen de l'ordre de 100 : la simulation d'une seule seconde de scénario nécessite environ cent secondes de calcul. Cette latence prohibe l'utilisation du simulateur pour des applications critiques nécessitant une interaction en temps réel, telles que la formation des pilotes et des opérateurs de guerre électronique. Dans ces configurations, où l'humain doit réagir dynamiquement aux menaces détectées par le capteur virtuel pour manœuvrer l'aéronef ou engager des contre-mesures, la fluidité de la simulation est un prérequis absolu. Par ailleurs, même dans le cadre de la validation algorithmique hors ligne, ce coût calculatoire devient un obstacle dirimant pour la génération massive de données. La validation statistique robuste des algorithmes de désentrelacement ou d'identification exige de couvrir des milliers de variations de scénarios, une tâche qui, avec le coefficient d'expansion actuel, nécessiterait des temps de calcul incompatibles avec les cycles de développement industriels, particulièrement pour les scénarios denses où la charge de calcul explose.

\subsection{Localisation du goulot d'étranglement} 
L'analyse de profilage de l'environnement numérique permet de localiser précisément l'origine de cette latence au niveau des troisième et quatrième modules fonctionnels, responsables de la détection, du pistage et de la caractérisation des impulsions. Contrairement aux modules 1 et 2, dont les calculs se font indépendamment sur chaque PDW et pourraient être parallélisés, les modules 3 et 4 fonctionnent de manière séquentielle empêchant toute parallélisation. De plus, en plus du nombre de palier qui croit avec la densité en impulsion (et donc la taille de la séquence de traitement de palier), chaque palier dispose de plus d'impulsion, où le module 3 a une complexité quadratique par rapport au nombre d'impulsion par palier.

Ce goulot d'étranglement identifié au cœur de la logique de traitement du capteur constitue le verrou technologique qu'il est nécessaire de lever. 
L'objectif de la thèse étant d'accélérer le fonctionnement de l'environnement numérique avec de l'IA, nous focaliserons nos efforts sur la reproduction du goulot d'étranglement, les modules 3 et 4, que nous appellerons (trouver une appellation), par IA. ajouter en commentaire sur les données, en mode si en entrée on a une séquences de PDW représentant l'information en entrée de CAN, on a en sortie la séquence de PDW qui auraient été intercepté dans ces conditions. commentaire sur la nature des données : séquence de taille variable en entrée, séquence de taille variable en sortie, des éléments "vecteurs" comme des mots mais pas des mot : espace continue

\section{Un problème pas beaucoup trop simple}
discussion sur la complexité du problème, double problématique génération et traitement de séquence.


\section{Identification du goulot d'étranglement et limites opérationnelles}
\subsection{L'impératif de temps réel et le coût de la simulation}
Si l'architecture modulaire de l'environnement numérique garantit une fidélité satisfaisante des données produites, son exploitation opérationnelle se heurte à une contrainte majeure de performance temporelle. Actuellement, la simulation détaillée des traitements du capteur présente un coefficient d'expansion temporel moyen de l'ordre de 100 : la simulation d'une seule seconde de scénario nécessite environ cent secondes de calcul. Cette latence prohibe l'utilisation du simulateur pour des applications critiques nécessitant une interaction en temps réel, telles que la formation des pilotes et des opérateurs de guerre électronique. Dans ces configurations, où l'humain doit réagir dynamiquement aux menaces détectées par le capteur virtuel pour manœuvrer l'aéronef ou engager des contre-mesures, la fluidité de la simulation est un prérequis absolu. Par ailleurs, même dans le cadre de la validation algorithmique hors ligne, ce coût calculatoire devient un obstacle dirimant pour la génération massive de données. La validation statistique robuste des algorithmes de désentrelacement ou d'identification exige de couvrir des milliers de variations de scénarios, une tâche qui, avec le coefficient d'expansion actuel, nécessiterait des temps de calcul incompatibles avec les cycles de développement industriels, particulièrement pour les scénarios denses où la charge de calcul devient exponentielle.
\subsection{Analyse de la complexité et localisation du verrou}
L'analyse de profilage de l'environnement numérique permet de localiser précisément l'origine de cette latence au niveau des troisième et quatrième modules fonctionnels, responsables de la détection spectrale, du pistage et de la caractérisation des impulsions. Une distinction structurelle fondamentale sépare ces modules des étages amont. Les modules 1 et 2 (génération et propagation) appliquent des transformations physiques indépendantes sur chaque impulsion ; ils se prêtent donc naturellement à une parallélisation massive sur CPU ou GPU. À l'inverse, les modules 3 et 4 opèrent intrinsèquement de manière séquentielle : l'état du système à l'instant $t$ (les pistes actives, les masquages en cours) dépend de l'histoire du traitement, empêchant toute parallélisation temporelle simple.
De surcroît, la complexité algorithmique de ces blocs est critique. Le nombre de paliers temporels à traiter croît linéairement avec la densité d'impulsions du scénario. Or, au sein de chaque palier, le module de détection doit effectuer des comparaisons croisées entre toutes les impulsions présentes pour résoudre les masquages et les ambiguïtés, induisant une complexité quadratique par rapport au nombre d'impulsions locales. Cette combinaison d'une structure séquentielle rigide et d'une complexité locale quadratique crée un goulot d'étranglement majeur dès que la densité du scénario augmente. Ce verrou, situé au cœur de la logique de traitement du capteur, constitue l'obstacle technologique qu'il est nécessaire de lever. L'objectif de la thèse est donc d'accélérer l'environnement numérique en substituant ces deux modules critiques, que nous désignerons conjointement comme le Sous-système de Traitement Numérique, par un modèle d'intelligence artificielle optimisé.

\section{Formalisation et complexité du problème d'apprentissage}
\subsection{Nature des données et définition de la tâche}
Le remplacement du Sous-système de Traitement Numérique par un modèle appris pose un défi spécifique de modélisation. La tâche ne se résume pas à une classification ou une régression simple, mais s'apparente à un problème de transduction de séquence complexe. L'entrée du modèle est constituée par la séquence de PDW "modulés" (sortie du module 2), représentant l'information physique brute présente aux bornes des convertisseurs analogique-numérique. La cible à prédire est la séquence de PDW "capteur" (sortie du module 4), représentant les impulsions effectivement construites et publiées par le système.\\

La difficulté première réside dans la nature des données manipulées. Contrairement aux tâches classiques de Traitement du Langage Naturel (NLP) où les séquences sont formées de mots discrets issus d'un dictionnaire fini, les PDW évoluent dans un espace continu multidimensionnel. Chaque élément de la séquence est un vecteur de valeurs réelles (Date, Fréquence, Largeur, Amplitude, Azimut) nécessitant une précision numérique fine, incompatible avec les approches de discrétisation standard. De plus, la topologie temporelle des séquences d'entrée et de sortie diffère significativement. En raison des phénomènes de masquage, de fusion et de scission décrits précédemment, la longueur de la séquence de sortie $M$ est variable et décorrélée de la longueur de la séquence d'entrée $N$. Le modèle ne doit donc pas simplement transformer les caractéristiques d'une impulsion donnée, mais générer une nouvelle chronologie d'événements cohérente.

\subsection{Une double problématique : Traitement de séquence et Génération}
Cette caractérisation place le problème à l'intersection de deux domaines de l'apprentissage profond. Cette caractérisation ancre le problème à l'intersection de deux paradigmes de l'apprentissage profond. D'une part, il s'agit fondamentalement d'un problème de traitement de séquence (Sequence Processing), imposé par la nature variable et décorrélée des dimensions temporelles en jeu. Contrairement à des tâches de régression à taille fixe, le modèle doit traiter une série d'entrée de longueur arbitraire $N$ pour produire une série de sortie de longueur $M$, sans qu'il existe de règle de proportionnalité simple entre $N$ et $M$. Cette contrainte structurelle oblige l'architecture à s'abstraire de la dimension temporelle absolue pour raisonner sur des dépendances contextuelles relatives, nécessitant des mécanismes capables de mapper une séquence de taille variable vers une représentation latente fixe ou dynamique.\\

D'autre part, il s'agit d'un problème de génération conditionnelle, car le modèle doit construire intégralement une séquence de sortie dont la structure événementielle diffère fondamentalement de l'entrée. Il ne s'agit pas simplement de filtrer ou d'altérer les impulsions incidentes, mais de générer une nouvelle série de descripteurs qui constituent une interprétation de la réalité physique par la logique de suivi. Dans cette reconstruction, la correspondance univoque disparaît : un événement physique unique peut se trouver traduit par une succession de plusieurs PDW distincts, brisant ainsi toute relation bijective et faisant de la séquence de sortie une entité structurellement autonome plutôt qu'une version dégradée de l'entrée. Cette dualité, combinée à l'exigence de précision sur des variables continues, écarte les solutions sur étagère et nécessite la conception d'une architecture capable de marier les mécanismes d'attention globaux propres à la compréhension de contexte et les capacités de régression fine nécessaires à la reconstruction physique du signal.

\subsection{Nature des données et définition formelle de la tâche}
L'objectif est de substituer au Sous-système de Traitement Numérique un modèle appris capable d'approximer sa fonction de transfert globale. Du point de vue des données, cette tâche se formalise comme un problème de transduction de séquence s'opérant dans un espace continu. L'entrée du modèle, notée $X$, est la séquence de PDW "modulés" issue du module de propagation (module 2). Elle représente l'information physique brute incidente aux bornes des convertisseurs analogique-numérique. La cible à prédire, notée $Y$, est la séquence de PDW "capteur" (sortie du module 4), correspondant aux impulsions effectivement construites et publiées par le système.\\

Ces données présentent deux caractéristiques structurelles fondamentales. Premièrement, elles appartiennent à un espace continu multidimensionnel. Chaque élément constitutif des séquences $X$ et $Y$ est un vecteur de valeurs réelles (Date, Fréquence, Largeur, Amplitude, Azimut) définissant les propriétés physiques de l'impulsion. Deuxièmement, les séquences se caractérisent par une cardinalité variable et asynchrone. La topologie temporelle diffère entre l'entrée et la sortie en raison des mécanismes internes du capteur (masquages, fusions, scissions). Ainsi, la longueur $N$ de la séquence d'entrée et la longueur $M$ de la séquence de sortie sont variables et ne respectent pas de règle de proportionnalité stricte ou de synchronisation élément par élément.

\subsection{Une double problématique : Traitement de séquence et Génération}
Cette caractérisation ancre le problème à l'intersection de deux paradigmes de l'apprentissage profond, induisant une complexité spécifique qui le distingue des applications classiques. D'une part, il s'agit fondamentalement d'un problème de traitement de séquence (Sequence Processing), complexifié par la nature continue des données. Contrairement aux tâches de Traitement du Langage Naturel (NLP) où les séquences sont formées de mots discrets issus d'un dictionnaire fini, les PDW exigent une précision numérique fine sur plusieurs variables continues simultanément. De plus, la nature variable et décorrélée des dimensions temporelles ($N \neq M$) empêche l'usage d'architectures à taille fixe. Cette contrainte structurelle oblige le modèle à s'abstraire de la dimension temporelle absolue pour raisonner sur des dépendances contextuelles relatives, nécessitant des mécanismes capables de mapper une séquence de taille variable vers une représentation latente dynamique.\\

D'autre part, il s'agit d'un problème de génération conditionnelle. Le modèle ne doit pas simplement filtrer ou altérer les impulsions incidentes, mais construire intégralement une séquence de sortie dont la structure événementielle diffère fondamentalement de l'entrée. La logique de suivi temporel du capteur brise la relation bijective : un événement physique unique peut se trouver traduit par une succession de plusieurs PDW distincts, ou inversement, plusieurs événements peuvent être fusionnés. Le modèle doit donc apprendre à générer une nouvelle série de descripteurs qui constituent une interprétation synthétique de la réalité physique. Dans cette reconstruction, la séquence de sortie devient une entité structurellement autonome plutôt qu'une version simplement dégradée de l'entrée. Cette dualité, combinée à l'exigence de précision, écarte les solutions sur étagère et nécessite la conception d'une architecture capable de marier les mécanismes d'attention globaux propres à la compréhension de contexte et les capacités de régression fine nécessaires à la reconstruction physique du signal.

