\chapter{État de l'art : PLAN (à supprimer après rédaction)}

Le chapitre sur l'état de l'art se découpe en 4 parties.

\section{Introduction}

Cette section aborde les aspects suivants :
\begin{itemize}
	\item Environnements numériques
	\item Injection 1 : génération et modélisation de l'environnement
	\item Injection 2 : simulation de phénomènes physiques
	\item Injection 3 : adaptation et interaction
\end{itemize}

\section{IA générative}

Cette section présente le domaine de l'IA générative. Notre problème peut y être naïvement associé mais en réalité quasiment aucune des méthodes ne sera applicable. Les aspects présentés sont:
\begin{itemize}
	\item Le concept d'IA générative. En notant que n'importe quelle fonction génère une sortie à partir d'une entrée et que la dérive de tout appeler IA générative est tentante.
	\item Les VAE et spécialement VAE conditionnels
	\item Les GAN et spécialement GAN conditionnels
	\item Les modèles de diffusion et spécialement ceux conditionnels
	\item Les modèles de langage et GPT
\end{itemize}

\section{Méthodes pour le traitement de séquence}
Cette section présente les architectures connues pour leurs capacités à traiter des séquences, de leurs formes les plus simples aux formes les plus complexes. Par ordre d'apparition:
\begin{itemize}
	\item Le concept de séquence: notion de proximité dans un ensemble. Série temporelle, image, texte.
	\item Réseau de convolution:
	\begin{itemize}
		\item Histoire de son apparition: dans l'image
		\item Comment la convolution interagit avec la séquence
		\item La convolution dans l'image (vue comme une séquence)
		\item La convolution dans le texte
		\item La convolution ailleurs
	\end{itemize}
	\item Réseau de neurones récurrents:
	\begin{itemize}
		\item Histoire de son apparition
		\item Comment un RNN interagit avec la séquence
		\item Variante SSM
		\item RNN dans le texte
		\item RNN dans les systèmes temporels (chaine de Markov)
	\end{itemize}
	\item Transformer:
	\begin{itemize}
		\item Histoire de son apparition: dans le langage
		\item Comment le Transformer interagit avec la séquence
		\item Transformer dans le texte (traduction, GPT, ...)
		\item Transformer dans les systèmes temporels (chaine de Markov)
		\item Transformer dans l'image
		\item Transformer ailleurs (généralisation)
	\end{itemize}
\end{itemize}

\section{Les améliorations}
Cette section met en avant les difficultés liées à l'apprentissage automatique, entre complexité calculatoire, mémorielle et instabilité en entrainement. À cette occasion, nous montrons les propositions existantes visant à résoudre ces problèmes. Par ordre d'apparition:
\begin{itemize}
	\item Compréhension des architectures: Mechanistic Interpretability
	\item Présentation des soucis de performances 
	\item Présentation des solutions aux soucis de performances
	\begin{itemize}
		\item Positional Encoding
		\item Certains mécanismes d'attention
		\item Pre-Training
		\item Embedding et tokenization
	\end{itemize}
	\item Présentation des soucis de stabilité
	\item Présentation des solutions aux soucis de stabilité:
	\begin{itemize}
		\item Layer-norm
		\item Initialisation
		\item Structure (hyper-paramètre de manière générale)
	\end{itemize}
	\item Présentation des soucis d'efficacité et leurs solutions
	\begin{itemize}
		\item Complexité mémoire et calcul: mécanisme d'attention
		\item Vitesse d'entrainement: MAMBA ?
	\end{itemize}
\end{itemize}


\chapter{État de l'art}
Ce chapitre présente les concepts et méthodes fondamentaux sur lesquels s'appuie cette thèse. Nous commencerons par définir la notion d'Environnement Numérique ainsi que les différents apports de l'IA à la simulation. Nous exposerons ensuite les approches d'IA générative, avant de nous concentrer sur les méthodes de traitement de séquence, en détaillant particulièrement l'architecture Transformer. Enfin, nous conclurons par une analyse des défis inhérents à l'entraînement de ces modèles profonds, en passant en revue les solutions techniques existantes pour garantir leur stabilité, leur efficacité calculatoire et leur interprétabilité.


\section{Introduction : Environnements Numériques et typologie des apports de l'IA}

Portée par l'augmentation de la puissance de calcul et la disponibilité croissante des données, l'utilisation de représentations virtuelles pour l'analyse et l'optimisation des systèmes physiques s'est généralisée. Dans ce contexte, les notions de « jumeau numérique » et « environnement numérique » sont souvent employés de manière interchangeable, engendrant une confusion sémantique que cette section a pour objectif de démêler. Nous retracerons dans un premier temps l'origine et les définitions, tant idéales que pragmatiques, du jumeau numérique. Dans un second temps, nous présenterons une définition unificatrice et fonctionnelle de l'environnement numérique. Enfin, une synthèse comparative nous permettra d'établir une distinction claire basée sur les flux de données et le critère d'individualisation, et de justifier le positionnement terminologique adopté dans le cadre de cette étude.

\subsection{Cadre Conceptuel : Environnement Virtuel et Jumeau Numérique}
Le concept de jumeau numérique, popularisé et formalisé dès le début des années 2000 par les travaux de Michael Grieves dans le domaine manufacturier \cite{grieves_digital_2014}, puis théorisé comme un pilier des systèmes cyber-physiques (CPS) par des auteurs comme Negri et al. \cite{negri_review_2017}, a connu une adoption rapide et variée à travers l'industrie.

Si le terme de "jumeau numérique" s'est imposé dans le paysage technologique, sa définition précise fait l'objet d'un débat animé entre une vision idéale et une approche pragmatique. D'un côté, une conception formelle, s'appuyant sur les travaux fondateurs de la NASA \cite{grieves_digital_2014}, défend l'idée qu'un véritable jumeau numérique se caractérise par un couplage bidirectionnel et dynamique avec son homologue physique. Dans cette perspective exigeante, le jumeau n'est pas une simple représentation ; il constitue une instance virtuelle dont l'état est synchronisé en continu par les flux de données issus du système physique et qui, en retour, pilote, optimise et prédit le comportement de ce dernier \cite{negri_review_2017}.. Cette boucle fermée est considérée comme la condition permettant de distinguer le jumeau numérique d'un simple modèle ou d'une simulation. De l'autre, une approche plus pragmatique, largement répandue dans l'industrie, adopte une définition évolutive et par niveaux de maturité. Dans cette vision, une maquette 3D enrichie de données, parfois qualifiée de "digital shadow", peut déjà être appelé "jumeau numérique". Cette flexibilité sémantique, bien que source de confusion, reflète la réalité des projets industriels où la complexité et le coût d'une intégration parfaite imposent une progression par étapes. Malgré tout, une ligne de démarcation essentielle fait consensus : l'existence d'un transfert de données automatique du système physique vers son représentant virtuel. Sans ce flux, la représentation demeure une simulation ou un modèle générique, que nous qualifierons ici d'« environnement numérique ». Par exemple les simulateurs de conduite autonome comme CARLA \cite{dosovitskiy_carla_2017} sont des environnements numériques essentiels pour l'entraînement des algorithmes d'IA, mais ils simulent un monde routier générique non couplé à un véhicule physique unique, et ne sont en ce sens pas des jumeaux numériques. En revanche, certains simulateurs de moteur d'avion, comme ceux déployés par General Electric \cite{tao_digital_2018}, qui est alimenté en temps réel par les données de vol de l'équipement spécifique, incarnent la définition minimale du jumeau numérique, souvent appelée « Digital Shadow ». Ils permettent un suivi individualisé de l'état de santé et de l'usure de chaque moteur de la flotte.

Pour désigner les représentations numériques qui ne sont pas couplées à une instance physique unique, nous recourons donc au terme plus large et unificateur d'Environnement Numérique (Virtual Environment - VE).

La notion d'Environnement Virtuel est interdisciplinaire, et sa définition varie selon que l'on se place dans la communauté de la Réalité Virtuelle, de l'Ingénierie Système ou de l'Intelligence Artificielle. La recherche en Réalité Virtuelle, historiquement focalisée sur l'immersion sensorielle et l'interaction humain-machine, définit souvent les VE comme des « mondes synthétiques générés par ordinateur dans lesquels l'utilisateur a un sentiment d'être présent et d'y interagir » \cite{sherman_understanding_2018}. Cette perspective met l'accent sur les aspects perceptuels et cognitifs. En revanche, dans les domaines de l'ingénierie et de l'IA, l'accent est davantage porté sur la fonction de simulation et de cadre d'expérimentation. Ici, un VE est vu comme un « modèle informatique exécutable d'un système » \cite{fritzson_principles_2015} ou un « cadre de simulation qui permet le test et la validation d'algorithmes dans des conditions contrôlées et reproductibles » \cite{brockman_openai_2016}. Cette vision est moins concernée par l'immersion de l'utilisateur que par la fidélité de la modélisation des processus et des interactions.

Pour englober ces différentes finalités – de la formation immersive au banc d'essai algorithmique – nous proposons la définition unificatrice suivante : Un Environnement Virtuel (VE) désigne une simulation numérique interactive modélisant un ensemble d'entités et de phénomènes, dans le but d'observer, d'analyser ou d'expérimenter des comportements au sein d'un cadre contrôlé.

Cette définition permet de tracer une ligne de démarcation nette avec le concept voisin de Jumeau Numérique. La distinction fondamentale réside dans le principe d'individualisation par les données. Le jumeau numérique se définit intrinsèquement comme l'avatar d'une instance physique unique, tel un moteur spécifique, dont l'essence est indissociable d'un lien de données continu avec son homologue réel. En revanche, l'environnement numérique se conçoit comme une représentation générique d'une classe de systèmes, dont la valeur réside dans la modélisation fidèle de lois physiques au sein d'un cadre reproductible, indépendamment d'une instance matérielle particulière.\\

Ainsi clarifiée, la notion de VE couvre un spectre étendu, allant du monde immersif interactif au simulateur technique. Dans le contexte spécifique du développement algorithmique, qui est le nôtre, le VE devient un outil de prototypage et de validation indispensable : il permet de reproduire des situations expérimentales complexes, de générer des données synthétiques massives et de tester des modèles de manière intensive, sûre et économique, sans les contraintes logistiques des dispositifs physiques réels. Pour structurer l'état de l'art des méthodes d'Intelligence Artificielle au service de ces environnements, nous organiserons la suite de notre analyse selon trois axes d'intervention distincts : la constitution géométrique et visuelle de l'environnement, la représentation des phénomènes physiques par apprentissage, et enfin les capacités d'interaction et d'adaptation dynamique.


\subsection{L'IA pour la constitution géométrique et visuelle de l'environnement}

L'intégration de l'apprentissage profond dans la chaîne de production des environnements virtuels marque une transition technologique de la modélisation explicite vers les représentations apprises. Cette évolution s'articule autour de deux axes. Le premier concerne la capacité de l'IA à encoder la scène non plus sous forme de maillages géométriques, mais via des représentations implicites optimisées pour le réalisme visuel. Ces méthodes encodent la scène dans les poids d'un réseau, permettant de modéliser des phénomènes complexes comme la transparence ou les réflexions dépendantes du point de vue. Elles réduisent ainsi l'écart de performance lors du transfert du simulateur vers le réel en fournissant aux agents des observations capteurs quasi-identiques à la réalité. Le second axe porte sur l'automatisation de la synthèse d'objets, où les modèles génératifs assistent la création géométrique pour accélérer le peuplement des univers virtuels. Ces deux mécanismes, la reconstruction par représentation implicite et la génération d'éléments par diffusion, seront détaillés respectivement dans les deux paragraphes suivants.

\subsubsection{Modélisation : reconstruction neurale et représentations implicites}
L'enjeu de la numérisation d'environnements pour la simulation est de reproduire le réel avec une fidélité suffisante pour garantir la validité des expérimentations virtuelles. Le défi technique est la Synthèse de Nouveaux Points de Vue qui permet d'observer une scène depuis des angles non capturés initialement. Les méthodes classiques de photogrammétrie montrent ici une limitation majeure : en figeant l'apparence sur des maillages statiques, elles échouent à reproduire les variations d'éclairage dépendantes de la position de l'observateur (reflets, transparence). Pour lever ce verrou, les \textit{Neural Radiance Fields} (NeRF) \cite{mildenhall_nerf_2022} ont introduit les représentations implicites. En apprenant la fonction de transport de la lumière via un réseau de neurones, cette approche permet de générer des vues inédites où les interactions lumineuses s'adaptent dynamiquement au mouvement de la caméra, garantissant un photoréalisme supérieur.

Toutefois, le coût calculatoire inhérent au NeRF limite son usage interactif. Des optimisations algorithmiques majeures ont été proposées pour pallier cette latence, notamment le \textit{Hash Encoding} multi-résolution \cite{muller_instant_2022} qui réduit drastiquement les temps d'apprentissage. Plus récemment, une rupture vers des représentations hybrides a été opérée avec le \textit{3D Gaussian Splatting} \cite{kerbl_3d_2023}. Cette méthode substitue au coûteux échantillonnage volumétrique une projection (rasterization) rapide de gaussiennes 3D anisotropes. Elle concilie ainsi la fidélité visuelle des représentations neurales avec les performances temps réel (> 100 FPS) indispensables à l'interactivité au sein d'un environnement numérique.


\subsubsection{Génération: synthèse d'objets par diffusion}
Au-delà de la reproduction du réel, la simulation requiert la capacité de générer des environnements variés incluant des objets ou des conditions non observés. L'IA générative intervient ici pour la création d'objets 3D, palliant la rareté des banques de modèles 3D par l'exploitation des vastes ensembles de données image-texte 2D.

L'état de l'art actuel s'appuie sur le transfert de connaissances depuis des modèles de diffusion 2D pré-entraînés vers la 3D. La méthode \textit{DreamFusion} (Poole et al., 2022) \cite{poole_dreamfusion_2022} a formalisé ce principe via le \textit{Score Distillation Sampling} (SDS). Cette technique utilise un modèle de diffusion 2D comme fonction de critique pour optimiser une représentation 3D (telle qu'un NeRF), de sorte que ses rendus 2D correspondent à une description textuelle donnée. Des itérations ultérieures, comme \textit{ProlificDreamer} (Wang et al., 2023) \cite{wang_prolificdreamer_2023}, ont affiné ce processus via le \textit{Variational Score Distillation} (VSD) pour améliorer la fidélité géométrique et la résolution des textures. Ces approches permettent d'envisager des pipelines de "texte-vers-environnement", où la description sémantique d'une scène suffit à instancier un cadre de simulation complet et physiquement cohérent.


\subsection{L'IA pour l'accélération et la modélisation des phénomènes physiques}

L'objectif de cette seconde injection est de substituer ou d'accélérer les solveurs numériques traditionnels (Éléments Finis, Volumes Finis) dont la complexité calculatoire limite les applications temps réel. L'état de l'art s'articule autour de la manière dont la connaissance physique est intégrée dans le modèle d'apprentissage. Nous distinguons trois niveaux d'intégration, allant de l'apprentissage pur par les données à l'intégration structurelle des lois physiques.

\subsubsection{Apprentissage par Observation (Data-Driven)}
Le premier niveau considère le simulateur comme une "boîte noire" dont il faut approximer la fonction de transfert à partir d'observations. L'IA apprend ici les corrélations spatio-temporelles sans connaissance explicite des équations sous-jacentes. Les Graph Neural Networks (GNN) se sont imposés comme l'architecture de référence pour cette tâche, notamment pour les systèmes lagrangiens (particules). Les travaux sur les \textit{Graph Network-based Simulators} (GNS) \cite{sanchez-gonzalez_learning_2020} démontrent une capacité remarquable à prédire la dynamique de fluides et de solides déformables en modélisant les interactions locales par passage de messages. Bien que très rapides à l'inférence, ces modèles souffrent d'un manque de garanties physiques : sans contrainte explicite, ils peuvent violer les lois de conservation (masse, énergie) et dériver sur de longues horizons temporels.

\subsubsection{Apprentissage contraint par les Équations (Physics-Informed)}
Pour pallier le manque de robustesse physique et la dépendance aux données, une seconde approche intègre les Équations aux Dérivées Partielles (EDP) directement dans l'optimisation. C'est le paradigme des Physics-Informed Neural Networks (PINNs) \cite{raissi_physics-informed_2019}. Ici, le réseau de neurones agit comme un approximateur universel de la solution, et sa fonction de coût inclut les résidus de l'équation physique (ex: Navier-Stokes). Cette méthode permet de s'affranchir partiellement ou totalement de données d'étiquetage (apprentissage non supervisé par la physique). Cependant, les PINNs font face à des défis d'optimisation majeurs lorsqu'ils sont confrontés à des dynamiques multi-échelles ou chaotiques.

\subsubsection{Apprentissage structuré par la Physique (Inductive Bias)}
Le troisième niveau d'intégration cherche à inscrire les lois physiques non plus dans la fonction de perte (contrainte douce), mais dans l'architecture même du réseau (contrainte dure ou biais inductif). D'une part, les Hamiltonian Neural Networks (HNN)  \cite{greydanus_hamiltonian_2019} imposent une structure symplectique au réseau. Au lieu d'apprendre directement les accélérations, le réseau apprend l'Hamiltonien (l'énergie totale) du système, garantissant par construction la conservation de l'énergie et la réversibilité temporelle, ce qui est crucial pour la stabilité des simulations orbitales ou mécaniques sur le très long terme. D'autre part, les Fourier Neural Operators (FNO) \cite{li_fourier_2021} exploitent la structure spectrale des solutions d'EDP. En apprenant l'opérateur intégral dans l'espace de Fourier, ils acquièrent une propriété d'invariance à la discrétisation (zero-shot super-resolution), permettant de prédire la physique à des résolutions arbitraires, une propriété structurelle absente des CNN ou MLP classiques.

\subsection{L'IA au service de l'interactivité et de l'adaptation décisionnelle}
Cette dernière dimension transforme l'environnement numérique d'un cadre passif en un écosystème réactif et adaptatif. L'objectif est d'enrichir la dynamique interactionnelle pour confronter le système sous test à des situations d'une complexité réaliste, impossible à coder manuellement via des scénarios déterministes.

\subsubsection{L'environnement peuplé d'agents apprenants (IA comme Acteur)} 
La première contribution de l'IA est le remplacement des entités scriptées (PNJ, trafic, adversaires) par des agents autonomes pilotés par des politiques neuronales. Contrairement aux machines à états finis classiques, prévisibles et limitées, ces agents sont entraînés via l'Apprentissage par Renforcement Multi-Agents (MARL) ou des mécanismes de Self-Play \cite{silver_mastering_2017}. Cela permet de peupler le VE d'adversaires ou de collaborateurs capables de stratégies émergentes et optimales. L'exemple du défi DARPA AlphaDogfight (2020), où des agents IA ont développé des manœuvres de combat aérien surclassant les experts humains, illustre comment l'injection d'agents apprenants permet de soumettre le système testé à des niveaux de difficulté et de réalisme inatteignables par des méthodes heuristiques \cite{demay_alphadogfight_2022}. Ici, l'environnement devient "intelligent" car ses composantes actives s'adaptent au comportement de l'utilisateur ou du système validé.

\subsubsection{L'environnement comme générateur de curriculum (IA comme Superviseur)} 
La seconde contribution concerne le pilotage des paramètres de la simulation par des algorithmes d'optimisation ou évolutionnaires. Au-delà du simple Domain Randomization aléatoire \cite{tobin_domain_2017}, qui manque de direction, l'IA est utilisée pour structurer activement l'apprentissage : c'est l'Apprentissage de Curriculum Automatique (Automatic Curriculum Learning). Des méthodes comme POET (Paired Open-Ended Trailblazer) utilisent des algorithmes évolutionnaires pour co-générer l'environnement en même temps que l'agent \cite{wang_paired_2019}. L'algorithme cherche spécifiquement à générer les configurations topologiques ou physiques (terrains accidentés, conditions météo limites) qui maximisent le progrès de l'agent, créant une "course à l'armement" entre la difficulté du monde et la compétence de l'agent. Dans ce cadre, les algorithmes évolutionnaires agissent comme une forme d'IA générative fonctionnelle, créant des scénarios pertinents et ciblés ("Edge cases") que le hasard seul ne produirait que rarement. \\ 

Ainsi, l'IA transforme le simulateur : d'un simple banc d'essai physique, il devient un partenaire d'entraînement actif, capable de générer des opposants redoutables et d'adapter sa propre complexité pour guider l'apprentissage.

\subsection{Ancrage dans la problématique}
Au regard des distinctions établies précédemment, la classification du système étudié s'opère sans équivoque. Le simulateur de capteur de Mesures de Soutien Électronique que nous analysons a pour fonction de générer des données synthétiques à partir de scénarios tactiques génériques. Il ne modélise pas le comportement d'un équipement matériel spécifique connecté en temps réel, mais reproduit le fonctionnement théorique d'une classe de capteurs face à des environnements simulés. Par conséquent, l'absence de couplage à une instance physique unique et la nature générique des scénarios valident l'usage exclusif du terme d'Environnement Numérique (VE) pour désigner notre cadre d'étude.\\

L'analyse des trois axes d'intégration de l'IA révèle par ailleurs la singularité de notre approche au sein de ce VE. Bien que l'objectif fonctionnel rejoigne la "Seconde injection" visant l'accélération de la physique, les méthodes classiques sont inadaptées car elles traitent principalement des champs spatiaux continus. Or, notre goulot d'étranglement réside dans la transformation de séquences d'événements discrets (les PDW). Notre problématique se situe donc à l'intersection de la simulation physique et du traitement de l'information : il s'agit d'apprendre la fonction de transfert du capteur, un processus qui s'apparente conceptuellement à une tâche de "traduction" d'un état physique vers un état perçu. Ce constat motive l'adoption d'une approche fondée sur l'IA générative constructive et les architectures de traitement de séquence, dont nous explorerons les fondements théoriques dans les sections suivantes.


\section{IA générative}

Dans le paysage contemporain de l'apprentissage profond, la définition de l'IA générative a évolué au-delà de la stricte opposition statistique entre modèles de densité et modèles discriminants. Là où un modèle classique condense l'information (classification, réduction de dimension), un modèle génératif apprend à construire des données de haute dimension, structurées spatialement ou temporellement, en capturant les dépendances complexes inhérentes au jeu d'entraînement. L'IA générative désigne aujourd'hui une classe d'architectures neuronales caractérisée par sa capacité de synthèse. 

Un modèle est qualifié de génératif dès lors qu'il construit une donnée structurée en capturant la distribution de probabilité sous-jacente. L'objectif n'est pas simplement d'estimer une valeur locale, mais de bâtir une cohérence globale, respectant les corrélations intrinsèques du domaine d'apprentissage. Cette définition par la capacité constructive est particulièrement pertinente pour la modélisation de systèmes physiques, où la distinction entre discret et continu s'estompe

Dans le contexte spécifique de l'accélération de simulation, nous nous intéressons particulièrement aux modèles génératifs conditionnels, capables de produire une sortie structurée complexe, tel un champ physique ou un état futur, correspondant à une condition initiale. Cette section explore l'évolution chronologique de ces architectures, depuis les approches opérant dans des espaces continus jusqu'aux paradigmes séquentiels discrets.

\subsection{L'approche probabiliste explicite : Les VAE (2013)}

La première avancée significative dans l'apprentissage profond de distributions complexes fut l'introduction des Auto-encodeurs Variationnels (VAE). Contrairement aux auto-encodeurs classiques qui compressent l'information en un point déterministe de l'espace latent, les VAE imposent une structure probabiliste à cet espace, généralement sous la forme d'une distribution gaussienne multivariée. L'innovation majeure réside dans l'introduction de l'astuce de reparamétrisation (reparameterization trick), qui rend le processus d'échantillonnage différentiable et permet l'optimisation du modèle par descente de gradient en maximisant la borne inférieure de la vraisemblance (ELBO) \cite{kingma_auto-encoding_2013}.
Cette capacité à structurer l'espace latent est particulièrement pertinente pour les problèmes de simulation où une même condition initiale peut mener à plusieurs résultats possibles. Dans leur article sur les VAE Conditionnels (C-VAE) \cite{sohn_learning_2015}, il est prouvé qu'il est possible de modéliser des sorties structurées multimodales en conditionnant la génération à la fois par une variable latente aléatoire et par une observation d'entrée. Bien que théoriquement élégants, les VAE ont souffert historiquement d'une limitation qualitative, leur fonction de perte tendant à produire des résultats lissés. Cependant, des développements récents ont redonné une pertinence majeure à cette famille, notamment via la quantification vectorielle de l'espace latent (VQ-VAE). Ces modèles discrets sont désormais au cœur d'architectures de pointe comme les World Models, où un agent apprend à "rêver" des futurs possibles dans un espace latent compact pour accélérer l'apprentissage par renforcement en robotique \cite{razavi_generating_2019}, \cite{ha_world_2018}.


\subsection{La révolution antagoniste : Les GAN (2014)}

Pour répondre au manque de piqué et de réalisme des méthodes variationnelles, une rupture paradigmatique a été introduite avec les Réseaux Antagonistes Génératifs (GAN). Cette approche délaisse l'estimation explicite de la densité de probabilité au profit d'une méthode implicite fondée sur la théorie des jeux. Le processus d'apprentissage est modélisé comme un jeu minimax à somme nulle entre un générateur qui tente de créer des données indiscernables du réel, et un discriminateur qui tente de distinguer les échantillons générés des données d'entraînement \cite{goodfellow_generative_2020}. Comme le soulignent les travaux théoriques sur les modèles implicites, cette formulation permet au générateur d'apprendre des statistiques d'ordre supérieur souvent ignorées par les méthodes classiques, s'affranchissant des contraintes de vraisemblance \cite{mohamed_learning_2017}.
Dans le cadre de la "traduction" d'environnement, les variantes conditionnelles telles que l'architecture Pix2Pix se sont imposées pour transformer une représentation sémantique en une image photoréaliste, produisant des structures fines et des textures détaillées \cite{isola_image--image_2017}. Si les GAN restent difficiles à stabiliser durant l'entraînement, ils ont démontré des capacités de généralisation spectaculaires au-delà de l'image statique. Des travaux comme tempoGAN ont par exemple appliqué ce principe à la mécanique des fluides, parvenant à super-résoudre des simulations volumétriques de fumée ou de liquide tout en garantissant une cohérence temporelle que les méthodes purement statistiques peinent à maintenir \cite{xie_tempogan_2018}.

\subsection{La génération par raffinement : Les Modèles de Diffusion (2020)}

La dernière vague d'innovation, qui définit une grande partie de l'état de l'art actuel, puise son inspiration dans la physique statistique hors équilibre. Les modèles de diffusion probabilistes proposent de construire la génération comme l'inversion d'un processus de destruction d'information. L'idée consiste à détruire progressivement la structure des données par l'ajout successif de bruit gaussien, puis d'entraîner un réseau de neurones à inverser ce processus temporel pour reconstruire la donnée originale étape par étape \cite{sohl-dickstein_deep_2015}. Ce concept a été porté à maturité avec les Denoising Diffusion Probabilistic Models (DDPM), qui offrent un compromis inédit : ils atteignent une qualité d'échantillonnage supérieure aux GAN tout en couvrant mieux la diversité de la distribution des données, évitant le problème d'effondrement de mode \cite{ho_denoising_2020}.
Bien que le processus itératif soit intrinsèquement lent, des méthodes d'échantillonnage accélérées (DDIM) ont rendu ces modèles exploitables en production \cite{song_denoising_2022}. Au-delà de la génération d'images 2D, ce paradigme est aujourd'hui le moteur de la génération de contenus pour les environnements virtuels. Des approches comme DreamFusion utilisent un modèle de diffusion 2D pré-entraîné pour optimiser une représentation volumétrique (NeRF), permettant de générer des objets 3D complets et cohérents à partir d'une simple description textuelle, ouvrant la voie à la création procédurale d'environnements physiques complexes \cite{poole_dreamfusion_2022}.


\subsection{Le paradigme séquentiel et l'Autorégression}

Enfin, une approche radicalement différente considère la génération comme une prédiction séquentielle discrète. Ce paradigme trouve ses racines dans les Réseaux de Neurones Récurrents (RNN), historiquement utilisés pour générer du texte ou des séries temporelles, mais limités par leur mémoire à court terme et leur séquentialité stricte \cite{graves_generating_2014}. La rupture fondamentale survient avec l'introduction de l'architecture Transformer et du mécanisme d'attention, qui permet de modéliser des dépendances à très long terme et de paralléliser le calcul \cite{vaswani_attention_2017}.
L'évolution majeure de ce paradigme réside dans le concept de pré-entraînement génératif (GPT). Il a été démontré qu'un modèle entraîné massivement sur l'objectif simple de prédire le prochain élément d'une séquence acquiert une capacité de généralisation et de compréhension structurelle émergente \cite{radford_improving_2018}. Aujourd'hui, cette approche déborde largement du cadre du texte. Des architectures multimodales comme Gato \cite{reed_generalist_2022} ou les Vision Transformers (ViT) \cite{dosovitskiy_image_2020} traitent les images ou les actions de contrôle robotique comme des séquences de tokens, unifiant ainsi la génération de contenu visuel et la prise de décision séquentielle au sein d'un même formalisme autorégressif. Cela positionne le traitement de séquence comme une méthode universelle pour la simulation, justifiant l'analyse détaillée des architectures séquentielles qui suivra.

\subsection{Ancrage dans la problématique}
L'analyse du paysage de l'IA générative nous permet d'identifier les architectures les plus adaptées à la modélisation de notre simulateur de capteur. Si les modèles probabilistes explicites comme les VAE offrent une gestion intéressante de l'incertitude structurelle, leur tendance historique à produire des sorties lissées peut poser question quant à la fidélité des signaux radar, où la précision des paramètres fins tels que les fréquences est critique. Concernant les approches antagonistes (GAN), bien qu'elles soient performantes pour la synthèse d'images, leur adaptation directe à des séquences d'événements discrets paramétriques (les PDW) s'avère complexe, notamment pour gérer la causalité temporelle et la nature irrégulière du flux d'impulsions, sans compter leur instabilité d'entraînement connue. De même, si les modèles de diffusion définissent l'état de l'art actuel, leur processus de débruitage itératif est intrinsèquement coûteux en temps de calcul. Cette caractéristique entre potentiellement en conflit avec notre objectif premier d'accélération de la simulation, en plus de nécessiter une adaptation lourde pour traiter des vecteurs de paramètres physiques plutôt que des pixels.

Cette analyse invite à considérer le paradigme séquentiel auto-régressif. Contrairement au Traitement du Langage Naturel qui opère sur des vocabulaires finis, notre simulation numérique évolue dans un espace métrique continu : chaque PDW est défini par des coordonnées réelles (temps d'arrivée, fréquence, largeur). Dans ce contexte, l'acte génératif ne consiste pas à sélectionner un symbole parmi un dictionnaire, mais à prédire directement les valeurs d'un état dans un espace vectoriel continu $\mathbb{R}^n$. Bien que ce processus s'apparente mathématiquement à une régression multivariée, il conserve la nature intrinsèque de la génération : le modèle doit bâtir, étape par étape, une cohérence globale du signal temporel. Ainsi, la reconstruction de la séquence de PDW perçue à partir de la séquence émise se formule comme une tâche de traduction de signal continu. Cela motive l'orientation de notre étude vers les architectures spécialisées dans le traitement de séquence, capables de capturer les dépendances à long terme comme le pistage temporel, justifiant l'analyse approfondie des RNN et des Transformers qui suivra.




\section{Méthodes de traitement : Séquences et structures spatiales}

\subsection{Typologie des données : De la causalité temporelle à la topologie spatiale}
Avant d'aborder les architectures neuronales spécifiques, il est essentiel de définir formellement l'objet mathématique qu'elles manipulent. Dans l'apprentissage statistique classique, les données sont souvent supposées être indépendantes et identiquement distribuées (hypothèse i.i.d.). Le traitement de séquences et de structures spatiales rompt fondamentalement avec cette hypothèse en introduisant une structure de dépendance intrinsèque. Une donnée n'est pas un simple ensemble non ordonné, mais une collection indexée $X = \{x_1, x_2, \dots, x_N\}$ où l'indice porte une information topologique ou causale déterminante. La valeur d'un élément $x_i$ n'a de sens que relativement à son contexte, c'est-à-dire son voisinage ou son historique. Cette propriété de dépendance locale, formalisée par les processus de Markov pour le temps ou les Champs de Markov pour l'espace, caractérise la nature de l'ensemble et impose des contraintes spécifiques de modélisation que les architectures neuronales doivent satisfaire.


\subsubsection{La séquence et la causalité}
Dans le cadre des séquences, la notion de proximité est dictée par la causalité : l'état présent est une fonction de l'histoire passée. C'est le fondement de la théorie de l'information de Shannon \cite{shannon_mathematical_1948}, où le langage est modélisé comme un processus stochastique discret. Dans cette vision, la probabilité d'apparition d'un symbole (lettre ou mot) dépend conditionnellement de la séquence des symboles précédents, définissant la notion d'entropie d'une source d'information. Cette logique s'applique identiquement aux séries temporelles continues. Des travaux sur les modèles ARIMA \cite{box_time_2015} ont montrés qu'une observation à l'instant $t$ est mathématiquement corrélée à ses prédécesseurs immédiats et aux termes d'erreur passés. Dans ce formalisme statistique, la séquence est définie par une dépendance directionnelle irréversible vers le futur.

\subsubsection{La donnée spatiale et la contiguïté}
Le champ physique ou l'image, de leur côté, se définissent comme des grilles spatiales (multidimensionnelle ou simplement bidimensionnelle) où la notion d'ordre séquentiel disparaît au profit de la contiguïté topologique. Ici, un pixel ou une cellule n'a pas de "passé" ou de "futur", mais un voisinage omnidirectionnel. Cette structure, régie par la contiguïté spatiale locale, s'affranchit de la causalité temporelle au profit d'un voisinage omnidirectionnel propre aux Champs de Markov. Elle permet de fonder théoriquement l'usage des opérations de convolution, où la notion d'ordre chronologique est remplacée par celle de proximité euclidienne. Ce concept de donnée spatiale reste néanmoins proche de celui de séquence : les travaux sur PixelRNN \cite{van_den_oord_pixel_2016} montre qu'en traitant l'image comme une séquence autorégressive, où chaque pixel dépend de ceux situés "avant" lui (en haut et à gauche), on peut modéliser la distribution conjointe des pixels et générer des structures visuelles cohérentes.


\subsubsection{Universalité de la modélisation séquentielle}
Finalement, le défi central des architectures de traitement que nous allons présenter (CNN, RNN, Transformer) est de modéliser cette fonction de dépendance conditionnelle $P(x_t | \text{Contexte})$. La nature de ce contexte varie selon le domaine : il sera un historique causal pour les séries temporelles et la génération de texte, alors qu'il est bidirectionnel et topologique pour l'image ou la compréhension sémantique globale. Cependant, l'objectif mathématique reste identique : capturer les corrélations à courte et longue portée qui structurent la donnée, transformant une collection de valeurs isolées en une entité cohérente.


\subsection{Réseaux de convolution}
Bien que les données séquentielles soient intuitivement associées à une dimension temporelle linéaire, le traitement de l'information repose fondamentalement sur l'extraction de motifs locaux et de relations de voisinage. C'est dans cette optique que les Réseaux de Neurones Convolutionnels (CNN) se positionnent comme une méthode incontournable. Initialement conçus pour la grille spatiale de l'image, ils formalisent une approche du traitement de séquence fondée sur la localité, l'invariance par translation et la hiérarchie des caractéristiques.

\subsubsection{Genèse et prédominance dans l'imagerie}
L'histoire des réseaux de convolution est indissociable de la vision par ordinateur et de la volonté de s'affranchir des descripteurs manuels  (SIFT \cite{lowe_distinctive_2004}, SURF \cite{bay_surf_2006} et HOG \cite{dalal_histograms_2005}). Inspirée par les travaux biologiques sur le cortex visuel, le premier modèle \cite{lecun_gradient-based_2002} a introduit les concepts fondateurs de champ récepteur local et de partage des poids pour la reconnaissance de caractères manuscrits. Cependant, c'est l'avènement d'AlexNet \cite{krizhevsky_imagenet_2012} qui a marqué le véritable point d'inflexion en démontrant la supériorité de l'apprentissage profond sur GPU pour l'extraction de caractéristiques. Cette percée a ouvert la voie à des architectures plus profondes et plus efficientes. Par exemple, l'architecture GoogLeNet \cite{szegedy_going_2015} factorise les convolutions pour réduire le coût de calcul tout en augmentant la largeur du réseau, permettant de traiter des motifs à différentes échelles simultanément.


\subsubsection{Mécanisme d'interaction : Filtrage local et expansion hiérarchique}
L'interaction fondamentale d'un réseau de convolution avec une séquence repose sur l'application répétée d'un opérateur de filtrage caractérisé par un noyau $w$ de support fini. Contrairement à une couche dense qui apprendrait un poids spécifique pour chaque élément de la séquence globale, la convolution impose une contrainte de partage des poids qui nécessite que les données soient structurées dans un espace métrique régulier. En effet, l'opération présuppose l'existence d'une fonction $p(\cdot)$ permettant d'associer à chaque élément $x$ sa position sur une grille sous-jacente, qu'elle soit unidimensionnelle pour des données temporelles ou multidimensionnelle pour des données spatiales.\\

Mathématiquement, le noyau $w$ est défini sur un support $\mathcal{V}$ centré à l'origine. Ainsi, le noyau définit pour tout élément cible $x_t$ un voisinage d'interaction $\mathcal{V}_t$, correspondant à la translation du support $\mathcal{V}$ en la position $p(x_t)$. L'opération de convolution consiste alors à calculer une somme pondérée des éléments appartenant à ce voisinage, où les poids sont déterminés exclusivement par la position relative entre les éléments et le centre. La sortie $h_t$ (avant activation) s'exprime par l'équation :
$$h_t = \sum_{x_j \in \mathcal{V}_t} w_{\Delta(x_j, x_t)} \cdot x_j + b$$
Dans cette expression, $\Delta(x_j, x_t) = p(x_j) - p(x_t)$ représente le vecteur de position relative du voisin $x_j$ par rapport au centre $x_t$, $b$ est un biais. Une conséquence directe de la structure en grille des données est que ce vecteur de différence correspond systématiquement à un n-uplet d'entiers ($n$ étant la dimension de la grille). Cette propriété discrète est fondamentale pour l'implémentation neuronale : elle implique que la fonction $w$ n'a pas besoin d'être modélisée comme une fonction continue, mais se réduit à un ensemble fini de paramètres scalaires (les poids du filtre) qu'il suffit d'apprendre pour chaque décalage entier possible dans le support. Cette formulation garantit l'invariance par translation, assurant que le même motif de poids est appliqué uniformément sur toute la structure. Par ailleurs, l'application de ce voisinage aux bornes d'une grille finie nécessite une gestion des effets de bord, typiquement résolue par l'ajout de valeurs nulles (zero-padding) en périphérie afin de conserver la dimension de la séquence traitée.\\

Ce mécanisme permet l'extraction robuste de motifs locaux, mais la compréhension de la structure globale de la séquence émerge de la composition hiérarchique de ces opérations. L'illustration \ref{convolution base} permet de visualiser comment l'empilement de couches induit une expansion mathématique du champ récepteur. Considérons une première couche définie par un filtre $w$ de taille 3. Pour un instant $t$, ce filtre induit un voisinage immédiat. L'équation locale est, en notant $\phi$ la fonction d'activation :
\begin{equation*}
\begin{split}
	h^{(1)}_t &= w_{1}x_{t-1} + w_{2}x_{t} + w_{3}x_{t+1} \\
	x^{(1)}_t &= \phi(h^{(1)}_t)
\end{split}
\end{equation*}
La sortie $x^{(1)}_t$ est une fonction des entrées $x_{t-1}$ à $x_{t+1}$.
Lorsqu'une seconde couche définie par un filtre $v$ de même support est appliquée sur cette représentation intermédiaire, elle opère selon le même principe d'invariance en translation :
\begin{equation*}
\begin{split}
	h^{(2)}_t & = v_{1}x^{(1)}_{t-1} + v_{2}x^{(1)}_{t} + v_{3}x^{(1)}_{t+1} \\
	x^{(2)}_t &= \phi(h^{(2)}_t)
\end{split}
\end{equation*}
La sortie $x^{(2)}_t$ est une fonction des entrées $x_{t-2}$ à $x_{t+2}$. Ainsi, par simple composition algébrique, l'horizon d'interaction s'est étendu de 3 éléments (couche 1) à 5 éléments (couche 2). La profondeur du réseau agit donc comme un multiplicateur mécanique de l'horizon d'interaction, permettant de reconstruire des dépendances causales à longue portée à partir de règles de construction strictement locales et invariantes.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.25]{BasicConvolution.png}
\end{center}
\caption{Illustration d'une convolution 1D standard et de l'expansion hiérarchique du champ récepteur}
\label{convolution base}
\end{figure}

Au-delà de l'expansion de l'horizon d'interaction, la géométrie du support de convolution détermine la nature causale ou non du traitement, une caractéristique nécessaire pour la modélisation de systèmes dynamiques. La partie gauche de la figure \ref{convolution comparaison} la configuration standard, dite convolution centrée. Pour calculer un élément de sortie $y_4$ à l'instant $t=4$, le champ récepteur effectif (cône violet) agrège les informations d'un voisinage symétrique de l'entrée $x$, de $x_2$ à $x_6$. Cette approche est naturelle pour l'analyse de données statiques (comme une image) ou le traitement de séquences complètes a posteriori, mais n'est pas adapté à la modélisation d'un système dynamique. Par exemple dans le cas de filtrage en ligne, le système ne peut physiquement pas accéder aux mesures futures pour débruiter les données du présent. Pour adapter l'architecture à ces contraintes temporelles strictes, on recourt à la convolution causale. Cette variante consiste à décaler le support du filtre de manière à ce que le voisinage d'interaction au temps $t$ ne contienne aucun indice supérieur à $t$. L'exemple illustré sur la partie droite de la figure \ref{convolution comparaison} assure que le cône d'influence de la sortie $y_5$ est alors strictement orienté vers le passé ($x_1$ à $x_5$). Dans cette configuration, le réseau conserve sa capacité d'extraction de motifs et de parallélisation, mais adopte une topologie d'interaction compatible avec la physique, simulant le comportement d'un système causal sans recourir à la mémoire récurrente.

\begin{figure}
\begin{center} 
\includegraphics[scale=0.45]{ComparaisonConvolution.png}
\end{center}
\caption{Impact de la topologie du support de convolution sur la causalité temporelle : approche centrée (gauche) et approche causale (droite)}
\label{convolution comparaison}
\end{figure}


\subsubsection{La convolution dans l'image : Une séquence spatiale 2D}
Dans le contexte de l'image, la séquence est bidimensionnelle et le CNN y opère une extraction hiérarchique. Les premières couches détectent des primitives simples comme des bords ou des textures, qui sont ensuite combinées pour former des motifs sémantiques complexes. Cette capacité d'abstraction a été poussée à son paroxysme par l'architecture VGG \cite{simonyan_very_2015}, qui a standardisé l'usage de filtres de très petite taille ($3\times3$) empilés en grande profondeur. Les auteurs ont démontré qu'une séquence de petites convolutions est plus efficace pour capturer des non-linéarités complexes qu'une seule grande convolution. Cependant, l'augmentation de la profondeur a engendré des problèmes de disparition du gradient, résolus avec ResNet \cite{he_deep_2016}. L'introduction de connexions résiduelles a permis d'entraîner des réseaux dépassant la centaine de couches, essentiels pour capturer les dépendances à très longue portée dans des images haute résolution.
Pour les tâches de "traduction" d'image vers image, cruciales en simulation (par exemple, passer d'une carte de densité à un champ de pression), il est impératif de ne pas perdre l'information spatiale lors de la compression. L'architecture U-Net \cite{navab_u-net_2015}, initialement pour la segmentation biomédicale combine un chemin de contraction et un chemin d'expansion reliés par des connexions latérales (skip connections). Cette structure permet de générer une sortie de même résolution que l'entrée en fusionnant le contexte sémantique global et les détails locaux. Cette architecture est aujourd'hui une référence pour les modèles de substitution en physique. D'autres variantes comme DenseNet \cite{iandola_densenet_2014} ont poussé cette logique plus loin en connectant chaque couche à toutes les suivantes pour maximiser le flux d'information, bien que cela se fasse au prix d'une consommation mémoire accrue.


\subsubsection{La convolution dans les séquences 1D (Texte, Audio, Séries Temporelles)}
Bien que souvent associés à l'image, les CNN se sont révélés extrêmement performants pour traiter des séquences unidimensionnelles, surpassant parfois les réseaux récurrents grâce à leur capacité de parallélisation. Dans le traitement du signal audio, l'architecture WaveNet \cite{van_den_oord_wavenet_2016} a marqué une rupture en utilisant des convolutions causales dilatées. Ce mécanisme permet au champ récepteur du réseau de croître exponentiellement avec la profondeur sans augmenter le nombre de paramètres, capturant ainsi des dépendances temporelles sur des milliers de pas de temps, ce qui est impossible pour un RNN standard \textcolor{red}{REF}.
Dans le domaine du traitement du langage naturel (NLP), cette logique a été appliquée avec succès à la traduction automatique. L'architecture ConvS2S \cite{gehring_convolutional_2017} entièrement convolutionnelle pour la séquence à séquence, utilise des mécanismes d'attention multi-pas pour pondérer l'importance des mots sources. De même, ByteNet \cite{kalchbrenner_neural_2017}, réalise la traduction en temps linéaire en empilant des convolutions dilatées. Ces travaux ont démontré que l'induction de biais locaux propres aux convolutions est pertinente pour la syntaxe et la sémantique locale.
Cette approche a été généralisée aux séries temporelles génériques sous le nom de Temporal Convolutional Networks (TCN) \cite{bai_empirical_2018}. L'étude comparative démontre que sur une vaste gamme de tâches séquentielles, comme la prédiction de charge énergétique ou la modélisation de séquences symboliques, les TCN surpassent souvent les réseaux récurrents (LSTM/GRU) \textcolor{red}{REF} tout en offrant une stabilité d'entraînement supérieure. Une étude récente \cite{tay_are_2022} prolonge ce constat en suggérant que des architectures convolutionnelles modernes pré-entraînées peuvent rivaliser avec les Transformers sur certaines tâches textuelles, soulignant la pertinence continue de ce paradigme.

\subsubsection{Généralisation : De la grille volumétrique aux topologies irrégulières}
Le principe de convolution, initialement restreint aux images planes, a fait l'objet d'extensions successives pour traiter des structures de données de plus en plus complexes. Une première étape de généralisation concerne les données volumétriques (3D) et spatio-temporelles, qui conservent toutefois une structure de grille régulière (euclidienne). Pour l'analyse de vidéos, C3D \cite{tran_learning_2015} déploie des filtres tridimensionnels ($x, y, t$) capturant conjointement l'espace et le mouvement. De même, dans le domaine médical, l'architecture V-Net \cite{milletari_v-net_2016} étend le principe du U-Net à la 3D en opérant sur des voxels. Bien que performantes, ces méthodes restent contraintes par la régularité de la grille : elles imposent une voxelisation des données qui induit une complexité cubique et une perte de résolution, les rendant inadaptées aux géométries éparses.

La généralisation la plus significative pour la simulation scientifique concerne donc les données non-euclidiennes, intrinsèquement irrégulières. Dans une simulation lagrangienne ou un système moléculaire, le "voisinage" n'est plus défini par une position dans une matrice, mais par la topologie d'un graphe. Les Graph Convolutional Networks (GCN) \cite{kipf_semi-supervised_2016} redéfinissent alors la convolution comme une agrégation spectrale ou spatiale des caractéristiques des nœuds connectés, indépendamment de leur disposition géométrique absolue. Cette approche a été enrichie par GraphSAGE \cite{hamilton_inductive_2017}, proposant une convolution inductive capable de généraliser à des nœuds invisibles durant l'entraînement. Enfin, pour traiter des nuages de points 3D bruts sans passer par la case voxelisation, des architectures comme PointNet++ \cite{qi_pointnet_2017} appliquent des opérations hiérarchiques directement sur des ensembles continus, comblant le fossé entre convolution discrète et géométrie continue.


\subsection{Réseaux de neurones récurrents et Espaces d'Etats (RNN et SSM)}
Si les réseaux de convolution abordent la séquence par une fenêtre glissante locale, une autre famille d'architectures adopte une approche intrinsèquement temporelle : la modélisation récursive. Qu'il s'agisse des Réseaux de Neurones Récurrents (RNN) historiques ou des récents Modèles d'Espaces d'États (SSM), le principe fondateur reste la persistance de l'information. Le modèle maintient un état caché interne $h_t$ qui agit comme une mémoire compressée de tout l'historique passé, mise à jour à chaque nouvelle observation. Cette formulation est particulièrement naturelle pour la simulation physique, car elle mime la dynamique des systèmes causaux où l'état futur dépend de l'état présent et des forces appliquées.

\subsubsection{Genèse et mécanismes d'interaction : De la boucle simple aux portes logiques}
L'histoire de cette approche débute avec les RNN classiques \cite{elman_finding_1990} qui introduisent une boucle de rétroaction permettant au réseau de maintenir une trace du contexte temporel. Cependant, bien que ces réseaux parviennent à générer des séquences continues complexes comme de l'écriture manuscrite, ils souffrent d'une instabilité critique lors de l'entraînement : le problème de la disparition ou de l'explosion du gradient \cite{graves_generating_2014}. Sur de longues séquences, le signal d'erreur se dilue, empêchant l'apprentissage des causes lointaines d'un événement. Pour y remédier, le LSTM (Long Short-Term Memory) \cite{hochreiter_long_1997} propose des "cellules" mémoires protégées par des portes logiques, et peut choisir de retenir ou d'effacer une information sur des milliers de pas de temps. Cette capacité a été affinée par l'introduction du GRU \cite{cho_properties_2014}, \cite{chung_empirical_2014}, une variante plus économe.


\subsubsection{Mécanisme d'interaction : Récurrence et Mémoire d'État}
L'interaction fondamentale des architectures récurrentes (RNN) et des modèles d'espaces d'états (SSM) avec la séquence repose sur un principe de persistance de l'information, radicalement différent de la localité spatiale des convolutions. Au lieu d'agréger un voisinage statique, ces modèles introduisent une variable latente dynamique, l'état caché $h_t$, qui agit comme une mémoire compressée de l'historique causal. Mathématiquement, la modélisation d'une telle trajectoire dynamique se formalise comme un problème de valeur initiale. Elle nécessite impérativement de définir une condition d'ancrage $h_0$ avant de décrire la loi d'évolution $f$. Pour une séquence d'entrée $x$, le système est ainsi entièrement décrit par le couple d'équations suivant :
$$\begin{cases} h_0 = \lambda \\ h_t = f(h_{t-1}, x_t; \theta) \quad \forall t \ge 1 \end{cases}$$
La première équation initialise l'état de la mémoire avant toute observation. Si la convention standard pose un vecteur nul ($\lambda = \vec{0}$), supposant une mémoire vierge, l'apprentissage machine laisse la possibilité au réseau d'optimiser l'encodage de cette mémoire vierge sous la forme d'un paramètre appris $\lambda$. La seconde équation décrit la mise à jour récursive où $f$ est une fonction de transition paramétrée par $\theta$. Cette formulation implique que $h_t$ ne dépend pas seulement de l'entrée locale $x_t$, mais indirectement de toute la trajectoire passée $\{x_0, \dots, x_t\}$ accumulée dans $h_{t-1}$.\\

En pratique, cette dynamique de mise à jour est régie par un ensemble de paramètres apprenables qui sont partagés sur des parties de la séquence, garantissant l'invariance du traitement par tâche conceptuelle (genre encodage d'une phrase puis autre matrice pour décodage et traduction). Historiquement, les premières architectures (genre RNN avant LSTM et GRU etc) encodaient simplement ces paramètres sous la forme d'une matrice $W_{ih}$ (Input-to-Hidden) qui projette l'entrée courante dans l'espace latent, d'une matrice $W_{hh}$ (Hidden-to-Hidden) qui contrôle l'évolution de la mémoire interne, et d'un vecteur de biais $b$. En notant $\phi$ la fonction d'activation non-linéaire, l'équation de propagation de récurrence s'écrit pour notre exemple : \\
$$h_{t+1} = \phi(W_{ih}x_t + W_{hh}h_t + b)$$

L'illustration \ref{RNN base} permet de visualiser la propagation de la dépendance temporelle à travers une architecture récurrente élémentaire, souvent qualifiée de RNN d'Elman \cite{elman_finding_1990}. Dans ce modèle simplifié (ici représenté sans biais pour la clarté), les informations sont transportées d'état caché en état caché à travers la matrice de poids $W_1$ et d'entrée à l'état caché à travers $W_2$. On illustre en bleu le cône de perception de l'état caché $h_3$ (représenté par le carré vert) intégrant par récurrence les informations des observations passées $x_1$, $x_2$ et $x_3$, créant un lien causal ininterrompu.
\begin{equation*}
\begin{split}
	h_3 & = \phi(W_{2}x_3 + W_{1}h_2) \\
	&= \phi(W_{2}x_3 + W_{1}\phi(W_{2}x_2 + W_{1}h_1))\\
	&= \phi(W_{2}x_2 + W_{1}\phi(W_{2}x_2 + W_{1}\phi(W_{2}x_1 + W_{1}\lambda)))
\end{split}
\end{equation*}
Cette formulation met en évidence la différence fondamentale avec la convolution : alors que le champ récepteur d'un CNN s'élargit progressivement par empilement de couches, ici le cône d'influence (en bleu) s'étend horizontalement vers le passé jusqu'à la première entrée, capturant théoriquement la totalité de l'historique causal disponible.\\

\begin{figure}
\begin{center} 
\includegraphics[scale=0.35]{BasicRNN.png}
\end{center}
\caption{Mécanisme fondamental de la récurrence et propagation de l'état mémoire}
\label{RNN base}
\end{figure}

En fonction de la fonction accomplie par l'architecture récurrente, les états cachés sont exploités différemment. Le mode "Flux à Flux" (ou Many-to-Many synchronisé), illustré à gauche dans la figure \ref{RNN comparaison}, aligne la production de la sortie sur la réception de l'entrée. À chaque pas de temps $t$, l'état caché $h_t$ est utilisé immédiatement pour prédire une sortie $y_t$. Cette configuration, où la causalité est stricte et le délai minimal, est caractéristique des systèmes de filtrage en ligne ou de contrôle, où la réaction doit être instantanée. Le mode "Séquence vers Séquence" (Seq2Seq), à droite dans la figure \ref{RNN comparaison}, nécessite d'opérer en deux phases distinctes. La phase d'encodage représenté en violet, ingurgite toutes les informations et les condenses dans l'état caché $h_5$. Les états cachés $h_1$ à $h_5$ ne sont utilisés que pour encoder l'information du passé, et ne sont exploités que pour transmettre cette information à l'avenir. La phase de décodage récupère ce contexte passé pour générer la séquence de sortie. Les états cachés de $h_6$ à $h_8$ sont à la fois utilisés pour transmettre et renseigner l'information de mémoire mais aussi pour prédire les nouveaux éléments, qui sont réinjectés en entrée (boucle autorégressive). Ce mécanisme permet de transformer une séquence en une autre de longueur différente et de modéliser des dépendances non-monotones, mais impose que toute l'information pertinente soit compressée dans un goulot d'étranglement. C'est cette distinction topologique, plus que la nature des données, qui différencie fondamentalement l'usage des RNN pour le suivi de signal (mode flux) de leur usage pour la traduction (mode Seq2Seq).

\begin{figure}
\begin{center} 
\includegraphics[scale=0.4]{ComparaisonRNN.png}
\end{center}
\caption{Topologies d'application des architectures récurrentes : traitement de flux (gauche) et traduction globale (droite)}
\label{RNN comparaison}
\end{figure}

\subsubsection{Renouveau architectural : Les Modèles d'Espaces d'Etats (SSM)}
Malgré leur robustesse, les LSTM conservent une limitation structurelle majeure : leur traitement séquentiel interdit la parallélisation sur GPU. C'est pour lever ce verrou qu'une nouvelle classe de modèles a émergé : les State Space Models (SSM). Ces modèles puisent leur origine théorique dans le papier HiPPO \cite{gu_hippo_2020}, qui formalise mathématiquement comment compresser optimalement une histoire continue dans un vecteur de taille fixe via des projections polynomiales orthogonales. Cette base a permis de développer S4 (Structured State Space sequence model) \cite{gu_efficiently_2022}, capable de modéliser des dépendances sur plus de 10 000 pas de temps en résolvant une équation différentielle continue discrétisée.
Cependant, les premiers SSM souffraient d'une rigidité dynamique, peinant à sélectionner l'information pertinente en fonction du contexte ("Content-based selection"). Cette limite a été adressée par l'architecture Mamba \cite{gu_mamba_2024}. En rendant les matrices d'état dépendantes de l'entrée, Mamba atteint des performances comparables aux premiers Transformers \textcolor{red}{REF} tout en conservant une complexité linéaire. Toutefois, ces modèles restent délicats à stabiliser sur des dynamiques hautement chaotiques où la discrétisation numérique peut introduire des dérives.

\subsubsection{Application au texte : L'ère du Sequence-to-Sequence et de la Traduction}
Dans le traitement du langage, l'approche récurrente a connu son apogée avec le paradigme Seq2Seq \cite{sutskever_sequence_2014}. En utilisant deux LSTM (un encodeur et un décodeur), cette architecture a permis de traiter des séquences de longueurs variables. Cette avancée a transformé l'industrie de la traduction avec le déploiement du Google’s Neural Machine Translation System (GNMT) \cite{wu_googles_2016} en 2016, réduisant les erreurs de traduction de près de 60 \% par rapport aux systèmes statistiques. Au-delà de la traduction, ce paradigme a permis des avancées dans la modélisation prédictive de parcours complexes, comme l'illustré par le modèle Doctor AI \cite{choi_doctor_2016}. Ce modèle utilise des RNN pour prédire les futurs diagnostics médicaux et la durée avant la prochaine visite à partir de l'historique clinique des patients, démontrant la capacité des RNN à capturer des dynamiques temporelles irrégulières et multivariées dans des données réelles bruitées.

\subsubsection{Application aux systèmes temporels, physiques et créatifs}
Au-delà du texte, les RNN se sont imposés comme l'outil naturel pour la modélisation de systèmes dynamiques continus, un domaine crucial pour la simulation. Une étude sur la prévision de systèmes chaotiques \cite{vlachas_data-driven_2018} a mis en avant que les LSTM pouvaient apprendre la dynamique de l'attracteur de Lorenz ou de l'équation de Kuramoto-Sivashinsky mieux que les modèles physiques simplifiés, en capturant les propriétés non-linéaires de l'évolution temporelle à court terme. Cette capacité à modéliser le chaos déterministe fait des RNN des candidats sérieux pour accélérer les simulations de mécanique des fluides turbulents.
Dans l'industrie, cette robustesse est exploitée pour la prévision probabiliste avec DeepAR \cite{salinas_deepar_2020}, utilisé par Amazon pour sa chaîne logistique. Ce modèle apprend une distribution de probabilité future à chaque pas de temps, permettant de quantifier l'incertitude via des simulations de Monte Carlo. Enfin, la capacité "générative constructive" des RNN a été pionnière dans la création artistique. Le modèle Performance RNN \cite{oore_this_2020}, développé par Google Magenta, a montré qu'un LSTM pouvait générer des performances de piano expressives (avec nuances de vélocité et de timing) en traitant la musique non pas comme une partition rigide, mais comme une séquence temporelle continue d'événements, prouvant que les RNN peuvent capturer des structures hiérarchiques globales (phrasé musical) tout en gérant des détails micro-temporels.


\subsection{L'architecture Transformer}
Si les réseaux récurrents ont introduit la mémoire et les réseaux convolutionnels la localité, l'architecture Transformer a proposé un changement de paradigme radical en postulant que l'interaction entre les éléments d'une séquence doit être modélisée par une relation directe de contenu à contenu, et non par une contrainte de proximité spatiale ou temporelle. Cette architecture, devenue l'épine dorsale de l'IA générative moderne, repose sur le mécanisme d'attention \cite{vaswani_attention_2017}.

\subsubsection{Genèse : Du goulot d'étranglement récurrent à l'Attention pure}
L'émergence du Transformer est le fruit d'une lente maturation visant à résoudre le goulot d'étranglement des architectures Encodeur-Décodeur récurrentes (RNN). Dans le paradigme Seq2Seq classique \cite{sutskever_sequence_2014}, toute l'information de la phrase source devait être compressée dans un unique vecteur de contexte de taille fixe, entraînant une perte d'information critique sur les longues séquences. Une première solution \cite{bahdanau_neural_2014} introduit un mécanisme d'attention additive permettant au décodeur de "chercher" (search) et d'aligner (align) les parties pertinentes de la phrase source à chaque étape de la génération. Ici, l'attention n'était encore qu'un module auxiliaire greffé sur des RNN. Une seconde étape conceptuelle fut franchie avec les Pointer Networks \cite{vinyals_pointer_2015} où le réseau de neurones apprend à résoudre des problèmes combinatoires en utilisant l'attention comme un pointeur pour sélectionner des éléments de l'entrée comme sortie. Cela a ancré l'idée que le mécanisme de sélection basé sur le contenu ("Content-based addressing") était suffisamment puissant pour structurer la sortie. La rupture définitive survient avec l'article Attention Is All You Need \cite{vaswani_attention_2017}. Les auteurs ont démontré que la récurrence, jugée jusqu'alors indispensable pour encoder l'ordre séquentiel, était en réalité superflue et limitante pour la parallélisation. Ne conserver que l'attention a permis un traitement massivement parallèle et une réduction le chemin maximal de propagation de l'information entre deux mots quelconques à une constante $O(1)$, contre $O(N)$ pour un RNN. Cette réduction drastique de la distance topologique facilite considérablement le flux de gradient et l'apprentissage des dépendances à long terme.\\

Contrairement aux CNN ou RNN qui se définissent par une opération élémentaire, le Transformer se définit d'abord comme une architecture systémique modulaire. Son fonctionnement repose sur le concept de "Scaled Dot-Product Attention", que l'on peut appréhender par une analogie avec la recherche d'information. Chaque élément de la séquence émet une Requête ($Query, Q$), une Clé ($Key, K$) et une Valeur ($Value, V$). Le mécanisme calcule la pertinence entre la requête d'un élément et les clés de tous les autres pour déterminer un poids d'attention. Ce poids pondère ensuite l'agrégation des valeurs, permettant de synthétiser un nouveau vecteur de contexte. Nous décrivons l’architecture en commençant par le flux macroscopique de l'information, avant d'analyser la composition interne et spécifiquement le mécanisme d’attention.
\subsubsection{Architecture Macroscopique : Le paradigme Encodeur-Décodeur}
Dans sa configuration canonique pour la traduction automatique \cite{vaswani_attention_2017}, l'architecture adopte une structure bipartite illustrée par la figure \ref{simple transformer}. Son objectif est de transformer une séquence source $(x_i)_{1 \le i \le N}$ en une séquence cible $(y_i)_{1 \le i \le M}$. Pour la clarté de l'analyse, nous formalisons ici les représentations intermédiaires : nous noterons $(h_i)_{1 \le i \le N}$ la séquence latente produite par l'encodeur, et $(z_i)_{1 \le i \le M}$ la séquence de vecteurs de sortie du décodeur. Par convention, chaque vecteur $z_k$ est la représentation destinée à être projetée pour prédire le symbole cible $y_k$. Cette architecture se justifie par deux apports majeurs : l'enrichissement sémantique via l'attention et l'accélération de l'apprentissage. En effet, contrairement aux réseaux récurrents (RNN) contraints à un traitement itératif, le Transformer permet de calculer l'intégralité de la séquence de prédiction $(z_i)_i$ en une unique passe parallèle.\\

Le flux de traitement débute par l'encodeur qui traite la source $(x_i)_i$. Grâce au mécanisme d'auto-attention, chaque élément agrège l'information du contexte global pour produire la séquence latente $(h_i)_i$ à haute teneur sémantique. Le décodeur exploite ensuite cette représentation $(h_i)_i$ pour générer la séquence $(z_i)_i$. C'est ici qu'intervient la distinction critique entre entraînement et inférence. Pour garantir que le modèle puisse générer des phrases mot après mot en inférence (où $y_k$ n'est pas encore connu), l'entraînement doit simuler cette causalité tout en restant parallèle. On applique donc un décalage des entrées : pour produire le vecteur de prédiction $z_k$ (visant le symbole $y_k$), le décodeur reçoit en entrée les symboles précédents $(S, y_1, \dots, y_{k-1})$. Ce décalage aligne mécaniquement l'information passée face à l'objectif de prédiction présent $z_k$.\\

Ce principe est illustré par la figure \ref{simple transformer} à travers le cône de perception du vecteur $z_3$. Puisque $z_3$ a pour fonction de prédire $y_3$, il a accès via l'attention croisée à la totalité de la source $(x_i)_i$, mais son accès à la séquence cible est strictement contraint aux antécédents $S, y_1$ et $y_2$ via le masquage. Ainsi, chaque vecteur $z_k$ est construit en intégrant tout le contexte source $(h_i)_{1 \le i \le N}$ et le passé cible $(y_i)_{1 \le i \le k-1}$, sans jamais violer la causalité temporelle en accédant à $y_k$ ou à ses successeurs. Une fois la séquence $(z_i)_i$ calculée, une simple projection linéaire suivie d'un Softmax permet d'obtenir les probabilités de chaque symbole $y_k$.\\
\begin{figure}
\begin{center}
\includegraphics[scale=0.55]{simple_transformer.png}
\end{center}
\caption{Flux d'information macroscopique dans l'architecture Transformer Encodeur-Décodeur}
\label{simple transformer}
\end{figure}

Cette structure macroscopique ne décrit pas un simple passage de couches, mais un empilement profond. Comme le montre la figure \ref{full transformer}, l'Encodeur et le Décodeur sont constitués respectivement d'une pile de $N$ blocs identiques. C'est la répétition de ces blocs qui confère au modèle sa capacité d'abstraction. Entrons dans le détail de ces deux blocs.\\
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{full_transformer.png}
\end{center}
\caption{Architecture Transformer complète : empilement des blocs}
\label{full transformer}
\end{figure}

\subsubsection{L'Encodeur}
L'illustration \ref{encoder transformer} détaille l'architecture interne d'un bloc d'encodage. Chaque bloc est conçu pour transformer les représentations vectorielles entrantes en une version plus contextualisée. Il s'articule autour de deux sous-modules séquentiels. Le premier est l'Attention Multi-Têtes, où les rôles de Requête, Clé et Valeur ($Q, K, V$) sont tous issus de la sortie du bloc précédent. Cela permet à chaque position de capturer des relations avec toutes les autres positions de la séquence. Le second est un réseau de neurones dense (Feed-Forward Network - FFN) appliqué indépendamment à chaque position. Pour permettre l'entraînement de réseaux profonds, chaque sous-module est encapsulé par une connexion résiduelle \cite{he_deep_2016} suivie d'une normalisation de couche (Layer Norm) \cite{ba_layer_2016}, assurant stabilité et une bonne propagation du gradient dans toutes les couches. Enfin, l'architecture étant invariante par permutation, l'ajout dès l'entrée d'un Encodage Positionnel est indispensable pour injecter la topologie temporelle ou spatiale dans les représentations vectorielles.\\
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{enc_transformer.png}
\end{center}
\caption{Architecture interne du bloc Encodeur}
\label{encoder transformer}
\end{figure}

\subsubsection{Le Décodeur}
L'architecture du Décodeur, présentée en figure \ref{decoder transformer}, adapte la structure de l'encodeur aux contraintes de la génération. Chaque bloc intègre trois sous-modules au lieu de deux. Le premier est une Attention Multi-Têtes Masquée. Ce masquage est l'implémentation technique de la causalité : il force les poids d'attention à zéro pour toutes les positions futures, interdisant au modèle de d’accéder aux mots $(y_i)_{k \le i \le M}$ lors du calcul de la représentation de $z_k$. Le second module est l'Attention Croisé : les Requêtes ($Q$) proviennent de la chaine de décodeur (ce que l'on cherche à enrichir), tandis que les Clés ($K$) et Valeurs ($V$) proviennent du contexte source $(h_i)_i$. C'est ce pont qui permet à la génération d'être conditionnée par la source. Enfin, on retrouve le réseau FFN classique. Comme pour l'encodeur, l'emploi systématique de connexions résiduelles et de normalisation de couche assure la stabilité du gradient à travers la profondeur du réseau.\\
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{dec_transformer.png}
\end{center}
\caption{Architecture interne du bloc Décodeur}
\label{decoder transformer}
\end{figure}

\subsubsection{Micro-architecture : Le mécanisme d'Attention et ses variantes}
Au cœur de ces blocs macroscopiques réside le véritable moteur de l'interaction : le mécanisme d'Attention. Contrairement aux architectures précédentes qui traitent la séquence par voisinage spatial ou récursivité temporelle, le Transformer repose sur un mécanisme d'interaction directe et globale. Il est basé sur un principe de recherche d’information entre une séquence de requête $\mathcal{Q} = (q_i)_{i \in \mathbf{I}}$ et un séquence valeur $\mathcal{V} = (v_j)_{j \in \mathbb{J}}$ caractérisée par une séquence de clé $\mathcal{K} = (k_j)_{j \in \mathbb{J}}$. Les ensembles d'indiciations de ces séquences $\mathbb{I}$ et $\mathbb{J}$ ne sont pas nécessairement identiques. On notera aussi par commodité $\mathcal{KV} = ((k_j, v_j))_{j \in \mathbb{J}}$.\\

En pratique, n’importe quelle séquence $(x_i)_{i \in \mathbb{X}}$, indicé par $\mathbb{X}$, peut être projetée dans un espace de représentation de requêtes $\mathcal{Q} = (Q(x_i))_{i \in \mathbb{X}}$, ou/et les espaces de représentation de clés et valeurs $\mathcal{K} = (K(x_i))_{i \in \mathbb{X}}$ et $\mathcal{V} = (V(x_i))_{i \in \mathbb{X}}$.
Pour toute requête donnée $q \in \mathcal{Q}$ et toute clé candidate $k \in \mathcal{K}$, le score d'attention défini par le produit scalaire normalisé mesure la pertinence de l'association :
\begin{equation}
a(q, k) = \frac{<q,k>}{\sqrt{d_{att}}}
\label{attentionscore}
\end{equation}

À requête $q$ fixée, la séquence de score d'attention $(a(q,k))_{k \in \mathcal{K}}$ est transformée par une fonction \textit{softmax} créant une pondération normalisée :
\begin{equation}
p(q,k, \mathcal{K}) = \frac{\exp(a(q, k))}{\sum_{k’ \in \mathcal{K}}\exp(a(q,k’))}
\label{pondatt}
\end{equation}
L'opérateur d'attention, noté $\mathsf{A}$, agrège alors l'information en calculant la somme des valeurs $\mathcal{V}$ pondérées par ces poids :
\begin{equation}
\mathsf{A}(q, \mathcal{KV}) = \sum_{(k, v) \in \mathcal{KV}} p(q,k, \mathcal{K}) v
\label{attentionoperator}
\end{equation}

Cette formulation mathématique traduit le concept intuitif : pour un couple $(k,v) \in \mathcal{KV}$, plus le score $a(q,k)$ est élevé, plus la contribution de la valeur $v$ est grande dans la représentation contextuelle de la requête $\mathsf{A}(q, \mathcal{KV})$.\\

Le facteur déterminant pour l'expressivité du mécanisme est l'aspect multi-tête : au lieu d’avoir un triplet de fonctions de projection $Q,K,V$ nous disposons de $h$ triplets $(Q_l,K_l,V_l)_{1 \le l \le h}$. Cette diversité permet la capture d'une plus grande variété d'interaction en projetant les séquences dans $h$ espaces de comparaisons différents.
N'importe quelle séquence traitée $(x_i)_{i \in \mathbb{X}}$, sera alors projetée simultanément dans $h$ espaces de représentation de requêtes
\begin{equation}
\mathcal{Q}_l =(Q_l(x_i))_{i \in \mathbb{X}}, \:\: 1 \le l \le h
\label{queryeq}
\end{equation}
et/ou dans $h$ espaces de représentation des clés et valeurs
\begin{equation}
\mathcal{K}_l = (K_l(x_i))_{i \in \mathbb{X}},\:\: \mathcal{V}_l  = (V_l(x_i))_{i \in \mathbb{X}},\:\: 1 \le l \le h
\end{equation}

En pratique, on dispose donc de $h$ triplets de séquences $(\mathcal{Q}_l,\mathcal{K}_l,\mathcal{V}_l)_{1 \le l \le h}$.
Le calcul de chaque tête d'attention est réalisé en appliquant, pour $l$ variant de $1$ à $h$, l'opérateur d'attention \ref{attentionoperator} aux séquences $\mathcal{Q}_l$, $\mathcal{K}_l$ et $\mathcal{V}_l$.
$$ \mathsf{A}(q, \mathcal{KV}_l) = \sum_{(k, v) \in \mathcal{KV}_l} \frac{\exp(a(q, k))}{\sum_{k’ \in \mathcal{K}_l}\exp(a(q,k’))} v $$

Enfin, les sorties des $h$ têtes sont concaténées (opérateur $\oplus$) pour former un vecteur de dimension $h \times d_{att}$. Ce vecteur est reprojeté par une fonction $O$, définissant ainsi l'opérateur d'attention multi-tête ($\mathsf{MHA}$ – Multi-Head Attention) applicable à tout élément $x$ de la séquence formant les requêtes \ref{queryeq}.
\begin{equation}
\mathsf{MHA}(x) = O( A(Q_1(x), \mathcal{KV}_1) \oplus A(Q_2(x), \mathcal{KV}_2) \oplus \dots \oplus A(Q_h(x), \mathcal{KV}_h))
\label{MHAeq}
\end{equation}
\: \\

Cette formulation générique connaît plusieurs variantes fondamentales qui se distinguent par la nature des interactions qu'elles sont sensées capturer.\\

Premièrement, lorsque l'objectif est d'enrichir une séquence cible $(x_i)_{i \in \mathbb{I}}$ par le contexte d'une séquence source distincte $(y_j)_{j \in \mathbb{J}}$, on assigne les rôles de manière asymétrique. Pour toute tête $l \in \{1, \dots, h\}$, l'ensemble des requêtes est généré par la cible, soit $\mathcal{Q}_l = \{Q_l(x_i)\}_{i \in \mathbb{I}}$, tandis que l'ensemble des paires clé-valeur est généré par la source, soit $\mathcal{KV}_l = \{(K_l(y_j), V_l(y_j))\}_{j \in \mathbb{J}}$. La sortie est calculé en appliquant directement l'opération $\mathsf{MHA}$ \ref{MHAeq} à ces séquences de requêtes, clés et valeur, définissant ainsi l'opérateur d'attention croisée multi-tête ($\mathsf{MHCA}$ - Multi-Head Cross-Attention). Une schématisation de ce mécanisme sur une unique tête est représenté dans la figure \ref{fig:cross_att}. \\

\begin{figure}[htbp]
\begin{center} 
\includegraphics[scale=0.4]{cross_attention.png}
\end{center}
\caption{Illustration de l'Attention Croisée ($\mathsf{MHCA}$). Les requêtes (en bleu) proviennent de la séquence cible et interrogent l'intégralité du contexte source (clés en jaune). À gauche, la requête $q_3$ agrège l'information de la séquence source (à l'origine des clés et valeurs $\mathcal{KV})$) pour construire $A(q_3,\mathcal{KV})) $, représenté dans la cellule verte. Le cône d'interaction est représenté en dégradé de bleu.  À droite, le même calcul est effectué indépendamment pour la requête $q_5$. Cette indépendance des calculs permet leur parallélisation.}
\label{fig:cross_att}
\end{figure}

Deuxièmement, lorsque l'objectif est d'enrichir une séquence $(x_i)_{i \in \mathbb{I}}$ par son propre contexte, la même séquence est utilisée pour tous les rôles. Pour toute tête $l$, on définit $\mathcal{Q}_l = \{Q_l(x_i)\}_{i \in \mathbb{I}}$ et $\mathcal{KV}_l = \{(K_l(x_i), V_l(x_i))\}_{i \in \mathbb{I}}$. L'application de l'opérateur $\mathsf{MHA}$ \ref{MHAeq} à ces séquences de requêtes, clés et valeurs définit alors l'opérateur d'auto-attention multi-tête ($\mathsf{MHSA}$ - Multi-Head Self-Attention). Une schématisation de ce mécanisme sur une unique tête est représenté à gauche de la figure \ref{fig:compare_att}. \\

Enfin, la génération autorégressive impose un respect strict de la causalité. Dans l'objectif est d'enrichir une séquence $(x_i)_{i \in \mathbb{I}}$ par son propre contexte de manière causale, les rôles sont encore assignés de manière symétrique, mais le calcul du score d’attention est modifié.
Soient $x_i$ et $x_j$ deux éléments de la séquence, générant respectivement une requête $q_i$ et une clé $k_j$ dans la tête $l$. Le score d'attention est redéfini ainsi:
\begin{align}
a(q_i, k_j) = &\frac{\langle q_i, k_j \rangle}{\sqrt{d_{att}}} &\text{si } j \le i \\
& -\infty &\text{si } j > i
\label{maskedattention}
\end{align}
\: \\

Cette condition assure que pour tout $j > i$, le poids $p$ associé est nul, empêchant toute fuite d'information du futur vers le passé.
L'attention masquée \ref{maskedattention} est réinjecté dans le calcul de pondération \ref{pondatt} et l'agrégation suit ensuite la mécanique standard jusqu'à l'équation \ref{MHAeq}, définissant ainsi l'opérateur d'auto-attention masquée multi-tête ($\mathsf{MMHSA}$ - Masked Multi-Head Self-Attention). Une schématisation de ce mécanisme sur une unique tête est représenté à droite de la figure \ref{fig:compare_att}. 

\begin{figure}[htbp]
\begin{center} 
\includegraphics[scale=0.4]{self_attention.png}
\end{center}
\caption{Comparaison des modes d'Auto-Attention. La séquence d'entré nourrit à la fois $\mathcal{Q}$ et $\mathcal{KV}$. À gauche, l'interaction est globale. La requête $q_4$ agrège l'information de toute la séquence pour construire  $A(q_4,\mathcal{KV}))$, représenté dans la cellule verte. Le cône d'interaction est représenté en dégradé de bleu. À droite, l'interaction est causale. L'accès aux clés futures est donc bloqué (score $-\infty$), cette restriction est visible sur le cône d'interaction en nuance de bleu. La requête $q_4$ agrège aux positions $j \le 4$ pour construire $A(q_4,\mathcal{KV}))$, qui constitue une prédiction des états futurs.  Pour les deux images, on représente en arrière plan les même calculs pour la requête $q_6$, réalisés de manière indépendante.}
\label{fig:compare_att}
\end{figure}



\subsubsection{Le Transformer dans le texte : La divergence des architectures}
Dans le traitement du langage naturel (NLP), le Transformer a provoqué une véritable explosion cambrienne des modèles, se scindant en trois familles distinctes.
La première, celle des Encodeurs, est incarnée par BERT \cite{devlin_bert_2019}. Utilisant une attention bidirectionnelle, ces modèles excellent dans la compréhension et la classification, car chaque mot a accès au contexte passé et futur simultanément.
La seconde, celle des Décodeurs, est dominée par la lignée GPT \cite{radford_improving_2018}, \cite{brown_language_2020}. Ici, l'attention est causale (masquée vers le futur), optimisée pour la génération autorégressive. C'est cette branche qui a mis en évidence les "lois d'échelle" (Scaling Laws), montrant que la performance de prédiction du prochain token suit une loi de puissance en fonction du nombre de paramètres et de données, ouvrant la voie aux gros modèles de langage (Large Language Model - LLM) actuels.
La troisième famille, Encodeur-Décodeur, reste fidèle à l'architecture originale \cite{vaswani_attention_2017} pour les tâches de traduction ou de résumé. Le modèle T5 \cite{raffel_exploring_2020} a poussé ce paradigme à son extrême en reformulant toute tâche NLP (y compris la classification) comme un problème de génération de texte-vers-texte.

\subsubsection{Le Transformer dans les systèmes temporels : Promesses et controverses}
L'application des Transformers aux séries temporelles continues (consommation énergétique, trafic, météo) a fait l'objet de recherches intenses \cite{wen_transformers_2023}. L'attrait principal réside dans la capacité théorique de l'attention à capturer des corrélations à très long terme et des saisonnalités complexes que les RNN peinent à retenir.
Des architectures spécifiques ont été proposées pour briser la complexité quadratique. Informer  \cite{zhou_informer_2021} introduit une attention "ProbSparse" pour sélectionner uniquement les interactions dominantes, réduisant la complexité à $O(N \log N)$. Autoformer \cite{wu_autoformer_2021} va plus loin en remplaçant le produit scalaire par une auto-corrélation pour mieux capturer les périodicités.
Cependant, l'efficacité réelle des Transformers sur des signaux continus est contestée. Une étude \cite{zeng_are_2023} assure qu'un simple modèle linéaire bien calibré (DLinear) surpassait souvent des Transformers complexes sur les benchmarks standards. La raison invoquée est que l'attention, conçue pour la sémantique discrète, tend à sur-interpréter le bruit dans les signaux continus et perd l'information d'ordre temporel cruciale, malgré les encodages positionnels. Néanmoins, des approches récentes comme PatchTST \cite{nie_time_2023}, qui segmentent le signal en patchs (comme en vision) avant d'appliquer l'attention, semblent redonner l'avantage aux Transformers en traitant des dynamiques locales plutôt que des points isolés.

\subsubsection{Le Transformer dans l'image : Patchs et hiérarchie}
L'hégémonie des CNN en vision a été remise en cause par le Vision Transformer (ViT) \cite{dosovitskiy_image_2020}. En découpant l'image en une séquence de patchs carrés traités comme des mots, ViT a prouvé qu'un Transformer pur, sans biais inductif de convolution, pouvait atteindre l'état de l'art, à condition d'être pré-entraîné sur des volumes de données massifs (JFT-300M, \cite{sun_revisiting_2017}).
Pour pallier le coût quadratique sur les images haute résolution et le manque de localité, l'architecture Swin Transformer \cite{liu_swin_2021} a réintroduit une structure hiérarchique similaire aux CNN. En calculant l'attention uniquement à l'intérieur de fenêtres locales glissantes (Shifted Windows), Swin combine la modélisation globale des Transformers avec l'efficacité locale des convolutions, devenant le standard actuel pour la segmentation et la détection d'objets.

\subsubsection{Généralisation : Physique et Prise de décision}
La capacité du Transformer à modéliser des graphes d'interaction arbitraires en fait un outil puissant pour la physique et la biologie. L'exemple le plus spectaculaire est AlphaFold 2 \cite{jumper_highly_2021}, qui a résolu le problème du repliement des protéines. Son module central, l'Evoformer, est une variante du Transformer qui traite la protéine comme un graphe dynamique, mettant à jour itérativement la représentation de la séquence d'acides aminés et la matrice de distances 3D par des mécanismes d'attention triangulaire.
Enfin, dans le domaine du contrôle et de la simulation, le Decision Transformer \cite{chen_decision_2021} a reformulé l'apprentissage par renforcement comme un problème de modélisation de séquence. Au lieu d'estimer des fonctions de valeur ou des gradients de politique, ce modèle prédit simplement l'action suivante conditionnée par les états passés et la récompense désirée (le "Return-to-go"). Cette approche "générative" du contrôle permet d'appliquer les techniques de pré-entraînement des LLM à la robotique ou à la navigation d'agents autonomes, unifiant ainsi perception, prédiction physique et prise de décision sous une même architecture.


\subsection{Ancrage dans la problématique}
L'exploration des architectures de traitement de séquence met en lumière un éventail de mécanismes complémentaires pour la modélisation de notre simulateur de capteur, dont la pertinence doit être pondérée par les contraintes spécifiques des signaux radar. Les réseaux convolutionnels, par leur biais inductif de localité, offrent une approche adaptée pour modéliser les interactions à courte portée, telles que les interférences immédiates entre impulsions proches au sein d'un même train. Cependant, leur architecture à fenêtre glissante impose une limitation structurelle majeure : la difficulté à maintenir un état mémoire persistant sur des horizons temporels arbitrairement longs, ce qui peut s'avérer insuffisant pour reproduire fidèlement les processus de pistage temporel qui nécessitent de lier des événements très distants.

De leur côté, les architectures récurrentes (RNN) et les modèles d'espaces d'états (SSM) présentent une affinité naturelle avec la causalité physique du capteur, mimant le comportement des algorithmes de traitement du signal qui mettent à jour des pistes au gré des réceptions. Néanmoins, leur usage impliquerait un changement de paradigme par rapport à notre approche orientée "traduction". Ces modèles excellent dans le traitement séquentiel flux à flux, mais leur application à une tâche de transformation globale de séquence (Seq2Seq) sur de très longs scénarios est complexe. Le goulot d'étranglement du vecteur de contexte, censé compresser toute l'information de la séquence d'entrée avant la génération, devient rapidement prohibitif face à la densité des données radar, limitant leur capacité à reconstruire fidèlement l'ensemble du scénario en une seule passe.

Enfin, l'architecture Transformer et le mécanisme d'attention apportent une capacité de modélisation contextuelle globale, permettant à chaque impulsion d'interagir directement avec l'ensemble de la séquence. Cette propriété est puissante pour capturer des corrélations complexes non-locales et apprendre la fonction de transfert globale du simulateur sans les contraintes de compression des RNN. Toutefois, l'application de ce modèle exige une vigilance particulière quant à sa complexité quadratique, qui peut devenir prohibitive face à la haute densité des flux d'impulsions radar, ainsi qu'à la nécessité d'adapter l'encodage positionnel pour traiter le temps continu irrégulier des PDW plutôt que des indices discrets.
