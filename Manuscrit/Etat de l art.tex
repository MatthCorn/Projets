\chapter{État de l'art : PLAN (à supprimer après rédaction)}

Le chapitre sur l'état de l'art se découpe en 4 parties.

\section{Introduction}

Cette section aborde les aspects suivants :
\begin{itemize}
	\item Environnements numériques
	\item Injection 1 : génération et modélisation de l'environnement
	\item Injection 2 : simulation de phénomènes physiques
	\item Injection 3 : adaptation et interaction
\end{itemize}

\section{IA générative}

Cette section présente le domaine de l'IA générative. Notre problème peut y être naïvement associé mais en réalité quasiment aucune des méthodes ne sera applicable. Les aspects présentés sont:
\begin{itemize}
	\item Le concept d'IA générative. En notant que n'importe quelle fonction génère une sortie à partir d'une entrée et que la dérive de tout appeler IA générative est tentante.
	\item Les VAE et spécialement VAE conditionnels
	\item Les GAN et spécialement GAN conditionnels
	\item Les modèles de diffusion et spécialement ceux conditionnels
	\item Les modèles de langage et GPT
\end{itemize}

\section{Méthodes pour le traitement de séquence}
Cette section présente les architectures connues pour leurs capacités à traiter des séquences, de leurs formes les plus simples aux formes les plus complexes. Par ordre d'apparition:
\begin{itemize}
	\item Le concept de séquence: notion de proximité dans un ensemble. Série temporelle, image, texte.
	\item Réseau de convolution:
	\begin{itemize}
		\item Histoire de son apparition: dans l'image
		\item Comment la convolution interagit avec la séquence
		\item La convolution dans l'image (vue comme une séquence)
		\item La convolution dans le texte
		\item La convolution ailleurs
	\end{itemize}
	\item Réseau de neurones récurrents:
	\begin{itemize}
		\item Histoire de son apparition
		\item Comment un RNN interagit avec la séquence
		\item Variante SSM
		\item RNN dans le texte
		\item RNN dans les systèmes temporels (chaine de Markov)
	\end{itemize}
	\item Transformer:
	\begin{itemize}
		\item Histoire de son apparition: dans le langage
		\item Comment le Transformer interagit avec la séquence
		\item Transformer dans le texte (traduction, GPT, ...)
		\item Transformer dans les systèmes temporels (chaine de Markov)
		\item Transformer dans l'image
		\item Transformer ailleurs (généralisation)
	\end{itemize}
\end{itemize}

\section{Les améliorations}
Cette section met en avant les difficultés liées à l'apprentissage automatique, entre complexité calculatoire, mémorielle et instabilité en entrainement. À cette occasion, nous montrons les propositions existantes visant à résoudre ces problèmes. Par ordre d'apparition:
\begin{itemize}
	\item Compréhension des architectures: Mechanistic Interpretability
	\item Présentation des soucis de performances 
	\item Présentation des solutions aux soucis de performances
	\begin{itemize}
		\item Positional Encoding
		\item Certains mécanismes d'attention
		\item Pre-Training
		\item Embedding et tokenization
	\end{itemize}
	\item Présentation des soucis de stabilité
	\item Présentation des solutions aux soucis de stabilité:
	\begin{itemize}
		\item Layer-norm
		\item Initialisation
		\item Structure (hyper-paramètre de manière générale)
	\end{itemize}
	\item Présentation des soucis d'efficacité et leurs solutions
	\begin{itemize}
		\item Complexité mémoire et calcul: mécanisme d'attention
		\item Vitesse d'entrainement: MAMBA ?
	\end{itemize}
\end{itemize}

\chapter{État de l'art}

\section{Introduction}

L'émergence de l'industrie 4.0 et le développement rapide de l'intelligence artificielle ont propulsé l'utilisation de représentations virtuelles pour simuler, analyser et optimiser des systèmes physiques. Dans ce paysage technologique en pleine effervescence, les termes « jumeau numérique » et « environnement numérique » sont souvent employés de manière interchangeable, engendrant une confusion sémantique préjudiciable à la précision scientifique. Afin de poser les bases conceptuelles solides nécessaires à ce travail, cette section a pour objectif de démêler ces notions. Nous retracerons dans un premier temps l'origine et les définitions, tant idéales que pragmatiques, du jumeau numérique. Dans un second temps, nous présenterons une définition unificatrice et fonctionnelle de l'environnement numérique. Enfin, une synthèse comparative nous permettra d'établir une distinction claire basée sur les flux de données et le critère d'individualisation, et de justifier le positionnement terminologique adopté dans le cadre de cette étude.

\subsection{Cadre Conceptuel : Environnement Virtuel et Jumeau Numérique}
Le concept de jumeau numérique, popularisé et formalisé dès le début des années 2000 par les travaux de Michael Grieves dans le domaine de la manufacturing \cite{grieves_digital_2014}, puis théorisé comme un pilier des systèmes cyber-physiques (CPS) par des auteurs comme Negri et al. \cite{negri_review_2017}, a connu une adoption rapide et variée à travers l'industrie.

Si le terme de "jumeau numérique" s'est imposé dans le paysage technologique, sa définition précise fait l'objet d'un débat animé entre une vision idéale et une approche pragmatique. D'un côté, les puristes, s'appuyant sur les travaux fondateurs de la NASA et de Grieves \cite{grieves_digital_2014}, défendent l'idée qu'un véritable jumeau numérique se caractérise par un couplage bidirectionnel et dynamique avec son homologue physique. Dans cette perspective exigeante, le jumeau n'est pas une simple représentation ; il est une représentation qui s'enrichit continuellement des données du physique et, en retour, pilote, optimise et prédit son comportement \cite{negri_review_2017}. Cette boucle fermée est considérée comme la condition permettant de distinguer le jumeau numérique d'un simple modèle ou d'une simulation. De l'autre, une approche plus pragmatique, largement répandue dans l'industrie, adopte une définition évolutive et par niveaux de maturité. Dans cette vision, une maquette 3D enrichie de données, parfois qualifiée de "digital shadow", peut déjà être labellisée "jumeau numérique". Cette flexibilité sémantique, bien que source de confusion, reflète la réalité des projets industriels où la complexité et le coût d'une intégration parfaite imposent une progression par étapes. Malgré tout, une ligne de démarcation essentielle fait consensus : l'existence d'un transfert de données automatique du système physique vers son représentant virtuel. Sans ce flux, la représentation demeure une simulation ou un modèle générique, que nous qualifierons ici d'« environnement numérique ». Par exemple les simulateurs de conduite autonome comme CARLA \cite{dosovitskiy_carla_2017} sont des environnements numériques essentiels pour l'entraînement des algorithmes d'IA, mais ils simulent un monde routier générique non couplé à un véhicule physique unique, et ne sont en se sens pas des jumeaux numériques. En revanche, certains simulateurs de moteur d'avion, comme ceux déployés par General Electric \cite{tao_digital_2018}, qui est alimenté en temps réel par les données de vol de l'équipement spécifique, incarnent la définition minimale du jumeau numérique, souvent appelée « Digital Shadow ». Ils permettent un suivi individualisé de l'état de santé et de l'usure de chaque moteur de la flotte.

Pour désigner les représentations numériques qui ne sont pas couplées à une instance physique unique, nous recourons donc au terme plus large et unificateur d'Environnement Numérique (Virtual Environment - VE).

La notion d'Environnement Virtuel est interdisciplinaire, et sa définition varie selon que l'on se place dans la communauté de la Réalité Virtuelle, de l'Ingénierie Système ou de l'Intelligence Artificielle. La recherche en Réalité Virtuelle, historiquement focalisée sur l'immersion sensorielle et l'interaction humain-machine, définit souvent les VE comme des « mondes synthétiques générés par ordinateur dans lesquels l'utilisateur a un sentiment d'être présent et d'y interagir » \cite{sherman_understanding_2018}. Cette perspective met l'accent sur les aspects perceptuels et cognitifs. En revanche, dans les domaines de l'ingénierie et de l'IA, l'accent est davantage porté sur la fonction de simulation et de cadre d'expérimentation. Ici, un VE est vu comme un « modèle informatique exécutable d'un système » \cite{fritzson_principles_2015} ou un « cadre de simulation qui permet le test et la validation d'algorithmes dans des conditions contrôlées et reproductibles » \cite{brockman_openai_2016}. Cette vision est moins concernée par l'immersion de l'utilisateur que par la fidélité de la modélisation des processus et des interactions.

Pour englober ces différentes finalités – de la formation immersive au banc d'essai algorithmique – nous proposons la définition unificatrice suivante : Un Environnement Virtuel (VE) désigne une simulation numérique interactive modélisant un ensemble d'entités et de phénomènes, dans le but d'observer, d'analyser ou d'expérimenter des comportements au sein d'un cadre contrôlé.

Ainsi, la notion de VE s'étend du monde immersif interactif au simulateur de système, selon l'objectif visé. Dans le contexte spécifique du développement algorithmique, qui est le nôtre, un VE est principalement un outil de prototypage et de validation : il permet de reproduire des situations expérimentales, de générer des données synthétiques et de tester des modèles ou des algorithmes de manière intensive, sûre et économique, sans recourir initialement à des dispositifs physiques.

L'intégration de l'Intelligence Artificielle au cœur des VE ne constitue pas une approche monolithique, mais se déploie selon trois grands axes d'intervention complémentaires. Ils adressent respectivement le défi de la création des VE, l'amélioration des performances de la simulation elle-même et les capacités d'interaction et d'adaptation du VE.

\subsection{Injection 1 : constitution géométrique et visuelle de l'environnement}

L'injection d'IA dans la conception des environnements numériques répond à deux impératifs distincts : la fidélité de la représentation du monde réel (modélisation) et la diversité des scénarios simulés (génération). Historiquement, ces tâches reposaient sur des processus manuels coûteux. L'apprentissage profond permet d'automatiser ces flux de travail à travers deux paradigmes complémentaires : la reconstruction neurale pour capturer le réel, et la synthèse générative pour créer des actifs inédits.

\paragraph{Modélisation : reconstruction neurale et représentations implicites}
Le premier défi réside dans la numérisation automatisée d'environnements physiques existants. Les approches classiques de photogrammétrie, basées sur des maillages polygonaux explicites, montrent leurs limites en termes de gestion des reflets, de la translucidité et de la densité de stockage. Une alternative significative a émergé avec les représentations implicites, notamment les \textit{Neural Radiance Fields} (NeRF) \cite{mildenhall_nerf_2022}. Cette méthode propose d'encoder la géométrie et l'apparence d'une scène non pas dans une structure de données géométrique, mais dans les poids d'un perceptron multicouche (MLP). 

Bien que précis, le NeRF original présente des coûts d'entraînement et d'inférence élevés. L'introduction du \textit{Hash Encoding} multi-résolution \cite{muller_instant_2022} a permis de réduire drastiquement les temps d'apprentissage. Plus récemment, une transition vers des représentations hybrides a été opérée avec le \textit{3D Gaussian Splatting} \cite{kerbl_3d_2023}. En substituant le lancer de rayon volumétrique par une rastérisation de gaussiennes 3D anisotropes, cette méthode concilie la qualité visuelle des représentations neurales avec les performances temps réel (> 100 FPS) requises pour l'interactivité au sein du VE.

\paragraph{Génération: synthèse d'actifs par diffusion}
Au-delà de la reproduction du réel, la simulation requiert la capacité de générer des environnements variés incluant des objets ou des conditions non observés. L'IA générative intervient ici pour la création d'actifs 3D, palliant la rareté des banques de modèles 3D par l'exploitation des vastes ensembles de données image-texte 2D.

L'état de l'art actuel s'appuie sur le transfert de connaissances depuis des modèles de diffusion 2D pré-entraînés vers la 3D. La méthode \textit{DreamFusion} (Poole et al., 2022) \cite{poole_dreamfusion_2022} a formalisé ce principe via le \textit{Score Distillation Sampling} (SDS). Cette technique utilise un modèle de diffusion 2D comme fonction de critique pour optimiser une représentation 3D (telle qu'un NeRF), de sorte que ses rendus 2D correspondent à une description textuelle donnée. Des itérations ultérieures, comme \textit{ProlificDreamer} (Wang et al., 2023) \cite{wang_prolificdreamer_2023}, ont affiné ce processus via le \textit{Variational Score Distillation} (VSD) pour améliorer la fidélité géométrique et la résolution des textures. Ces approches permettent d'envisager des pipelines de "texte-vers-environnement", où la description sémantique d'une scène suffit à instancier un cadre de simulation complet et physiquement cohérent.


\subsection{Injection 2 : représentation des phénomènes physiques}

L'objectif de cette seconde injection est de substituer ou d'accélérer les solveurs numériques traditionnels (Éléments Finis, Volumes Finis) dont la complexité calculatoire limite les applications temps réel. L'état de l'art s'articule autour de la manière dont la connaissance physique est intégrée dans le modèle d'apprentissage. Nous distinguons trois niveaux d'intégration, allant de l'apprentissage pur par les données à l'intégration structurelle des lois physiques.

\paragraph{Apprentissage par Observation (Data-Driven)}
Le premier niveau considère le simulateur comme une "boîte noire" dont il faut approximer la fonction de transfert à partir d'observations. L'IA apprend ici les corrélations spatio-temporelles sans connaissance explicite des équations sous-jacentes. Les Graph Neural Networks (GNN) se sont imposés comme l'architecture de référence pour cette tâche, notamment pour les systèmes lagrangiens (particules). Les travaux sur les \textit{Graph Network-based Simulators} (GNS) \cite{sanchez-gonzalez_learning_2020} démontrent une capacité remarquable à prédire la dynamique de fluides et de solides déformables en modélisant les interactions locales par passage de messages. Bien que très rapides à l'inférence, ces modèles souffrent d'un manque de garanties physiques : sans contrainte explicite, ils peuvent violer les lois de conservation (masse, énergie) et dériver sur de longues horizons temporels.

\paragraph{Apprentissage contraint par les Équations (Physics-Informed)}
Pour pallier le manque de robustesse physique et la dépendance aux données, une seconde approche intègre les Équations aux Dérivées Partielles (EDP) directement dans l'optimisation. C'est le paradigme des Physics-Informed Neural Networks (PINNs) \cite{raissi_physics-informed_2019}. Ici, le réseau de neurones agit comme un approximateur universel de la solution, et sa fonction de coût inclut les résidus de l'équation physique (ex: Navier-Stokes). Cette méthode permet de s'affranchir partiellement ou totalement de données d'étiquetage (apprentissage non supervisé par la physique). Cependant, les PINNs font face à des défis d'optimisation majeurs (paysage de perte complexe) lorsqu'ils sont confrontés à des dynamiques multi-échelles ou chaotiques.

\paragraph{Apprentissage structuré par la Physique (Inductive Bias)}
Le troisième niveau d'intégration cherche à inscrire les lois physiques non plus dans la fonction de perte (contrainte douce), mais dans l'architecture même du réseau (contrainte dure ou biais inductif). D'une part, les Hamiltonian Neural Networks (HNN)  \cite{greydanus_hamiltonian_2019} imposent une structure symplectique au réseau. Au lieu d'apprendre directement les accélérations, le réseau apprend l'Hamiltonien (l'énergie totale) du système, garantissant par construction la conservation de l'énergie et la réversibilité temporelle, ce qui est crucial pour la stabilité des simulations orbitales ou mécaniques sur le très long terme. D'autre part, les Fourier Neural Operators (FNO) \cite{li_fourier_2021} exploitent la structure spectrale des solutions d'EDP. En apprenant l'opérateur intégral dans l'espace de Fourier, ils acquièrent une propriété d'invariance à la discrétisation (zero-shot super-resolution), permettant de prédire la physique à des résolutions arbitraires, une propriété structurelle absente des CNN ou MLP classiques.

\subsection{Injection 3 : interaction et adaptation décisionnelle}
Cette dernière dimension transforme l'environnement numérique d'un cadre passif en un écosystème réactif et adaptatif. L'objectif est d'enrichir la dynamique interactionnelle pour confronter le système sous test à des situations d'une complexité réaliste, impossible à coder manuellement via des scénarios déterministes.

\paragraph{L'environnement peuplé d'agents apprenants (IA comme Acteur)} 
La première contribution de l'IA est le remplacement des entités scriptées (PNJ, trafic, adversaires) par des agents autonomes pilotés par des politiques neuronales. Contrairement aux machines à états finis classiques, prévisibles et limitées, ces agents sont entraînés via l'Apprentissage par Renforcement Multi-Agents (MARL) ou des mécanismes de Self-Play \cite{silver_mastering_2017}. Cela permet de peupler le VE d'adversaires ou de collaborateurs capables de stratégies émergentes et optimales. L'exemple du défi DARPA AlphaDogfight (2020), où des agents IA ont développé des manœuvres de combat aérien surclassant les experts humains, illustre comment l'injection d'agents apprenants permet de soumettre le système testé à des niveaux de difficulté et de réalisme inatteignables par des méthodes heuristiques \cite{demay_alphadogfight_2022}. Ici, l'environnement devient "intelligent" car ses composantes actives s'adaptent au comportement de l'utilisateur ou du système validé.

\paragraph{L'environnement comme générateur de curriculum (IA comme Superviseur)} 
La seconde contribution concerne le pilotage des paramètres de la simulation par des algorithmes d'optimisation ou évolutionnaires. Au-delà du simple Domain Randomization aléatoire \cite{tobin_domain_2017}, qui manque de direction, l'IA est utilisée pour structurer activement l'apprentissage : c'est l'Apprentissage de Curriculum Automatique (Automatic Curriculum Learning). Des méthodes comme POET (Paired Open-Ended Trailblazer) utilisent des algorithmes évolutionnaires pour co-générer l'environnement en même temps que l'agent \cite{wang_paired_2019}. L'algorithme cherche spécifiquement à générer les configurations topologiques ou physiques (terrains accidentés, conditions météo limites) qui maximisent le progrès de l'agent, créant une "course à l'armement" entre la difficulté du monde et la compétence de l'agent. Dans ce cadre, les algorithmes évolutionnaires agissent comme une forme d'IA générative fonctionnelle, créant des scénarios pertinents et ciblés ("Edge cases") que le hasard seul ne produirait que rarement. \\ 

Ainsi, l'IA transforme le simulateur : d'un simple banc d'essai physique, il devient un partenaire d'entraînement actif, capable de générer des opposants redoutables et d'adapter sa propre complexité pour guider l'apprentissage.

\textcolor{gray}{
\subsection{Rapport à la problématique}
La distinction fondamentale entre le jumeau numérique et l'environnement numérique réside donc dans le principe d'individualisation par les données et la nature du couplage à un actif physique. Le jumeau numérique, qu'il soit envisagé sous sa forme idéale de couplage bidirectionnel ou sous sa forme minimale de « Digital Shadow », se définit intrinsèquement comme l'avatar numérique d'une instance physique unique, tel le moteur d'avion portant un numéro de série spécifique ou une ligne de production particulière. Son essence et sa valeur opérationnelle sont indissociables du lien data continu avec son jumeau physique. En revanche, l'environnement numérique se conçoit comme une représentation générique d'une classe de systèmes, un modèle de cœur humain standard ou un simulateur de réseau routier type. Son essence réside dans la modélisation fidèle de comportements et de lois physiques au sein d'un cadre contrôlé et reproductible. Par conséquent, et afin d'éviter toute ambiguïté terminologique, ce mémoire utilise de manière exclusive et justifiée le terme d'environnement numérique pour désigner le cadre de simulation générique qui constitue son objet d'étude central. Le système modélisé que nous analysons, à l'instar du simulateur de conduite CARLA \cite{dosovitskiy_carla_2017}, est un banc d'essai virtuel destiné au développement et à la validation algorithmique. }

\section{IA générative}

Dans le paysage contemporain de l'apprentissage profond, la définition de l'IA générative a évolué au-delà de la stricte opposition statistique entre modèles de densité et modèles discriminants. Là où un modèle classique condense l'information (classification, réduction de dimension), un modèle génératif apprend à construire des données de haute dimension, structurées spatialement ou temporellement, en capturant les dépendances complexes inhérentes au jeu d'entraînement. L'IA générative désigne aujourd'hui une classe d'architectures neuronales caractérisée par sa capacité de synthèse. 

Un modèle est qualifié de génératif dès lors qu'il construit une donnée structurée en capturant la distribution de probabilité sous-jacente. L'objectif n'est pas simplement d'estimer une valeur locale, mais de bâtir une cohérence globale, respectant les corrélations intrinsèques du domaine d'apprentissage. Cette définition par la capacité constructive est particulièrement pertinente pour la modélisation de systèmes physiques, où la distinction entre discret et continu s'estompe

Dans le contexte spécifique de l'accélération de simulation, nous nous intéressons particulièrement aux modèles génératifs conditionnels, capables de produire une sortie structurée complexe, tel un champ physique ou un état futur, correspondant à une condition initiale. Cette section explore l'évolution chronologique de ces architectures, depuis les approches opérant dans des espaces continus jusqu'aux paradigmes séquentiels discrets.

\subsection{L'approche probabiliste explicite : Les VAE (2013)}

La première avancée significative dans l'apprentissage profond de distributions complexes fut l'introduction des Auto-encodeurs Variationnels (VAE). Contrairement aux auto-encodeurs classiques qui compressent l'information en un point déterministe de l'espace latent, les VAE imposent une structure probabiliste à cet espace, généralement sous la forme d'une distribution gaussienne multivariée. L'innovation majeure réside dans l'introduction de l'astuce de reparamétrisation (reparameterization trick), qui rend le processus d'échantillonnage différentiable et permet l'optimisation du modèle par descente de gradient en maximisant la borne inférieure de la vraisemblance (ELBO) \cite{kingma_auto-encoding_2013}.
Cette capacité à structurer l'espace latent est particulièrement pertinente pour les problèmes de simulation où une même condition initiale peut mener à plusieurs résultats possibles. Dans leur article sur les VAE Conditionnels (C-VAE) \cite{sohn_learning_2015}, il est prouvé qu'il est possible de modéliser des sorties structurées multimodales en conditionnant la génération à la fois par une variable latente aléatoire et par une observation d'entrée. Bien que théoriquement élégants, les VAE ont souffert historiquement d'une limitation qualitative, leur fonction de perte tendant à produire des résultats lissés. Cependant, des développements récents ont redonné une pertinence majeure à cette famille, notamment via la quantification vectorielle de l'espace latent (VQ-VAE). Ces modèles discrets sont désormais au cœur d'architectures de pointe comme les World Models, où un agent apprend à "rêver" des futurs possibles dans un espace latent compact pour accélérer l'apprentissage par renforcement en robotique \cite{razavi_generating_2019}, \cite{ha_world_2018}.


\subsection{La révolution antagoniste : Les GAN (2014)}

Pour répondre au manque de piqué et de réalisme des méthodes variationnelles, une rupture paradigmatique a été introduite avec les Réseaux Antagonistes Génératifs (GAN). Cette approche délaisse l'estimation explicite de la densité de probabilité au profit d'une méthode implicite fondée sur la théorie des jeux. Le processus d'apprentissage est modélisé comme un jeu minimax à somme nulle entre un générateur qui tente de créer des données indiscernables du réel, et un discriminateur qui tente de distinguer les échantillons générés des données d'entraînement \cite{goodfellow_generative_2020}. Comme le soulignent les travaux théoriques sur les modèles implicites, cette formulation permet au générateur d'apprendre des statistiques d'ordre supérieur souvent ignorées par les méthodes classiques, s'affranchissant des contraintes de vraisemblance \cite{mohamed_learning_2017}.
Dans le cadre de la "traduction" d'environnement, les variantes conditionnelles telles que l'architecture Pix2Pix se sont imposées pour transformer une représentation sémantique en une image photoréaliste, produisant des structures fines et des textures détaillées \cite{isola_image--image_2017}. Si les GAN restent difficiles à stabiliser durant l'entraînement, ils ont démontré des capacités de généralisation spectaculaires au-delà de l'image statique. Des travaux comme tempoGAN ont par exemple appliqué ce principe à la mécanique des fluides, parvenant à super-résoudre des simulations volumétriques de fumée ou de liquide tout en garantissant une cohérence temporelle que les méthodes purement statistiques peinent à maintenir \cite{xie_tempogan_2018}.

\subsection{La génération par raffinement : Les Modèles de Diffusion (2020)}

La dernière vague d'innovation, qui définit une grande partie de l'état de l'art actuel, puise son inspiration dans la physique statistique hors équilibre. Les modèles de diffusion probabilistes proposent de construire la génération comme l'inversion d'un processus de destruction d'information. L'idée consiste à détruire progressivement la structure des données par l'ajout successif de bruit gaussien, puis d'entraîner un réseau de neurones à inverser ce processus temporel pour reconstruire la donnée originale étape par étape \cite{sohl-dickstein_deep_2015}. Ce concept a été porté à maturité avec les Denoising Diffusion Probabilistic Models (DDPM), qui offrent un compromis inédit : ils atteignent une qualité d'échantillonnage supérieure aux GAN tout en couvrant mieux la diversité de la distribution des données, évitant le problème d'effondrement de mode \cite{ho_denoising_2020}.
Bien que le processus itératif soit intrinsèquement lent, des méthodes d'échantillonnage accélérées (DDIM) ont rendu ces modèles exploitables en production \cite{song_denoising_2022}. Au-delà de la génération d'images 2D, ce paradigme est aujourd'hui le moteur de la génération d'actifs pour les environnements virtuels. Des approches comme DreamFusion utilisent un modèle de diffusion 2D pré-entraîné pour optimiser une représentation volumétrique (NeRF), permettant de générer des objets 3D complets et cohérents à partir d'une simple description textuelle, ouvrant la voie à la création procédurale d'environnements physiques complexes \cite{poole_dreamfusion_2022}.


\subsection{Le paradigme séquentiel et l'Autorégression}

Enfin, une approche radicalement différente considère la génération comme une prédiction séquentielle discrète. Ce paradigme trouve ses racines dans les Réseaux de Neurones Récurrents (RNN), historiquement utilisés pour générer du texte ou des séries temporelles, mais limités par leur mémoire à court terme et leur séquentialité stricte \cite{graves_generating_2014}. La rupture fondamentale survient avec l'introduction de l'architecture Transformer et du mécanisme d'attention, qui permet de modéliser des dépendances à très long terme et de paralléliser le calcul \cite{vaswani_attention_2017}.
L'évolution majeure de ce paradigme réside dans le concept de pré-entraînement génératif (GPT). Il a été démontré qu'un modèle entraîné massivement sur l'objectif simple de prédire le prochain élément d'une séquence acquiert une capacité de généralisation et de compréhension structurelle émergente \cite{radford_improving_2018}. Aujourd'hui, cette approche déborde largement du cadre du texte. Des architectures multimodales comme Gato \cite{reed_generalist_2022} ou les Vision Transformers (ViT) \cite{dosovitskiy_image_2020} traitent les images ou les actions de contrôle robotique comme des séquences de tokens, unifiant ainsi la génération de contenu visuel et la prise de décision séquentielle au sein d'un même formalisme autorégressif. Cela positionne le traitement de séquence comme une méthode universelle pour la simulation, justifiant l'analyse détaillée des architectures séquentielles qui suivra.

\textcolor{gray}{
\subsection{Rapport à la problématique}
Si le Traitement du Langage Naturel (NLP) opère sur des vocabulaires finis via des distributions catégorielles, la simulation numérique opère souvent dans des espaces métriques continus. Dans ce contexte, une architecture générative ne sélectionne pas un symbole, mais prédit directement les coordonnées d'un état dans un espace vectoriel continu $\mathbb{R}^n$. Bien que ce processus s'apparente mathématiquement à une régression multivariée, il conserve la nature intrinsèque de la génération : le modèle doit bâtir, étape par étape, une cohérence globale.
}

\section{Méthode traitement de séquence}
\subsection{Réseaux de convolution}
Bien que les données séquentielles soient intuitivement associées à une dimension temporelle linéaire, le traitement de l'information repose fondamentalement sur l'extraction de motifs locaux et de relations de voisinage. C'est dans cette optique que les Réseaux de Neurones Convolutionnels (CNN) se positionnent comme une méthode incontournable. Initialement conçus pour la grille spatiale de l'image, ils formalisent une approche du traitement de séquence fondée sur la localité, l'invariance par translation et la hiérarchie des caractéristiques.
\subsubsection{Genèse et prédominance dans l'imagerie}
L'histoire des réseaux de convolution est indissociable de la vision par ordinateur et de la volonté de s'affranchir des descripteurs manuels  (SIFT \cite{lowe_distinctive_2004}, SURF \cite{bay_surf_2006} et HOG \cite{dalal_histograms_2005}). Inspirée par les travaux biologiques sur le cortex visuel, le premier modèle \cite{lecun_gradient-based_2002} a introduit les concepts fondateurs de champ récepteur local et de partage des poids pour la reconnaissance de caractères manuscrits. Cependant, c'est l'avènement d'AlexNet \cite{krizhevsky_imagenet_2012} qui a marqué le véritable point d'inflexion en démontrant la supériorité de l'apprentissage profond sur GPU pour l'extraction de caractéristiques. Cette percée a ouvert la voie à des architectures plus profondes et plus efficientes. Par exemple, l'architecture GoogLeNet \cite{szegedy_going_2015} factorise les convolutions pour réduire le coût de calcul tout en augmentant la largeur du réseau, permettant de traiter des motifs à différentes échelles simultanément.
\subsubsection{La convolution dans l'image : Une séquence spatiale 2D}
Dans le contexte de l'image, la séquence est bidimensionnelle et le CNN y opère une extraction hiérarchique. Les premières couches détectent des primitives simples comme des bords ou des textures, qui sont ensuite combinées pour former des motifs sémantiques complexes. Cette capacité d'abstraction a été poussée à son paroxysme par l'architecture VGG \cite{simonyan_very_2015}, qui a standardisé l'usage de filtres de très petite taille ($3\times3$) empilés en grande profondeur. Les auteurs ont démontré qu'une séquence de petites convolutions est plus efficace pour capturer des non-linéarités complexes qu'une seule grande convolution. Cependant, l'augmentation de la profondeur a engendré des problèmes de disparition du gradient, résolus avec ResNet \cite{he_deep_2016}. L'introduction de connexions résiduelles a permis d'entraîner des réseaux dépassant la centaine de couches, essentiels pour capturer les dépendances à très longue portée dans des images haute résolution.
Pour les tâches de "traduction" d'image vers image, cruciales en simulation (par exemple, passer d'une carte de densité à un champ de pression), il est impératif de ne pas perdre l'information spatiale lors de la compression. L'architecture U-Net \cite{navab_u-net_2015}, initialement pour la segmentation biomédicale combine un chemin de contraction et un chemin d'expansion reliés par des connexions latérales (skip connections). Cette structure permet de générer une sortie de même résolution que l'entrée en fusionnant le contexte sémantique global et les détails locaux. Cette architecture est aujourd'hui une référence pour les modèles de substitution en physique. D'autres variantes comme DenseNet \cite{iandola_densenet_2014} ont poussé cette logique plus loin en connectant chaque couche à toutes les suivantes pour maximiser le flux d'information, bien que cela se fasse au prix d'une consommation mémoire accrue.
\subsubsection{La convolution dans les séquences 1D (Texte, Audio, Séries Temporelles)}
Bien que souvent associés à l'image, les CNN se sont révélés extrêmement performants pour traiter des séquences unidimensionnelles, surpassant parfois les réseaux récurrents grâce à leur capacité de parallélisation. Dans le traitement du signal audio, l'architecture WaveNet \cite{van_den_oord_wavenet_2016} a marqué une rupture en utilisant des convolutions causales dilatées. Ce mécanisme permet au champ récepteur du réseau de croître exponentiellement avec la profondeur sans augmenter le nombre de paramètres, capturant ainsi des dépendances temporelles sur des milliers de pas de temps, ce qui est impossible pour un RNN standard \textcolor{red}{REF}.
Dans le domaine du traitement du langage naturel (NLP), cette logique a été appliquée avec succès à la traduction automatique. L'architecture ConvS2S \cite{gehring_convolutional_2017} entièrement convolutionnelle pour la séquence à séquence, utilise des mécanismes d'attention multi-pas pour pondérer l'importance des mots sources. De même, ByteNet \cite{kalchbrenner_neural_2017}, réalise la traduction en temps linéaire en empilant des convolutions dilatées. Ces travaux ont démontré que l'induction de biais locaux propres aux convolutions est pertinente pour la syntaxe et la sémantique locale.
Cette approche a été généralisée aux séries temporelles génériques sous le nom de Temporal Convolutional Networks (TCN) \cite{bai_empirical_2018}. L'étude comparative démontre que sur une vaste gamme de tâches séquentielles, comme la prédiction de charge énergétique ou la modélisation de séquences symboliques, les TCN surpassent souvent les réseaux récurrents (LSTM/GRU) \textcolor{red}{REF} tout en offrant une stabilité d'entraînement supérieure. Une étude récente \cite{tay_are_2022} prolonge ce constat en suggérant que des architectures convolutionnelles modernes pré-entraînées peuvent rivaliser avec les Transformers sur certaines tâches textuelles, soulignant la pertinence continue de ce paradigme.
\subsubsection{Généralisation : La convolution au-delà des grilles euclidiennes}
Le principe de convolution, initialement restreint aux grilles régulières 1D ou 2D, a été généralisé pour traiter des structures de données complexes multidimensionnelles ou irrégulières, typiques de la simulation scientifique avancée.
Une première extension naturelle concerne les données volumétriques (3D) et spatio-temporelles (Vidéo/4D). Pour l'analyse de vidéos ou de simulations dynamiques, C3D \cite{tran_learning_2015} utilise des filtres de convolution tridimensionnels ($x, y, t$) pour apprendre simultanément les caractéristiques spatiales et le mouvement temporel. Dans le domaine médical et physique, l'architecture V-Net \cite{milletari_v-net_2016} étend le principe du U-Net à la 3D, utilisant des noyaux volumétriques pour segmenter des structures dans l'espace tridimensionnel. Cependant, ces méthodes souffrent d'une complexité cubique qui limite souvent la résolution spatiale traitable.
La généralisation la plus significative concerne les données non-euclidiennes, structurées sous forme de graphes. Dans une simulation physique lagrangienne (maillage non structuré) ou un système moléculaire, la notion de "voisinage" n'est pas définie par une grille mais par la topologie. Les Graph Convolutional Networks (GCN) \cite{kipf_semi-supervised_2016}, propose que l'opération de convolution devienne une agrégation spectrale ou spatiale des caractéristiques des nœuds voisins. Cette approche a été enrichie par des méthodes comme GraphSAGE \cite{hamilton_inductive_2017}, qui propose une convolution inductive capable de généraliser à des nœuds invisibles durant l'entraînement, essentielle pour les graphes dynamiques en simulation. Enfin, pour traiter des nuages de points 3D bruts (issus de LiDAR ou de scan), des architectures comme PointNet++ \cite{qi_pointnet_2017} appliquent des opérations de convolution hiérarchiques directement sur des ensembles de points désordonnés, permettant de traiter la géométrie 3D sans passer par une voxelisation coûteuse.


\subsection{Réseaux de neurones récurrents et Espaces d'Etats (RNN et SSM)}
Si les réseaux de convolution abordent la séquence par une fenêtre glissante locale, une autre famille d'architectures adopte une approche intrinsèquement temporelle : la modélisation récursive. Qu'il s'agisse des Réseaux de Neurones Récurrents (RNN) historiques ou des récents Modèles d'Espaces d'États (SSM), le principe fondateur reste la persistance de l'information. Le modèle maintient un état caché interne $h_t$ qui agit comme une mémoire compressée de tout l'historique passé, mise à jour à chaque nouvelle observation. Cette formulation est particulièrement naturelle pour la simulation physique, car elle mime la dynamique des systèmes causaux où l'état futur dépend de l'état présent et des forces appliquées.
\subsubsection{Genèse et mécanismes d'interaction : De la boucle simple aux portes logiques}
L'histoire de cette approche débute avec les RNN classiques \cite{elman_finding_1990} qui introduisent une boucle de rétroaction permettant au réseau de maintenir une trace du contexte temporel. Cependant, bien que ces réseaux parviennent à générer des séquences continues complexes comme de l'écriture manuscrite, ils souffrent d'une instabilité critique lors de l'entraînement : le problème de la disparition ou de l'explosion du gradient \cite{graves_generating_2014}. Sur de longues séquences, le signal d'erreur se dilue, empêchant l'apprentissage des causes lointaines d'un événement. Pour y remédier, le LSTM (Long Short-Term Memory) \cite{hochreiter_long_1997} propose des "cellules" mémoires protégées par des portes logiques, et peut choisir de retenir ou d'effacer une information sur des milliers de pas de temps. Cette capacité a été affinée par l'introduction du GRU \cite{cho_properties_2014}, \cite{chung_empirical_2014}, une variante plus économe.
\subsubsection{Renouveau architectural : Les Modèles d'Espaces d'Etats (SSM)}
Malgré leur robustesse, les LSTM conservent une limitation structurelle majeure : leur traitement séquentiel interdit la parallélisation sur GPU. C'est pour lever ce verrou qu'une nouvelle classe de modèles a émergé : les State Space Models (SSM). Ces modèles puisent leur origine théorique dans le papier HiPPO \cite{gu_hippo_2020}, qui formalise mathématiquement comment compresser optimalement une histoire continue dans un vecteur de taille fixe via des projections polynomiales orthogonales. Cette base a permis de développer S4 (Structured State Space sequence model) \cite{gu_efficiently_2022}, capable de modéliser des dépendances sur plus de 10 000 pas de temps en résolvant une équation différentielle continue discrétisée.
Cependant, les premiers SSM souffraient d'une rigidité dynamique, peinant à sélectionner l'information pertinente en fonction du contexte ("Content-based selection"). Cette limite a été adressée par l'architecture Mamba \cite{gu_mamba_2024}. En rendant les matrices d'état dépendantes de l'entrée, Mamba atteint des performances comparables aux premiers Transformers \textcolor{red}{REF} tout en conservant une complexité linéaire. Toutefois, ces modèles restent délicats à stabiliser sur des dynamiques hautement chaotiques où la discrétisation numérique peut introduire des dérives.
\subsubsection{Application au texte : L'ère du Sequence-to-Sequence et de la Traduction}
Dans le traitement du langage, l'approche récurrente a connu son apogée avec le paradigme Seq2Seq \cite{sutskever_sequence_2014}. En utilisant deux LSTM (un encodeur et un décodeur), cette architecture a permis de traiter des séquences de longueurs variables. Cette avancée a transformé l'industrie de la traduction avec le déploiement du Google’s Neural Machine Translation System (GNMT) \cite{wu_googles_2016} en 2016, réduisant les erreurs de traduction de près de 60 \% par rapport aux systèmes statistiques. Au-delà de la traduction, ce paradigme a permis des avancées dans la modélisation prédictive de parcours complexes, comme l'illustré par le modèle Doctor AI \cite{choi_doctor_2016}. Ce modèle utilise des RNN pour prédire les futurs diagnostics médicaux et la durée avant la prochaine visite à partir de l'historique clinique des patients, démontrant la capacité des RNN à capturer des dynamiques temporelles irrégulières et multivariées dans des données réelles bruitées.
\subsubsection{Application aux systèmes temporels, physiques et créatifs}
Au-delà du texte, les RNN se sont imposés comme l'outil naturel pour la modélisation de systèmes dynamiques continus, un domaine crucial pour la simulation. Une étude sur la prévision de systèmes chaotiques \cite{vlachas_data-driven_2018} a mis en avant que les LSTM pouvaient apprendre la dynamique de l'attracteur de Lorenz ou de l'équation de Kuramoto-Sivashinsky mieux que les modèles physiques simplifiés, en capturant les propriétés non-linéaires de l'évolution temporelle à court terme. Cette capacité à modéliser le chaos déterministe fait des RNN des candidats sérieux pour accélérer les simulations de mécanique des fluides turbulents.
Dans l'industrie, cette robustesse est exploitée pour la prévision probabiliste avec DeepAR \cite{salinas_deepar_2020}, utilisé par Amazon pour sa chaîne logistique. Ce modèle apprend une distribution de probabilité future à chaque pas de temps, permettant de quantifier l'incertitude via des simulations de Monte Carlo. Enfin, la capacité "générative constructive" des RNN a été pionnière dans la création artistique. Le modèle Performance RNN \cite{oore_this_2020}, développé par Google Magenta, a montré qu'un LSTM pouvait générer des performances de piano expressives (avec nuances de vélocité et de timing) en traitant la musique non pas comme une partition rigide, mais comme une séquence temporelle continue d'événements, prouvant que les RNN peuvent capturer des structures hiérarchiques globales (phrasé musical) tout en gérant des détails micro-temporels.




\subsection{Transformer}
Si les réseaux récurrents ont introduit la mémoire et les réseaux convolutionnels la localité, l'architecture Transformer a proposé un changement de paradigme radical en postulant que l'interaction entre les éléments d'une séquence doit être modélisée par une relation directe de contenu à contenu, et non par une contrainte de proximité spatiale ou temporelle. Cette architecture, devenue l'épine dorsale de l'IA générative moderne, repose sur le mécanisme d'attention.
\subsubsection{Histoire : De l'alignement au "Pointer Network" et à l'Attention pure}
L'émergence du Transformer est le fruit d'une lente maturation visant à résoudre le goulot d'étranglement des architectures Encodeur-Décodeur récurrentes (RNN). Dans le paradigme Seq2Seq classique, toute l'information de la phrase source devait être compressée dans un unique vecteur de contexte de taille fixe, entraînant une perte d'information critique sur les longues séquences. La première solution majeure fut proposée par Bahdanau et al. (2014) dans Neural Machine Translation by Jointly Learning to Align and Translate [1]. Ils ont introduit un mécanisme d'attention additive permettant au décodeur de "chercher" (search) et d'aligner (align) les parties pertinentes de la phrase source à chaque étape de la génération. Ici, l'attention n'était encore qu'un module auxiliaire greffé sur des RNN.
Une seconde étape conceptuelle décisive fut franchie avec les Pointer Networks de Vinyals et al. (2015) [2]. Ce travail a démontré qu'un réseau de neurones pouvait apprendre à résoudre des problèmes combinatoires (comme l'enveloppe convexe) en utilisant l'attention comme un pointeur pour sélectionner des éléments de l'entrée comme sortie, plutôt que de générer des symboles abstraits. Cela a ancré l'idée que le mécanisme de sélection basé sur le contenu ("Content-based addressing") était suffisamment puissant pour structurer la sortie.
La rupture définitive survient avec l'article Attention Is All You Need de Vaswani et al. (2017) [3]. Les auteurs ont démontré que la récurrence, jugée jusqu'alors indispensable pour encoder l'ordre séquentiel, était en réalité superflue et limitante pour la parallélisation. En ne conservant que le mécanisme d'attention (devenu Self-Attention), ils ont permis un traitement parallèle massif des séquences, réduisant la distance de propagation de l'information entre deux mots quelconques à une constante $O(1)$, contre $O(N)$ pour un RNN.
\subsubsection{Mécanisme d'interaction et complexité}
Au cœur du Transformer réside l'attention par produit scalaire (Scaled Dot-Product Attention). Contrairement à la convolution qui applique un filtre fixe localement, l'attention est dynamique et globale : chaque élément de la séquence interagit avec tous les autres. Chaque jeton est projeté en trois vecteurs : une Requête ($Q$), une Clé ($K$) et une Valeur ($V$). Le réseau calcule une matrice de similarité entre toutes les paires $Q$ et $K$, déterminant ainsi le poids avec lequel chaque élément doit "écouter" les autres pour construire sa propre représentation contextuelle.
Cependant, cette interaction "tout-vers-tout" induit une limitation intrinsèque majeure : la complexité calculatoire et mémorielle est quadratique $O(N^2)$ par rapport à la longueur de la séquence $N$. Cela rend le modèle prohibitif pour les très longues séquences (au-delà de quelques milliers de tokens) sans artifices d'optimisation. De plus, l'opération d'attention étant invariante par permutation (elle traite la séquence comme un ensemble, un "sac de mots"), le Transformer est nativement amnésique à l'ordre des éléments. Il nécessite donc l'injection explicite d'informations de position (Positional Encodings), soit absolues (sinusoïdales), soit relatives, pour reconstruire la topologie temporelle ou spatiale de la donnée.
\subsubsection{Le Transformer dans le texte : La divergence des architectures}
Dans le traitement du langage naturel (NLP), le Transformer a provoqué une véritable explosion cambrienne des modèles, se scindant en trois familles distinctes.
La première, celle des Encodeurs, est incarnée par BERT (Devlin et al., 2018) [4]. Utilisant une attention bidirectionnelle, ces modèles excellent dans la compréhension et la classification, car chaque mot a accès au contexte passé et futur simultanément.
La seconde, celle des Décodeurs, est dominée par la lignée GPT (Radford et al., 2018, Brown et al., 2020) [5, 6]. Ici, l'attention est causale (masquée vers le futur), optimisée pour la génération autorégressive. C'est cette branche qui a mis en évidence les "lois d'échelle" (Scaling Laws), montrant que la performance de prédiction du prochain token suit une loi de puissance en fonction du nombre de paramètres et de données, ouvrant la voie aux LLM actuels.
La troisième famille, Encodeur-Décodeur, reste fidèle à l'architecture originale de Vaswani pour les tâches de traduction ou de résumé. Le modèle T5 (Raffel et al., 2020) a poussé ce paradigme à son extrême en reformulant toute tâche NLP (y compris la classification) comme un problème de génération de texte-vers-texte [7].
\subsubsection{Le Transformer dans les systèmes temporels : Promesses et controverses}
L'application des Transformers aux séries temporelles continues (consommation énergétique, trafic, météo) a fait l'objet d'une recherche intense, synthétisée par Wen et al. (2023) dans Transformers in Time Series: A Survey [8]. L'attrait principal réside dans la capacité théorique de l'attention à capturer des corrélations à très long terme et des saisonnalités complexes que les RNN peinent à retenir.
Des architectures spécifiques ont été proposées pour briser la complexité quadratique. Informer (Zhou et al., 2021) introduit une attention "ProbSparse" pour sélectionner uniquement les interactions dominantes, réduisant la complexité à $O(N \log N)$ [9]. Autoformer (Wu et al., 2021) va plus loin en remplaçant le produit scalaire par une auto-corrélation pour mieux capturer les périodicités [10].
Cependant, l'efficacité réelle des Transformers sur des signaux continus est contestée. Zeng et al. (2023) ont démontré dans Are Transformers Effective for Time Series Forecasting? qu'un simple modèle linéaire bien calibré (DLinear) surpassait souvent des Transformers complexes sur les benchmarks standards [11]. La raison invoquée est que l'attention, conçue pour la sémantique discrète, tend à sur-interpréter le bruit dans les signaux continus et perd l'information d'ordre temporel cruciale, malgré les encodages positionnels. Néanmoins, des approches récentes comme PatchTST (Nie et al., 2023), qui segmentent le signal en patchs (comme en vision) avant d'appliquer l'attention, semblent redonner l'avantage aux Transformers en traitant des dynamiques locales plutôt que des points isolés [12].
\subsubsection{Le Transformer dans l'image : Patchs et hiérarchie}
L'hégémonie des CNN en vision a été remise en cause par le Vision Transformer (ViT) de Dosovitskiy et al. (2020) [13]. En découpant l'image en une séquence de patchs carrés traités comme des mots, ViT a prouvé qu'un Transformer pur, sans biais inductif de convolution, pouvait atteindre l'état de l'art, à condition d'être pré-entraîné sur des volumes de données massifs (JFT-300M).
Pour pallier le coût quadratique sur les images haute résolution et le manque de localité, l'architecture Swin Transformer (Liu et al., 2021) a réintroduit une structure hiérarchique similaire aux CNN [14]. En calculant l'attention uniquement à l'intérieur de fenêtres locales glissantes (Shifted Windows), Swin combine la modélisation globale des Transformers avec l'efficacité locale des convolutions, devenant le standard actuel pour la segmentation et la détection d'objets.
\subsubsection{Généralisation : Physique et Prise de décision}
La capacité du Transformer à modéliser des graphes d'interaction arbitraires en fait un outil puissant pour la physique et la biologie. L'exemple le plus spectaculaire est AlphaFold 2 (Jumper et al., 2021), qui a résolu le problème du repliement des protéines [15]. Son module central, l'Evoformer, est une variante du Transformer qui traite la protéine comme un graphe dynamique, mettant à jour itérativement la représentation de la séquence d'acides aminés et la matrice de distances 3D par des mécanismes d'attention triangulaire.
Enfin, dans le domaine du contrôle et de la simulation, le Decision Transformer (Chen et al., 2021) a reformulé l'apprentissage par renforcement comme un problème de modélisation de séquence [16]. Au lieu d'estimer des fonctions de valeur ou des gradients de politique, ce modèle prédit simplement l'action suivante conditionnée par les états passés et la récompense désirée (le "Return-to-go"). Cette approche "générative" du contrôle permet d'appliquer les techniques de pré-entraînement des LLM à la robotique ou à la navigation d'agents autonomes, unifiant ainsi perception, prédiction physique et prise de décision sous une même architecture.

\subsubsection*{Bibliographie de la section}
1 Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. \textit{ICLR}.\\
2 Vinyals, O., Fortunato, M., and Jaitly, N. (2015). Pointer Networks. \textit{NeurIPS}.\\
3 Vaswani, A., et al. (2017). Attention Is All You Need. \textit{NeurIPS}.\\
4 Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. \textit{NAACL}.\\
5 Radford, A., et al. (2018). Improving Language Understanding by Generative Pre-Training. \textit{OpenAI}.\\
6 Brown, T., et al. (2020). Language Models are Few-Shot Learners. \textit{NeurIPS}.\\
7 Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. \textit{JMLR}.\\
8 Wen, Q., et al. (2023). Transformers in Time Series: A Survey. \textit{IJCAI}.\\
9 Zhou, H., et al. (2021). Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. \textit{AAAI}.\\
10 Wu, H., et al. (2021). Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. \textit{NeurIPS}.\\
11 Zeng, A., et al. (2023). Are Transformers Effective for Time Series Forecasting? \textit{AAAI}.\\
12 Nie, Y., et al. (2023). A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. \textit{ICLR}.\\
13 Dosovitskiy, A., et al. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. \textit{ICLR}.\\
14 Liu, Z., et al. (2021). Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. \textit{ICCV}.\\
15 Jumper, J., et al. (2021). Highly accurate protein structure prediction with AlphaFold. \textit{Nature}.\\
16 Chen, L., et al. (2021). Decision Transformer: Reinforcement Learning via Sequence Modeling. \textit{NeurIPS}.\\



