\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Introduction}{29}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}IA générative}{29}{section.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Méthodes pour le traitement de séquence}{29}{section.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Les améliorations}{30}{section.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}État de l'art}{31}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Environnements Numériques et fondements de l'Intelligence Artificielle}{31}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Distinction conceptuelle : Environnement Virtuel et Jumeau Numérique}{31}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Principes mathématiques de l'Apprentissage Profond}{33}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2.1}Risque théorique, risque empirique et surapprentissage}{33}{subsubsection.3.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2.2}Optimisation par descente de gradient et rétropropagation}{34}{subsubsection.3.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2.3}Géométrie de l'optimisation}{35}{subsubsection.3.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}L'IA pour la constitution géométrique et visuelle de l'environnement}{36}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.1}Reconstruction neurale et représentations implicites}{36}{subsubsection.3.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.2}Synthèse géométrique par modèles de diffusion}{36}{subsubsection.3.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}L'IA pour l'accélération et la modélisation des phénomènes physiques}{37}{subsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.1}Apprentissage par Observation (Data-Driven)}{37}{subsubsection.3.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.2}Apprentissage sous contraintes physiques (Physics-Informed)}{37}{subsubsection.3.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.3}Apprentissage structuré par biais inductif physique}{38}{subsubsection.3.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}L'IA au service de l'interactivité et de l'adaptation décisionnelle}{38}{subsection.3.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5.1}Intégration d'agents autonomes par apprentissage par renforcement}{38}{subsubsection.3.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5.2}Génération procédurale et apprentissage par curriculum}{38}{subsubsection.3.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Ancrage dans la problématique}{39}{subsection.3.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Architectures neuronales génératives}{39}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Auto-Encodeurs Variationnels (VAE)}{40}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Réseaux Antagonistes Génératifs (GAN)}{40}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Modèles de Diffusion Probabilistes}{41}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Le paradigme séquentiel et l'Autorégression}{41}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Ancrage dans la problématique}{42}{subsection.3.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Architectures de traitement séquentiel et spatial}{42}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Typologie des données : causalité temporelle et topologie spatiale}{42}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.1}Propriétés causales des données séquentielles}{43}{subsubsection.3.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.2}Topologie et contiguïté des données spatiales}{43}{subsubsection.3.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1.3}Universalité de la modélisation séquentielle}{43}{subsubsection.3.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Réseaux de convolution}{43}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.1}Origine et application à la vision par ordinateur}{44}{subsubsection.3.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.2}Filtrage local et expansion hiérarchique du champ récepteur}{44}{subsubsection.3.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustration d'une convolution 1D standard et de l'expansion hiérarchique du champ récepteur}}{46}{figure.caption.9}\protected@file@percent }
\newlabel{convolution base}{{3.1}{46}{Illustration d'une convolution 1D standard et de l'expansion hiérarchique du champ récepteur}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.3}Application aux structures spatiales bidimensionnelles}{46}{subsubsection.3.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Impact de la topologie du support de convolution sur la causalité temporelle : approche centrée (gauche) et approche causale (droite)}}{47}{figure.caption.10}\protected@file@percent }
\newlabel{convolution comparaison}{{3.2}{47}{Impact de la topologie du support de convolution sur la causalité temporelle : approche centrée (gauche) et approche causale (droite)}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.4}Application aux séquences unidimensionnelles}{47}{subsubsection.3.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2.5}Généralisation aux données volumétriques et topologies irrégulières}{47}{subsubsection.3.3.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Réseaux de neurones récurrents et Espaces d'Etats (RNN et SSM)}{48}{subsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.1}Évolution architecturale : des RNN simples aux portes logiques (LSTM/GRU)}{48}{subsubsection.3.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.2}Principe de récurrence et mise à jour de l'état mémoire}{49}{subsubsection.3.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Mécanisme fondamental de la récurrence et propagation de l'état mémoire}}{50}{figure.caption.11}\protected@file@percent }
\newlabel{RNN base}{{3.3}{50}{Mécanisme fondamental de la récurrence et propagation de l'état mémoire}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.3}Modèles d'Espaces d'États (SSM)}{50}{subsubsection.3.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Topologies d'application des architectures récurrentes : traitement de flux (gauche) et traduction globale (droite)}}{51}{figure.caption.12}\protected@file@percent }
\newlabel{RNN comparaison}{{3.4}{51}{Topologies d'application des architectures récurrentes : traitement de flux (gauche) et traduction globale (droite)}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.4}Traitement du langage naturel et architectures Sequence-to-Sequence}{51}{subsubsection.3.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3.5}AApplication aux systèmes dynamiques et temporels continus}{51}{subsubsection.3.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}L'architecture Transformer}{52}{subsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.1}Limites de la récurrence et introduction du mécanisme d'Attention}{52}{subsubsection.3.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.2}Architecture macroscopique Encodeur-Décodeur}{53}{subsubsection.3.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Flux d'information macroscopique dans l'architecture Transformer Encodeur-Décodeur}}{54}{figure.caption.13}\protected@file@percent }
\newlabel{simple transformer}{{3.5}{54}{Flux d'information macroscopique dans l'architecture Transformer Encodeur-Décodeur}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.3}L'Encodeur}{54}{subsubsection.3.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.4}Le Décodeur}{54}{subsubsection.3.3.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Architecture Transformer complète : empilement des blocs}}{55}{figure.caption.14}\protected@file@percent }
\newlabel{full transformer}{{3.6}{55}{Architecture Transformer complète : empilement des blocs}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Architecture interne du bloc Encodeur}}{55}{figure.caption.15}\protected@file@percent }
\newlabel{encoder transformer}{{3.7}{55}{Architecture interne du bloc Encodeur}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Architecture interne du bloc Décodeur}}{56}{figure.caption.16}\protected@file@percent }
\newlabel{decoder transformer}{{3.8}{56}{Architecture interne du bloc Décodeur}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.5}Le mécanisme d'Attention et ses variantes}{56}{subsubsection.3.3.4.5}\protected@file@percent }
\newlabel{attentionscore}{{3.1}{57}{Le mécanisme d'Attention et ses variantes}{equation.3.1}{}}
\newlabel{pondatt}{{3.2}{57}{Le mécanisme d'Attention et ses variantes}{equation.3.2}{}}
\newlabel{attentionoperator}{{3.3}{57}{Le mécanisme d'Attention et ses variantes}{equation.3.3}{}}
\newlabel{queryeq}{{3.4}{57}{Le mécanisme d'Attention et ses variantes}{equation.3.4}{}}
\newlabel{MHAeq}{{3.6}{57}{Le mécanisme d'Attention et ses variantes}{equation.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Illustration de l'Attention Croisée ($\mathsf  {MHCA}$). Les requêtes (en bleu) proviennent de la séquence cible et interrogent l'intégralité du contexte source (clés en jaune). À gauche, la requête $q_3$ agrège l'information de la séquence source (à l'origine des clés et valeurs $\mathcal  {KV})$) pour construire $A(q_3,\mathcal  {KV})) $, représenté dans la cellule verte. Le cône d'interaction est représenté en dégradé de bleu. À droite, le même calcul est effectué indépendamment pour la requête $q_5$. Cette indépendance des calculs permet leur parallélisation.}}{58}{figure.caption.17}\protected@file@percent }
\newlabel{fig:cross_att}{{3.9}{58}{Illustration de l'Attention Croisée ($\mathsf {MHCA}$). Les requêtes (en bleu) proviennent de la séquence cible et interrogent l'intégralité du contexte source (clés en jaune). À gauche, la requête $q_3$ agrège l'information de la séquence source (à l'origine des clés et valeurs $\mathcal {KV})$) pour construire $A(q_3,\mathcal {KV})) $, représenté dans la cellule verte. Le cône d'interaction est représenté en dégradé de bleu. À droite, le même calcul est effectué indépendamment pour la requête $q_5$. Cette indépendance des calculs permet leur parallélisation}{figure.caption.17}{}}
\newlabel{maskedattention}{{3.8}{59}{Le mécanisme d'Attention et ses variantes}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.6}Application au traitement du langage naturel}{59}{subsubsection.3.3.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.7}Application et limites sur les séries temporelles continues}{59}{subsubsection.3.3.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Comparaison des modes d'Auto-Attention. La séquence d'entré nourrit à la fois $\mathcal  {Q}$ et $\mathcal  {KV}$. À gauche, l'interaction est globale. La requête $q_4$ agrège l'information de toute la séquence pour construire $A(q_4,\mathcal  {KV}))$, représenté dans la cellule verte. Le cône d'interaction est représenté en dégradé de bleu. À droite, l'interaction est causale. L'accès aux clés futures est donc bloqué (score $-\infty $), cette restriction est visible sur le cône d'interaction en nuance de bleu. La requête $q_4$ agrège aux positions $j \le 4$ pour construire $A(q_4,\mathcal  {KV}))$, qui constitue une prédiction des états futurs. Pour les deux images, on représente en arrière plan les même calculs pour la requête $q_6$, réalisés de manière indépendante.}}{60}{figure.caption.18}\protected@file@percent }
\newlabel{fig:compare_att}{{3.10}{60}{Comparaison des modes d'Auto-Attention. La séquence d'entré nourrit à la fois $\mathcal {Q}$ et $\mathcal {KV}$. À gauche, l'interaction est globale. La requête $q_4$ agrège l'information de toute la séquence pour construire $A(q_4,\mathcal {KV}))$, représenté dans la cellule verte. Le cône d'interaction est représenté en dégradé de bleu. À droite, l'interaction est causale. L'accès aux clés futures est donc bloqué (score $-\infty $), cette restriction est visible sur le cône d'interaction en nuance de bleu. La requête $q_4$ agrège aux positions $j \le 4$ pour construire $A(q_4,\mathcal {KV}))$, qui constitue une prédiction des états futurs. Pour les deux images, on représente en arrière plan les même calculs pour la requête $q_6$, réalisés de manière indépendante}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.8}Application à la vision par ordinateur (Vision Transformers)}{61}{subsubsection.3.3.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4.9}Généralisation aux systèmes physiques et à la prise de décision}{61}{subsubsection.3.3.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Ancrage dans la problématique}{61}{subsection.3.3.5}\protected@file@percent }
\@setckpt{Etat de l art}{
\setcounter{page}{63}
\setcounter{equation}{8}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{3}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{2}
\setcounter{section@level}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{52}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{111}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{2}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
}
